{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c7cc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "from typing import Optional, List, Tuple\n",
    "import ast\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Local imports\n",
    "from utils_notebook import (\n",
    "    SemanticCleaner, \n",
    "    extract_answer, \n",
    "    calcular_metricas_memoria,\n",
    "    format_choices,\n",
    "    parse_simple_score,\n",
    "    make_question\n",
    ")\n",
    "\n",
    "cleaner = SemanticCleaner()\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eefa79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../dataset/arc_challenge_train_processed.csv')\n",
    "df_valid = pd.read_csv('../dataset/arc_challenge_valid_processed.csv')\n",
    "df_questions = pd.concat([df_train, df_valid], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7caa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alt_by_question(question_id: str) -> str:  # Mudei para str\n",
    "    row = df_questions[df_questions['id'] == question_id]\n",
    "    if not row.empty:\n",
    "        alternatives_str = row.iloc[0]['choices']\n",
    "        alternatives = ast.literal_eval(alternatives_str)\n",
    "        formatted = \"\\n\".join(f\"  ‚Ä¢ {text}\" for text in alternatives['text'])\n",
    "        return formatted\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea776bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(  get_alt_by_question('Mercury_SC_415702'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "77cd754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de fatos:  3993\n"
     ]
    }
   ],
   "source": [
    "gt =  pd.read_csv('scientific_facts_audit_by_gpt_2.csv') \n",
    "\n",
    "gt = gt.fillna(\"N/A\")\n",
    "gt = gt[gt['fact'] != \"N/A\"].copy()\n",
    "gt.drop_duplicates(subset=['fact'], keep='first', inplace=True)\n",
    "#gt = gt[gt['origin'] == 'train']\n",
    "gt['fact_clean'] = gt['fact'].apply(lambda x: cleaner.clean(x))\n",
    "print(\"Quantidade de fatos: \", len(gt))\n",
    "\n",
    "mapping = {\n",
    "    'CORRETO': 'CORRECT',\n",
    "    'ERRADO': 'INCORRECT'\n",
    "}\n",
    "\n",
    "# Aplica a substitui√ß√£o na coluna 'verdict'\n",
    "gt['verdict'] = gt['verdict'].replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f037fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_train = gt[gt['origin'] == 'train'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "2b4665ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_framework =  pd.read_csv('vecstore_reconstructed_in_csv/our_framework_memory_restored.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a534539f",
   "metadata": {},
   "source": [
    "#  0) Number of wrong used facts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d14c06",
   "metadata": {},
   "source": [
    "### 0.0 Auditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "30ac0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactAuditVerdict(BaseModel):\n",
    "    verdict: Literal[\"CORRECT\", \"INCORRECT\"] = Field(\n",
    "        description=\"Classify strictly as CORRECT if the fact is scientifically accurate, true, and represents generalizable knowledge. Classify as INCORRECT if it contains scientific errors, hallucinations, is harmful, or represents overly specific/non-generalizable information.\"\n",
    "    )\n",
    "    \n",
    "# Instanciar LLM e Prompt\n",
    "auditor = ChatOpenAI(\n",
    "    model=\"gpt-5-mini-2025-08-07\",\n",
    "    temperature=0.1,\n",
    ").with_structured_output(FactAuditVerdict)\n",
    "\n",
    "audit_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a rigorous scientific fact auditor. Your task is to evaluate whether a scientific fact is:\n",
    "    \n",
    "1. CORRECT: The fact is scientifically accurate, true, and represents generalizable scientific knowledge that can be applied broadly.\n",
    "2. INCORRECT: The fact contains scientific inaccuracies, hallucinations, harmful content, or overly specific/context-dependent information.\n",
    "\n",
    "Output instructions: \n",
    "    - Only output the classification as CORRECT or INCORRECT. Do not provide explanations or justifications, or poncturation.\n",
    "     \n",
    "Apply strict scientific standards. When in doubt, prefer marking as INCORRECT.\"\"\"),\n",
    "    (\"human\", \"Evaluate this scientific fact:\\n\\n{fact}\")\n",
    "])\n",
    "\n",
    "audit_chain = audit_prompt | auditor\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def audit_missing_facts(df, audit_chain, fact_col='retrieved_fact', verdict_col='is_fact_correct'):\n",
    "    \"\"\"\n",
    "    Identifica fatos sem veredito em um DataFrame, executa a auditoria via LLM,\n",
    "    salva os resultados de forma otimizada e realiza uma verifica√ß√£o de sanidade.\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # 1. TRATAMENTO DE FATOS VAZIOS: Marca como 'NO_FACT' as linhas onde a extra√ß√£o falhou\n",
    "    mask_sem_fato = df_result[fact_col].isna() | (df_result[fact_col] == '')\n",
    "    df_result.loc[mask_sem_fato, verdict_col] = 'NO_FACT'\n",
    "    \n",
    "    # 2. IDENTIFICA√á√ÉO: Filtra o que realmente precisa ser auditado\n",
    "    mask_nao_auditados = df_result[verdict_col].isna()\n",
    "    fatos_unicos = df_result.loc[mask_nao_auditados, fact_col].dropna().unique()\n",
    "    \n",
    "    if len(fatos_unicos) == 0:\n",
    "        print(\"‚úì Nenhum fato novo pendente de auditoria.\")\n",
    "        return df_result\n",
    "        \n",
    "    print(f\"Iniciando auditoria de {len(fatos_unicos)} fatos √∫nicos...\")\n",
    "    audit_results = {}\n",
    "    \n",
    "    # 3. AUDITORIA: Loop com a LLM\n",
    "    pbar = tqdm(fatos_unicos, desc=\"Auditando\")\n",
    "    for fato in pbar:\n",
    "        try:\n",
    "            resultado = audit_chain.invoke({\"fact\": fato})\n",
    "            audit_results[fato] = resultado.verdict\n",
    "            pbar.set_postfix({\"Verdict\": resultado.verdict})\n",
    "        except Exception as e:\n",
    "            audit_results[fato] = None\n",
    "            pbar.set_postfix({\"Erro\": \"Falha na API\"})\n",
    "\n",
    "    # 4. ATUALIZA√á√ÉO SEGURA NO DATAFRAME\n",
    "    resultados_validos = {fato: veredito for fato, veredito in audit_results.items() if veredito is not None}\n",
    "    \n",
    "    if resultados_validos:\n",
    "        # Mapeia os resultados encontrados para as linhas correspondentes\n",
    "        novos_vereditos = df_result.loc[mask_nao_auditados, fact_col].map(resultados_validos)\n",
    "        \n",
    "        # Preenche o DataFrame apenas onde a API retornou sucesso\n",
    "        df_result.loc[mask_nao_auditados, verdict_col] = novos_vereditos.fillna(df_result.loc[mask_nao_auditados, verdict_col])\n",
    "        \n",
    "        # 5. PROVA REAL (Sanity Check)\n",
    "        for fato_auditado, veredito_esperado in resultados_validos.items():\n",
    "            linhas_do_fato = df_result[df_result[fact_col] == fato_auditado]\n",
    "            if not linhas_do_fato.empty:\n",
    "                veredito_no_df = linhas_do_fato.iloc[0][verdict_col]\n",
    "                assert veredito_no_df == veredito_esperado, f\"ERRO: Fato '{fato_auditado[:30]}...' n√£o foi salvo corretamente!\"\n",
    "                \n",
    "        print(\"\\n‚úì Sanity Check: Todos os resultados v√°lidos da API foram salvos no DataFrame com sucesso.\")\n",
    "\n",
    "    # 6. ESTAT√çSTICAS R√ÅPIDAS\n",
    "    total = len(df_result)\n",
    "    corretos = (df_result[verdict_col] == 'CORRECT').sum()\n",
    "    incorretos = (df_result[verdict_col] == 'INCORRECT').sum()\n",
    "    sem_fatos = (df_result[verdict_col] == 'NO_FACT').sum()\n",
    "    pendentes = df_result[verdict_col].isna().sum()\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"RESUMO: Total de Linhas: {total} | ‚úÖ Corretos: {corretos} | ‚ùå Incorretos: {incorretos} | üì≠ Sem Fato: {sem_fatos} | ‚ö†Ô∏è Pendentes: {pendentes}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cea947",
   "metadata": {},
   "source": [
    "## A) Our Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8fb48f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_framework_audicted = pd.read_csv(\"our_framework_audicted_facts.csv\")\n",
    "\n",
    "of_test = pd.read_csv(\"hybrid_refinement_cognitive_test_var_07.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "a0634ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_facts(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    facts = re.findall(r'\\*\\s*Fact:\\s*\"{1,2}(.*?)\"{1,2}', text)\n",
    "    return facts\n",
    "\n",
    "of_test['retrieved_fact'] = of_test['retrieved_context'].apply(extract_facts)\n",
    "df_expanded = of_test.explode('retrieved_fact').reset_index(drop=True)\n",
    "# df_expanded.dropna(subset=['retrieved_fact'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "32173190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se cada fato recuperado √© correto ou incorreto segundo o audit\n",
    "df_expanded['is_fact_correct'] = None\n",
    "for index, row in df_expanded.iterrows():\n",
    "    retrieved_fact = df_expanded.at[index, 'retrieved_fact']\n",
    "    if pd.notna(retrieved_fact) and retrieved_fact in our_framework_audicted['scientific_fact'].values:\n",
    "        verdict = our_framework_audicted[our_framework_audicted['scientific_fact'] == retrieved_fact]['verdict'].values[0]\n",
    "        df_expanded.at[index, 'is_fact_correct'] = verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "5e218ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando auditoria de 2 fatos √∫nicos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc949dc7102d46cab24b4d0b3993347b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Auditando:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Sanity Check: Todos os resultados v√°lidos da API foram salvos no DataFrame com sucesso.\n",
      "--------------------------------------------------------------------------------\n",
      "RESUMO: Total de Linhas: 3511 | ‚úÖ Corretos: 2365 | ‚ùå Incorretos: 1144 | üì≠ Sem Fato: 2 | ‚ö†Ô∏è Pendentes: 0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_expanded = audit_missing_facts(df_expanded, audit_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "0195f048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AN√ÅLISE DOS FATOS RECUPERADOS NO TESTE\n",
      "================================================================================\n",
      "\n",
      "Total de fatos recuperados (linhas): 3511\n",
      "Fatos auditados: 3511 (100.00%)\n",
      "Fatos n√£o auditados: 0 (0.00%)\n",
      "\n",
      "Distribui√ß√£o dos fatos auditados:\n",
      "is_fact_correct\n",
      "CORRECT      2365\n",
      "INCORRECT    1144\n",
      "NO_FACT         2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dos fatos auditados:\n",
      "  - Corretos: 2365 (67.40%)\n",
      "  - Incorretos: 1144 (32.60%)\n"
     ]
    }
   ],
   "source": [
    "# Estat√≠sticas sobre os fatos recuperados e sua corre√ß√£o\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISE DOS FATOS RECUPERADOS NO TESTE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_fatos = len(df_expanded)\n",
    "fatos_auditados = df_expanded['is_fact_correct'].notna().sum()\n",
    "fatos_nao_auditados = df_expanded['is_fact_correct'].isna().sum()\n",
    "\n",
    "print(f\"\\nTotal de fatos recuperados (linhas): {total_fatos}\")\n",
    "print(f\"Fatos auditados: {fatos_auditados} ({fatos_auditados/total_fatos*100:.2f}%)\")\n",
    "print(f\"Fatos n√£o auditados: {fatos_nao_auditados} ({fatos_nao_auditados/total_fatos*100:.2f}%)\")\n",
    "\n",
    "if fatos_auditados > 0:\n",
    "    print(\"\\nDistribui√ß√£o dos fatos auditados:\")\n",
    "    print(df_expanded['is_fact_correct'].value_counts(dropna=False))\n",
    "    \n",
    "    corretos = (df_expanded['is_fact_correct'] == 'CORRECT').sum()\n",
    "    incorretos = (df_expanded['is_fact_correct'] == 'INCORRECT').sum()\n",
    "    \n",
    "    if corretos + incorretos > 0:\n",
    "        print(f\"\\nDos fatos auditados:\")\n",
    "        print(f\"  - Corretos: {corretos} ({corretos/(corretos+incorretos)*100:.2f}%)\")\n",
    "        print(f\"  - Incorretos: {incorretos} ({incorretos/(corretos+incorretos)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6e8f5",
   "metadata": {},
   "source": [
    "## B) Semantic Base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "842b9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic = pd.read_csv(\"semantic_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "da64926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_knowledge(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # In semantic_test.csv, the facts are presented as \"Knowledge: [fact]\"\n",
    "    facts = re.findall(r'Knowledge:\\s*(.*?)(?=\\n|$)', text)\n",
    "    return [f.strip() for f in facts if f.strip()]\n",
    "\n",
    "semantic['retrieved_fact'] = semantic['retrieved_context'].apply(extract_knowledge)\n",
    "df_expanded = semantic.explode('retrieved_fact').reset_index(drop=True)\n",
    "df_expanded.dropna(subset=['retrieved_fact'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d737e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se cada fato recuperado √© correto ou incorreto segundo o audit\n",
    "df_expanded['is_fact_correct'] = None\n",
    "for index, row in df_expanded.iterrows():\n",
    "    retrieved_fact = df_expanded.at[index, 'retrieved_fact']\n",
    "    if pd.notna(retrieved_fact) and retrieved_fact in gt['fact_clean'].values:\n",
    "        verdict = gt[gt['fact_clean'] == retrieved_fact]['verdict'].values[0]\n",
    "        df_expanded.at[index, 'is_fact_correct'] = verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "202e7629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Nenhum fato novo pendente de auditoria.\n"
     ]
    }
   ],
   "source": [
    "df_expanded = audit_missing_facts(df_expanded, audit_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "68b7f3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AN√ÅLISE DOS FATOS RECUPERADOS NO TESTE\n",
      "================================================================================\n",
      "\n",
      "Total de fatos recuperados (linhas): 3467\n",
      "Fatos auditados: 3467 (100.00%)\n",
      "Fatos n√£o auditados: 0 (0.00%)\n",
      "\n",
      "Distribui√ß√£o dos fatos auditados:\n",
      "is_fact_correct\n",
      "CORRECT      1871\n",
      "INCORRECT    1596\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dos fatos auditados:\n",
      "  - Corretos: 1871 (53.97%)\n",
      "  - Incorretos: 1596 (46.03%)\n"
     ]
    }
   ],
   "source": [
    "# Estat√≠sticas sobre os fatos recuperados e sua corre√ß√£o\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISE DOS FATOS RECUPERADOS NO TESTE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_fatos = len(df_expanded)\n",
    "fatos_auditados = df_expanded['is_fact_correct'].notna().sum()\n",
    "fatos_nao_auditados = df_expanded['is_fact_correct'].isna().sum()\n",
    "\n",
    "print(f\"\\nTotal de fatos recuperados (linhas): {total_fatos}\")\n",
    "print(f\"Fatos auditados: {fatos_auditados} ({fatos_auditados/total_fatos*100:.2f}%)\")\n",
    "print(f\"Fatos n√£o auditados: {fatos_nao_auditados} ({fatos_nao_auditados/total_fatos*100:.2f}%)\")\n",
    "\n",
    "if fatos_auditados > 0:\n",
    "    print(\"\\nDistribui√ß√£o dos fatos auditados:\")\n",
    "    print(df_expanded['is_fact_correct'].value_counts(dropna=False))\n",
    "    \n",
    "    corretos = (df_expanded['is_fact_correct'] == 'CORRECT').sum()\n",
    "    incorretos = (df_expanded['is_fact_correct'] == 'INCORRECT').sum()\n",
    "    \n",
    "    if corretos + incorretos > 0:\n",
    "        print(f\"\\nDos fatos auditados:\")\n",
    "        print(f\"  - Corretos: {corretos} ({corretos/(corretos+incorretos)*100:.2f}%)\")\n",
    "        print(f\"  - Incorretos: {incorretos} ({incorretos/(corretos+incorretos)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05481691",
   "metadata": {},
   "source": [
    "## C) Semantic Cognitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "d831f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_cognitive_test = pd.read_csv('semantic_cognitive_test.csv')\n",
    "\n",
    "def extract_facts(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # In semantic_refinement_cognitive_test.csv, the facts are presented as \"Fact: [fact]\"\n",
    "    facts = re.findall(r'Fact:\\s*(.*?)(?=\\n|$)', text)\n",
    "    return [f.strip() for f in facts if f.strip()]\n",
    "\n",
    "semantic_cognitive_test['retrieved_fact'] = semantic_cognitive_test['retrieved_context'].apply(extract_facts)\n",
    "df_expanded = semantic_cognitive_test.explode('retrieved_fact').reset_index(drop=True)\n",
    "# df_expanded.dropna(subset=['retrieved_fact'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "9c0ea801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se cada fato recuperado √© correto ou incorreto segundo o audit\n",
    "df_expanded['is_fact_correct'] = None\n",
    "for index, row in df_expanded.iterrows():\n",
    "    retrieved_fact = df_expanded.at[index, 'retrieved_fact']\n",
    "    if pd.notna(retrieved_fact) and retrieved_fact in gt['fact_clean'].values:\n",
    "        verdict = gt[gt['fact_clean'] == retrieved_fact]['verdict'].values[0]\n",
    "        df_expanded.at[index, 'is_fact_correct'] = verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "ca638310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Nenhum fato novo pendente de auditoria.\n"
     ]
    }
   ],
   "source": [
    "df_expanded = audit_missing_facts(df_expanded, audit_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0e1ba9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AN√ÅLISE DOS FATOS RECUPERADOS NO TESTE\n",
      "================================================================================\n",
      "\n",
      "Total de fatos recuperados (linhas): 3514\n",
      "Fatos auditados: 3514 (100.00%)\n",
      "Fatos n√£o auditados: 0 (0.00%)\n",
      "\n",
      "Distribui√ß√£o dos fatos auditados:\n",
      "is_fact_correct\n",
      "CORRECT      2278\n",
      "INCORRECT    1236\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dos fatos auditados:\n",
      "  - Corretos: 2278 (64.83%)\n",
      "  - Incorretos: 1236 (35.17%)\n"
     ]
    }
   ],
   "source": [
    "# Estat√≠sticas sobre os fatos recuperados e sua corre√ß√£o\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISE DOS FATOS RECUPERADOS NO TESTE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_fatos = len(df_expanded)\n",
    "fatos_auditados = df_expanded['is_fact_correct'].notna().sum()\n",
    "fatos_nao_auditados = df_expanded['is_fact_correct'].isna().sum()\n",
    "\n",
    "print(f\"\\nTotal de fatos recuperados (linhas): {total_fatos}\")\n",
    "print(f\"Fatos auditados: {fatos_auditados} ({fatos_auditados/total_fatos*100:.2f}%)\")\n",
    "print(f\"Fatos n√£o auditados: {fatos_nao_auditados} ({fatos_nao_auditados/total_fatos*100:.2f}%)\")\n",
    "\n",
    "if fatos_auditados > 0:\n",
    "    print(\"\\nDistribui√ß√£o dos fatos auditados:\")\n",
    "    print(df_expanded['is_fact_correct'].value_counts(dropna=False))\n",
    "    \n",
    "    corretos = (df_expanded['is_fact_correct'] == 'CORRECT').sum()\n",
    "    incorretos = (df_expanded['is_fact_correct'] == 'INCORRECT').sum()\n",
    "    \n",
    "    if corretos + incorretos > 0:\n",
    "        print(f\"\\nDos fatos auditados:\")\n",
    "        print(f\"  - Corretos: {corretos} ({corretos/(corretos+incorretos)*100:.2f}%)\")\n",
    "        print(f\"  - Incorretos: {incorretos} ({incorretos/(corretos+incorretos)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c131fc61",
   "metadata": {},
   "source": [
    "## D) Semantic Cognitive Refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "5c12ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('semantic_refinement_cognitive_test.csv')\n",
    "\n",
    "def extract_facts(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # Extract everything after \"Fact:\" until the end of the line\n",
    "    facts = re.findall(r'Fact:\\s*(.*)', text)\n",
    "    return [f.strip() for f in facts if f.strip()]\n",
    "\n",
    "df['retrieved_fact'] = df['retrieved_context'].apply(extract_facts)\n",
    "df_expanded = df.explode('retrieved_fact').reset_index(drop=True)\n",
    "df_expanded.dropna(subset=['retrieved_fact'], inplace=True) # Aqui retiramos que tinha contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "26a085b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se cada fato recuperado √© correto ou incorreto segundo o audit\n",
    "df_expanded['is_fact_correct'] = None\n",
    "for index, row in df_expanded.iterrows():\n",
    "    retrieved_fact = df_expanded.at[index, 'retrieved_fact']\n",
    "    if pd.notna(retrieved_fact) and retrieved_fact in gt['fact_clean'].values:\n",
    "        verdict = gt[gt['fact_clean'] == retrieved_fact]['verdict'].values[0]\n",
    "        df_expanded.at[index, 'is_fact_correct'] = verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "d43b048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando auditoria de 141 fatos √∫nicos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa1f41f4fcd41ce8f88c9c1608c7c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Auditando:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Sanity Check: Todos os resultados v√°lidos da API foram salvos no DataFrame com sucesso.\n",
      "--------------------------------------------------------------------------------\n",
      "RESUMO: Total de Linhas: 3509 | ‚úÖ Corretos: 2365 | ‚ùå Incorretos: 1144 | üì≠ Sem Fato: 0 | ‚ö†Ô∏è Pendentes: 0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_expanded = audit_missing_facts(df_expanded, audit_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "f2221cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ESTAT√çSTICAS FINAIS - SEMANTIC COGNITIVE REFINED (AP√ìS AUDITORIA GPT)\n",
      "================================================================================\n",
      "\n",
      "Total de fatos recuperados: 3509\n",
      "\n",
      "Distribui√ß√£o final:\n",
      "  - CORRECT: 2365 (67.40%)\n",
      "  - INCORRECT: 1144 (32.60%)\n",
      "  - N√£o auditados: 0 (0.00%)\n",
      "\n",
      "Dos fatos auditados:\n",
      "  - Corretos: 2365 (67.40%)\n",
      "  - Incorretos: 1144 (32.60%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Estat√≠sticas finais ap√≥s auditoria completa\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTAT√çSTICAS FINAIS - SEMANTIC COGNITIVE REFINED (AP√ìS AUDITORIA GPT)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_fatos = len(df_expanded)\n",
    "corretos = (df_expanded['is_fact_correct'] == 'CORRECT').sum()\n",
    "incorretos = (df_expanded['is_fact_correct'] == 'INCORRECT').sum()\n",
    "nao_auditados = df_expanded['is_fact_correct'].isna().sum()\n",
    "\n",
    "print(f\"\\nTotal de fatos recuperados: {total_fatos}\")\n",
    "print(f\"\\nDistribui√ß√£o final:\")\n",
    "print(f\"  - CORRECT: {corretos} ({corretos/total_fatos*100:.2f}%)\")\n",
    "print(f\"  - INCORRECT: {incorretos} ({incorretos/total_fatos*100:.2f}%)\")\n",
    "print(f\"  - N√£o auditados: {nao_auditados} ({nao_auditados/total_fatos*100:.2f}%)\")\n",
    "\n",
    "if corretos + incorretos > 0:\n",
    "    print(f\"\\nDos fatos auditados:\")\n",
    "    print(f\"  - Corretos: {corretos} ({corretos/(corretos+incorretos)*100:.2f}%)\")\n",
    "    print(f\"  - Incorretos: {incorretos} ({incorretos/(corretos+incorretos)*100:.2f}%)\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b5130",
   "metadata": {},
   "source": [
    "## E) Concatenated Cognitive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d8dac782",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_cognitive_reflec_test = pd.read_csv('semantic_cog_reflec_test.csv')\n",
    "\n",
    "def extract_facts(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # Match * Fact: \"...\" or \"\"...\"\"\n",
    "    facts = re.findall(r'\\*\\s*Fact:\\s*\"{1,2}(.*?)\"{1,2}', text)\n",
    "    return [f.strip() for f in facts if f.strip()]\n",
    "\n",
    "semantic_cognitive_reflec_test['retrieved_fact'] = semantic_cognitive_reflec_test['retrieved_context'].apply(extract_facts)\n",
    "df_expanded = semantic_cognitive_reflec_test.explode('retrieved_fact').reset_index(drop=True)\n",
    "# df_expanded.dropna(subset=['retrieved_fact'], inplace=True)\n",
    "\n",
    "# Verificar se cada fato recuperado √© correto ou incorreto segundo o audit\n",
    "df_expanded['is_fact_correct'] = None\n",
    "for index, row in df_expanded.iterrows():\n",
    "    retrieved_fact = df_expanded.at[index, 'retrieved_fact']\n",
    "    if pd.notna(retrieved_fact) and retrieved_fact in gt['fact_clean'].values:\n",
    "        verdict = gt[gt['fact_clean'] == retrieved_fact]['verdict'].values[0]\n",
    "        df_expanded.at[index, 'is_fact_correct'] = verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "0a63d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_expanded = audit_missing_facts(df_expanded, audit_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "5da2854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AN√ÅLISE DOS FATOS RECUPERADOS NO TESTE\n",
      "================================================================================\n",
      "\n",
      "Total de fatos recuperados (linhas): 3477\n",
      "Fatos auditados: 3463 (99.60%)\n",
      "Fatos n√£o auditados: 14 (0.40%)\n",
      "\n",
      "Distribui√ß√£o dos fatos auditados:\n",
      "is_fact_correct\n",
      "CORRECT      2179\n",
      "INCORRECT    1284\n",
      "None           14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dos fatos auditados:\n",
      "  - Corretos: 2179 (62.92%)\n",
      "  - Incorretos: 1284 (37.08%)\n"
     ]
    }
   ],
   "source": [
    "# Estat√≠sticas sobre os fatos recuperados e sua corre√ß√£o\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISE DOS FATOS RECUPERADOS NO TESTE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_fatos = len(df_expanded)\n",
    "fatos_auditados = df_expanded['is_fact_correct'].notna().sum()\n",
    "fatos_nao_auditados = df_expanded['is_fact_correct'].isna().sum()\n",
    "\n",
    "print(f\"\\nTotal de fatos recuperados (linhas): {total_fatos}\")\n",
    "print(f\"Fatos auditados: {fatos_auditados} ({fatos_auditados/total_fatos*100:.2f}%)\")\n",
    "print(f\"Fatos n√£o auditados: {fatos_nao_auditados} ({fatos_nao_auditados/total_fatos*100:.2f}%)\")\n",
    "\n",
    "if fatos_auditados > 0:\n",
    "    print(\"\\nDistribui√ß√£o dos fatos auditados:\")\n",
    "    print(df_expanded['is_fact_correct'].value_counts(dropna=False))\n",
    "    \n",
    "    corretos = (df_expanded['is_fact_correct'] == 'CORRECT').sum()\n",
    "    incorretos = (df_expanded['is_fact_correct'] == 'INCORRECT').sum()\n",
    "    \n",
    "    if corretos + incorretos > 0:\n",
    "        print(f\"\\nDos fatos auditados:\")\n",
    "        print(f\"  - Corretos: {corretos} ({corretos/(corretos+incorretos)*100:.2f}%)\")\n",
    "        print(f\"  - Incorretos: {incorretos} ({incorretos/(corretos+incorretos)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9660aa3",
   "metadata": {},
   "source": [
    "### F) Concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e8685",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate = pd.read_csv('hybrid_test.csv')\n",
    "\n",
    "def extract_knowledge(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # Extracting things after 'Knowledge:'\n",
    "    facts = re.findall(r'Knowledge:\\s*(.*?)(?=\\n|$)', text)\n",
    "    return [f.strip().strip('\"\\'') for f in facts if f.strip()]\n",
    "\n",
    "concatenate['retrieved_fact'] = concatenate['retrieved_context'].apply(extract_knowledge)\n",
    "df_expanded = concatenate.explode('retrieved_fact').reset_index(drop=True)\n",
    "df_expanded.dropna(subset=['retrieved_fact'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "e90f39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded['is_fact_correct'] = None\n",
    "for index, row in df_expanded.iterrows():\n",
    "    retrieved_fact = df_expanded.at[index, 'retrieved_fact']\n",
    "    if pd.notna(retrieved_fact) and retrieved_fact in gt['fact_clean'].values:\n",
    "        verdict = gt[gt['fact_clean'] == retrieved_fact]['verdict'].values[0]\n",
    "        df_expanded.at[index, 'is_fact_correct'] = verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "e2169581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Nenhum fato novo pendente de auditoria.\n"
     ]
    }
   ],
   "source": [
    "df_expanded = audit_missing_facts(df_expanded, audit_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "9f38a8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AN√ÅLISE DOS FATOS RECUPERADOS NO TESTE\n",
      "================================================================================\n",
      "\n",
      "Total de fatos recuperados (linhas): 3509\n",
      "Fatos auditados: 3509 (100.00%)\n",
      "Fatos n√£o auditados: 0 (0.00%)\n",
      "\n",
      "Distribui√ß√£o dos fatos auditados:\n",
      "is_fact_correct\n",
      "CORRECT      1899\n",
      "INCORRECT    1610\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dos fatos auditados:\n",
      "  - Corretos: 1899 (54.12%)\n",
      "  - Incorretos: 1610 (45.88%)\n"
     ]
    }
   ],
   "source": [
    "# Estat√≠sticas sobre os fatos recuperados e sua corre√ß√£o\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISE DOS FATOS RECUPERADOS NO TESTE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_fatos = len(df_expanded)\n",
    "fatos_auditados = df_expanded['is_fact_correct'].notna().sum()\n",
    "fatos_nao_auditados = df_expanded['is_fact_correct'].isna().sum()\n",
    "\n",
    "print(f\"\\nTotal de fatos recuperados (linhas): {total_fatos}\")\n",
    "print(f\"Fatos auditados: {fatos_auditados} ({fatos_auditados/total_fatos*100:.2f}%)\")\n",
    "print(f\"Fatos n√£o auditados: {fatos_nao_auditados} ({fatos_nao_auditados/total_fatos*100:.2f}%)\")\n",
    "\n",
    "if fatos_auditados > 0:\n",
    "    print(\"\\nDistribui√ß√£o dos fatos auditados:\")\n",
    "    print(df_expanded['is_fact_correct'].value_counts(dropna=False))\n",
    "    \n",
    "    corretos = (df_expanded['is_fact_correct'] == 'CORRECT').sum()\n",
    "    incorretos = (df_expanded['is_fact_correct'] == 'INCORRECT').sum()\n",
    "    \n",
    "    if corretos + incorretos > 0:\n",
    "        print(f\"\\nDos fatos auditados:\")\n",
    "        print(f\"  - Corretos: {corretos} ({corretos/(corretos+incorretos)*100:.2f}%)\")\n",
    "        print(f\"  - Incorretos: {incorretos} ({incorretos/(corretos+incorretos)*100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
