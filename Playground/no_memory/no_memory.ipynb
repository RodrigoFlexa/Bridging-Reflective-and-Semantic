{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f287465",
   "metadata": {},
   "source": [
    "# 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07afc1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import List, Optional\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "# Add vectorstore & prompts folders to path\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Local imports\n",
    "from utils_notebook import (\n",
    "    format_choices,\n",
    "    make_question\n",
    ")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d07445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['A','B','C','D']\n",
    "number = {'A':0,'B':1,'C':2,'D':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f292f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset do arc. (Caso não tenha é necessário instalar e guardar na pasta datasets como csv)\n",
    "# train_df = pd.read_csv(\"dataset/arc_challenge_train_processed.csv\")\n",
    "valid_df = pd.read_csv(\"../../dataset/arc_challenge_valid_processed.csv\")\n",
    "test_df = pd.read_csv(\"../../dataset/arc_challenge_test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c0f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt5_nano = ChatOpenAI(model='gpt-5-nano-2025-08-07', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1d9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi2 = ChatOllama(model=\"phi\", temperature=0) # phi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb5d32",
   "metadata": {},
   "source": [
    "### C) Prompt and Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30a140dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_memory_template = \"\"\"Instruction: You are an expert science tutor. Your goal is to answer the target multiple-choice question below.\n",
    "\n",
    "Apply relevant scientific principles - fundamental definitions and laws from your knowledge.\n",
    "\n",
    "Use scientific principles to ground your facts and guide your logic.\n",
    "\n",
    "Structure your response strictly as:\n",
    "1. **Reasoning:** Explain the step-by-step logic to reach the correct answer.\n",
    "2. **Principles:** List the scientific concepts applied.\n",
    "3. **Answer:** State only the correct option letter (A, B, C, or D).\n",
    "\n",
    "### TARGET QUESTION:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "no_memory_prompt = PromptTemplate.from_template(no_memory_template)\n",
    "no_memory_chain = no_memory_prompt | phi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92384120",
   "metadata": {},
   "source": [
    "# 1) Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc16d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(\n",
    "    small_llm_model,\n",
    "    model_text_output: str,\n",
    "    valid_labels: List[str] = None,\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extração robusta baseada em padrões de alta confiança\n",
    "    com fallback para LLM Juiz em caso de falha.\n",
    "    \"\"\"\n",
    "    if valid_labels is None:\n",
    "        valid_labels = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "    if not model_text_output:\n",
    "        return \"N/A\"\n",
    "\n",
    "    text = model_text_output.strip()\n",
    "\n",
    "    if \"```python\" in text or \"def solution\" in text:\n",
    "        if debug: print(\"⚠ [CODE DETECTED] Enviando para LLM Judge.\")\n",
    "        return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "    strong_patterns = [\n",
    "        r\"\\\\boxed\\s*\\{\\s*([A-H])\\s*\\}\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]?\\s*(?:is)?\\s*(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]]\",\n",
    "        r\"The\\s+(?:correct\\s+)?(?:answer|option|choice)\\s+is\\s*(?:Option)?\\s*[:\\-]?\\s*[\\(\\[]([A-H])[\\)\\]]\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]\\s*(?:is\\s+)?(?:Option\\s+)?([A-H])(?=\\s|\\.|,|!|\\?|$)\",\n",
    "        r\"(?:Therefore|Thus|Hence|So),\\s*(?:the\\s+answer\\s+is\\s*)?(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]]\",\n",
    "        r\"(?:^|\\n)\\s*Answer\\s*:\\s*([A-H])(?=\\s|$|\\.|\\,)\",\n",
    "        r\"\\*\\*([A-H])\\*\\*\",\n",
    "        r\"^([A-H])$\"\n",
    "]\n",
    "    \n",
    "    for pattern in strong_patterns:\n",
    "        matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "        if matches:\n",
    "            candidate = matches[-1].group(1).upper()\n",
    "            if candidate in valid_labels:\n",
    "                if debug: print(f\"✓ [REGEX] Padrão: '{pattern}' -> {candidate}\")\n",
    "                return candidate\n",
    "\n",
    "    if debug: print(\"✗ [REGEX] Falha. Chamando LLM Judge.\")\n",
    "    return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "\n",
    "def _llm_judge(\n",
    "    small_llm_model,\n",
    "    text: str,\n",
    "    valid_labels: List[str],\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    context_block = f\"\\n### CONTEXT:\\n{question}\\n\" if question else \"\"\n",
    "\n",
    "    prompt = f\"\"\"You are an Answer Extraction Bot.\n",
    "Your ONLY job is to identify which option the following \"Model Output\" concluded is correct.\n",
    "{context_block}\n",
    "### Model Output to Analyze:\n",
    "{text}\n",
    "\n",
    "### Instructions:\n",
    "1. Look for an explicit answer (e.g., \"Answer: A\").\n",
    "2. If NO explicit letter is found, infer from the conclusion and the Context options.\n",
    "3. Do NOT solve the problem yourself. Trust the \"Model Output\".\n",
    "4. If the model refuses to answer or is unclear, return \"E\".\n",
    "\n",
    "Output format: Just the single letter (A, B, C, D, or E). No other text.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = small_llm_model.invoke(prompt)\n",
    "        output = response.content if hasattr(response, 'content') else str(response)\n",
    "        clean_cand = re.sub(r\"[^A-E]\", \"\", output.strip().upper())\n",
    "        if len(clean_cand) > 1:\n",
    "            clean_cand = clean_cand[-1]\n",
    "        if clean_cand in valid_labels:\n",
    "            if debug: print(f\"✓ [LLM JUIZ] Inferido: {clean_cand}\")\n",
    "            return clean_cand\n",
    "        return \"N/A\"\n",
    "    except Exception as e:\n",
    "        if debug: print(f\"✗ [ERRO JUIZ] {e}\")\n",
    "        return \"N/A\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ddfe21",
   "metadata": {},
   "source": [
    "# 2) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32a3a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_dataset_no_memory(\n",
    "    df,\n",
    "    chain=no_memory_chain,\n",
    "    desc=\"Avaliando sem Memória Externa\",\n",
    "    full_prompt=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    full_prompt=True  → passa a questão com as instruções do template.\n",
    "    full_prompt=False → passa somente a questão diretamente ao LLM (sem template).\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    acertos = 0\n",
    "    total = len(df)\n",
    "    erros = 0\n",
    "    loop = tqdm(df.iterrows(), total=total, desc=desc)\n",
    "\n",
    "    for idx, row in loop:\n",
    "        try:\n",
    "            full_question = make_question(row, inline=False)[0]\n",
    "\n",
    "            if full_prompt:\n",
    "                response_obj = chain.invoke({\"question\": full_question})\n",
    "            else:\n",
    "                response_obj = phi2.invoke(full_question)\n",
    "\n",
    "            response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "\n",
    "            pred = extract_answer(\n",
    "                small_llm_model=gpt5_nano,\n",
    "                model_text_output=response_text,\n",
    "                question=full_question,\n",
    "            )\n",
    "            if pred not in LABELS:\n",
    "                pred = 'E'\n",
    "                erros += 1\n",
    "\n",
    "            target = row['answerKey']\n",
    "            is_correct = (pred == target)\n",
    "            acertos += int(is_correct)\n",
    "\n",
    "            acc_atual = (acertos / (loop.n + 1)) * 100\n",
    "            loop.set_postfix(acc=f\"{acc_atual:.2f}%\", last=pred, erros=erros)\n",
    "\n",
    "            resultados.append({\n",
    "                'index': idx,\n",
    "                'question': full_question,\n",
    "                'raw_output': response_text,\n",
    "                'pred': pred,\n",
    "                'target': target,\n",
    "                'is_correct': is_correct,\n",
    "                'source': desc\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Erro no índice {idx}: {e}\")\n",
    "            resultados.append({\n",
    "                'index': idx,\n",
    "                'error': str(e),\n",
    "                'is_correct': False,\n",
    "                'source': desc\n",
    "            })\n",
    "\n",
    "    df_res = pd.DataFrame(resultados)\n",
    "    print(f\"\\n✅ {desc} — {acertos}/{total} acertos ({(acertos/total)*100:.2f}%)\")\n",
    "    return df_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b88dcb",
   "metadata": {},
   "source": [
    "# 3) Running\n",
    "\n",
    "## 3.1) With instructions (full prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba26ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset Validação: 100%|██████████| 299/299 [14:04<00:00,  2.82s/it, acc=69.57%, erros=21, last=A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Dataset Validação finalizado com 208/299 acertos (69.57%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid_with_prompt = avaliar_dataset_no_memory(\n",
    "    df=valid_df,\n",
    "    chain=no_memory_chain,\n",
    "    desc=\"Validação (with prompt)\",\n",
    "    full_prompt=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac5cd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset Teste: 100%|██████████| 1172/1172 [1:55:08<00:00,  5.89s/it, acc=65.96%, erros=86, last=B] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Dataset Teste finalizado com 773/1172 acertos (65.96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_with_prompt = avaliar_dataset_no_memory(\n",
    "    df=test_df,\n",
    "    chain=no_memory_chain,\n",
    "    desc=\"Teste (with prompt)\",\n",
    "    full_prompt=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb83937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_with_prompt.to_csv(\"no_memory_test.csv\", index=False)\n",
    "df_valid_with_prompt.to_csv(\"no_memory_valid.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
