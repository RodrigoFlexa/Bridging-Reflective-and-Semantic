{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8266045a",
   "metadata": {},
   "source": [
    "# Hybrid Memory System (Semantic + Reflection) with Cognitive Scoring + Fact Generation & Refinement\n",
    "\n",
    "**Fases:**\n",
    "- **Validation**: Update de scores (EMA), decay, **fact generation**\n",
    "- **Teste**: Freeze Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa3d087",
   "metadata": {},
   "source": [
    "# 0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ac431",
   "metadata": {},
   "source": [
    "### A) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10beb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "# Add vectorstore & prompts folders to path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'vectorstore')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'prompts')))\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Memory managers & metadata builders\n",
    "from cognitive_memory_manager import CognitiveMemoryManager, build_metadata_semantic\n",
    "from simple_vector_memory import SimpleVectorMemory, build_metadata_reflection\n",
    "\n",
    "# Prompt templates\n",
    "from templates import (\n",
    "    HYBRID_TEMPLATE,\n",
    "    SCORE_TEMPLATE,\n",
    "    EXTRACT_FACTS_TEMPLATE,\n",
    "    REFINE_FACT_TEMPLATE,\n",
    ")\n",
    "\n",
    "# # Local imports\n",
    "from utils_notebook import (\n",
    "    SemanticCleaner,\n",
    "    calcular_metricas_memoria,\n",
    "    format_choices,\n",
    "    parse_simple_score,\n",
    "    make_question\n",
    ")\n",
    "cleaner = SemanticCleaner()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f7102",
   "metadata": {},
   "source": [
    "### B) Language Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4432b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['A','B','C','D']\n",
    "number = {'A':0,'B':1,'C':2,'D':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a36875f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc0344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset do arc\n",
    "valid_df = pd.read_csv(\"../../dataset/arc_challenge_valid_processed.csv\")\n",
    "test_df = pd.read_csv(\"../../dataset/arc_challenge_test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f74751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM judge\n",
    "gpt5_nano = ChatOpenAI(model='gpt-5-nano-2025-08-07',temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb7caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi2 = ChatOllama(model=\"phi\", temperature=0)\n",
    "phi2_creative = ChatOllama(model=\"phi\", temperature=0.6)  # for fact generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e674601",
   "metadata": {},
   "source": [
    "### C) Prompts and Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd8c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_prompt = PromptTemplate.from_template(HYBRID_TEMPLATE)\n",
    "rag_chain = hybrid_prompt | phi2\n",
    "\n",
    "score_prompt = PromptTemplate.from_template(SCORE_TEMPLATE)\n",
    "score_chain = score_prompt | phi2 | StrOutputParser()\n",
    "\n",
    "extract_facts_prompt = PromptTemplate.from_template(EXTRACT_FACTS_TEMPLATE)\n",
    "extract_facts_chain = extract_facts_prompt | gpt5_nano | StrOutputParser()\n",
    "\n",
    "refine_fact_prompt = PromptTemplate.from_template(REFINE_FACT_TEMPLATE)\n",
    "refine_fact_chain = refine_fact_prompt | phi2_creative | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c1f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_organizer(fact, temperature=0.1):\n",
    "    \"\"\"Limpa e resume um fato cient√≠fico.\"\"\"\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Extract ONLY one objective scientific sentence from the text below. \n",
    "NO introductions, NO explanations, NO conversational filler.\n",
    "\n",
    "Text: \"{fact.strip()}\"\n",
    "Scientific Fact (Write it in only one sentence):\"\"\"\n",
    "    \n",
    "    model = ChatOllama(model=\"phi\", temperature=temperature)\n",
    "    fact_cleaned = model.invoke(prompt).content.strip().strip('\"').strip(\"'\")\n",
    "    fact_cleaned = cleaner.clean(fact_cleaned)\n",
    "    return fact_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af97ae",
   "metadata": {},
   "source": [
    "# 1) Utils Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738c627",
   "metadata": {},
   "source": [
    "### A) Context Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a6c2dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_hybrid_context(semantic_items, reflection_items, show_scores=False):\n",
    "    formatted_parts = []\n",
    "    \n",
    "    if semantic_items:\n",
    "        formatted_parts.append(\"### PART 1: SCIENTIFIC PRINCIPLES (THEORY)\")\n",
    "        for i, fact in enumerate(semantic_items, 1):\n",
    "            raw_q = fact['metadata'].get('question', '')\n",
    "            q_text = raw_q.split('\\n')[0].strip()\n",
    "            f_text = fact.get('content', '').strip()\n",
    "            \n",
    "            if show_scores:\n",
    "                sim = fact.get('similarity', 0)\n",
    "                mem = fact.get('memory_strength', 0)\n",
    "                final = fact.get('final_score', 0)\n",
    "                block = f\"\"\"\n",
    "    * **Principle #{i}** (Sim: {sim:.2f}, Mem: {mem:.2f}, Final: {final:.2f})\n",
    "        Context: \"{q_text}\"\n",
    "        Fact: \"{f_text}\"\n",
    "\"\"\"\n",
    "            else:\n",
    "                block = f\"\"\"\n",
    "    * **Principle #{i}**\n",
    "        * Context: \"{q_text}\"\n",
    "        * Fact: \"{f_text}\"\n",
    "\"\"\"\n",
    "            formatted_parts.append(block)\n",
    "\n",
    "    if reflection_items:\n",
    "        formatted_parts.append(\"\\n### PART 2: SOLVED EXAMPLES (PRACTICE)\")\n",
    "        for i, reflection in enumerate(reflection_items, 1):\n",
    "            raw_q = reflection['metadata'].get('question', '')\n",
    "            q_text = raw_q.split('\\n')[0].strip()\n",
    "            r_text = reflection.get('content', '').strip()\n",
    "            \n",
    "            block = f\"\"\"\n",
    "    * **Example Case #{i}**\n",
    "        > Context: {q_text}\n",
    "        > Analysis: {r_text}\n",
    "\"\"\"\n",
    "            formatted_parts.append(block)\n",
    "    \n",
    "    if not formatted_parts:\n",
    "        return \"No specific reference information found.\"\n",
    "    \n",
    "    return \"\\n\".join(formatted_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13296074",
   "metadata": {},
   "source": [
    "### B) Answer Extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69b4b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Essa fun√ß√£o deve posteriormente ser dividida em fun√ß√µes menores, cada uma respons√°vel por um aspecto espec√≠fico do processo de formata√ß√£o.\n",
    "# TODO Migrar a fun√ß√£o para um arquivo utils\n",
    "\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "def extract_answer(\n",
    "    small_llm_model, \n",
    "    model_text_output: str, \n",
    "    valid_labels: List[str] = None,\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extra√ß√£o robusta baseada em padr√µes de alta confian√ßa (Tier 1) \n",
    "    com fallback para LLM Juiz em caso de falha.\n",
    "    \"\"\"\n",
    "    if valid_labels is None:\n",
    "        valid_labels = ['A', 'B', 'C', 'D', 'E']\n",
    "    \n",
    "    if not model_text_output:\n",
    "        return \"N/A\"\n",
    "    \n",
    "    text = model_text_output.strip()\n",
    "\n",
    "    if \"```python\" in text or \"def solution\" in text:\n",
    "        if debug: print(\"‚ö† [CODE DETECTED] Enviando para LLM Judge.\")\n",
    "        return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "    strong_patterns = [\n",
    "        r\"\\\\boxed\\s*\\{\\s*([A-H])\\s*\\}\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]?\\s*(?:is)?\\s*(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]](?![a-z])\",\n",
    "        r\"The\\s+(?:correct\\s+)?(?:answer|option|choice)\\s+is\\s*(?:Option)?\\s*[:\\-]?\\s*[\\(\\[]([A-H])[\\)\\]](?![a-z])\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]\\s*(?:is\\s+)?(?:Option\\s+)?([A-H])(?=\\s|\\.|,|!|\\?|$)(?![A-Za-z])\",\n",
    "        r\"(?:Therefore|Thus|Hence|So),\\s*(?:the\\s+answer\\s+is\\s*)?(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]](?![a-z])\",\n",
    "        r\"(?:^|\\n)\\s*Answer\\s*:\\s*([A-H])(?=\\s|$|\\.|\\,)(?![A-Za-z])\",\n",
    "        r\"\\*\\*([A-H])\\*\\*(?![a-z])\",\n",
    "        r\"^([A-H])$\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in strong_patterns:\n",
    "        matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "        if matches:\n",
    "            # Pega a √∫ltima ocorr√™ncia desse padr√£o espec√≠fico\n",
    "            last_match = matches[-1]\n",
    "            candidate = last_match.group(1).upper()\n",
    "            \n",
    "            if candidate in valid_labels:\n",
    "                if debug: \n",
    "                    print(f\"‚úì [REGEX] Padr√£o encontrado: '{pattern}' -> {candidate}\")\n",
    "                return candidate\n",
    "\n",
    "    #  FALLBACK: LLM \n",
    "    # Se nenhum padr√£o forte foi encontrado\n",
    "    # Deixe o LLM ler e decidir.\n",
    "    \n",
    "    if debug: print(\"‚úó [REGEX] Falha nos padr√µes fortes. Chamando LLM Judge.\")\n",
    "    return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "\n",
    "def _llm_judge(\n",
    "    small_llm_model, \n",
    "    text: str, \n",
    "    valid_labels: List[str],\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Juiz focado em INFER√äNCIA. Ele deve ler o racioc√≠nio e mapear para a letra correta,\n",
    "    mesmo que o modelo n√£o tenha dito a letra explicitamente.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Monta contexto apenas se a pergunta existir\n",
    "    context_block = \"\"\n",
    "    if question:\n",
    "        context_block = f\"\"\"\n",
    "### CONTEXT (Use this to infer the answer letter):\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are an Answer Extraction Bot. \n",
    "Your ONLY job is to identify which option the following \"Model Output\" concluded is correct.\n",
    "\n",
    "{context_block}\n",
    "\n",
    "### Model Output to Analyze:\n",
    "{text}\n",
    "\n",
    "### Instructions:\n",
    "1. Look for an explicit answer (e.g., \"Answer: A\").\n",
    "2. If NO explicit letter is found, read the conclusion of the \"Model Output\" and match it against the Options in the Context. **Infer the letter.**\n",
    "   - Example: If Context has \"(A) 5 (B) 10\" and Model Output says \"The result is 10\", you must output B.\n",
    "3. Do NOT calculate or solve the problem yourself. Trust the \"Model Output\".\n",
    "4. If the model refuses to answer or is unclear, return \"E\".\n",
    "\n",
    "Output format: Just the single letter (A, B, C, D, or E). No other text.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = small_llm_model.invoke(prompt) \n",
    "        output = response.content if hasattr(response, 'content') else str(response)\n",
    "        clean_cand = re.sub(r\"[^A-E]\", \"\", output.strip().upper())\n",
    "        \n",
    "        if len(clean_cand) > 1:\n",
    "            clean_cand = clean_cand[-1]\n",
    "            \n",
    "        if clean_cand in valid_labels:\n",
    "            if debug: print(f\"‚úì [LLM JUIZ] Inferido: {clean_cand}\")\n",
    "            return clean_cand\n",
    "            \n",
    "        return \"N/A\"\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug: print(f\"‚úó [ERRO JUIZ] {e}\")\n",
    "        return \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa518c30",
   "metadata": {},
   "source": [
    "### C) Check Fact Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82d426b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_invalid(question, fact, original_fact, check_size=True, check_content=True, \n",
    "               check_similarity=True, min_similarity=0.2, return_reason=False):\n",
    "    \"\"\"\n",
    "    Valida se um fato cient√≠fico gerado √© inv√°lido.\n",
    "    Inclui verifica√ß√£o de drift sem√¢ntico via Similaridade de Cosseno.\n",
    "    \"\"\"\n",
    "    if not fact or not isinstance(fact, str):\n",
    "        return (True, \"Fato vazio ou inv√°lido\") if return_reason else True\n",
    "\n",
    "    fact_lower = fact.lower()\n",
    "    word_count = len(fact.split())\n",
    "    \n",
    "    # 1. Valida√ß√µes de Estrutura\n",
    "    if check_size:\n",
    "        size_checks = [\n",
    "            (word_count > 100, f\"Muito longo ({word_count} tokens)\"),\n",
    "            (word_count < 10, f\"Muito curto ({word_count} tokens)\"),\n",
    "            ('\\n' in fact, \"Cont√©m quebras de linha\"),\n",
    "            ('#' in fact or '_' in fact, \"Cont√©m caracteres especiais (#/_)\"),\n",
    "            (fact_lower == original_fact.lower(), \"Id√™ntico ao original\")\n",
    "        ]\n",
    "        for condition, reason in size_checks:\n",
    "            if condition: return (True, reason) if return_reason else True\n",
    "\n",
    "    # 2. Valida√ß√µes de Conte√∫do\n",
    "    if check_content:\n",
    "        forbidden_terms = ['score', 'correct', 'answer', 'instruction:', 'critique:']\n",
    "        for term in forbidden_terms:\n",
    "            if term in fact_lower:\n",
    "                return (True, f\"Cont√©m termo proibido: {term}\") if return_reason else True\n",
    "\n",
    "    # 3. Valida√ß√£o de Similaridade\n",
    "    if check_similarity:\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(stop_words=None)\n",
    "            tfidf = vectorizer.fit_transform([fact_lower, question.lower()])\n",
    "            sim = cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]\n",
    "            \n",
    "            if sim < min_similarity:\n",
    "                reason = f\"Baixa similaridade ({sim:.2f}). Poss√≠vel alucina√ß√£o.\"\n",
    "                return (True, reason) if return_reason else True\n",
    "        except Exception as e:\n",
    "            return (True, \"Erro no c√°lculo de similaridade\") if return_reason else True\n",
    "\n",
    "    return (False, None) if return_reason else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73978d3b",
   "metadata": {},
   "source": [
    "### D) Score Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3780413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_score_new_facts(question, response_text, is_correct, pred, target, row, manager):\n",
    "    \"\"\"\n",
    "    Extrai fatos cient√≠ficos da resposta do modelo, atribui scores e salva imediatamente.\n",
    "    \n",
    "    Returns:\n",
    "        List[dict]: Lista de fatos com scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Extra√ß√£o de fatos com GPT-5 nano\n",
    "        facts_text = extract_facts_chain.invoke({\n",
    "            \"model_response\": response_text,\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        # Parse fatos (separados por linha)\n",
    "        raw_facts = [f.strip() for f in facts_text.split('\\n') if f.strip()]\n",
    "        if not raw_facts:\n",
    "            return []\n",
    "        \n",
    "        # Limitar a 2 fatos\n",
    "        raw_facts = raw_facts[:2]\n",
    "        \n",
    "        # 2. Limpar fatos\n",
    "        cleaned_facts = []\n",
    "        for fact in raw_facts:\n",
    "            cleaned = cleaner.clean(fact)\n",
    "            if cleaned and len(cleaned.split()) >= 10:  # M√≠nimo 10 palavras\n",
    "                cleaned_facts.append(cleaned)\n",
    "        \n",
    "        if not cleaned_facts:\n",
    "            return []\n",
    "        \n",
    "        # 3. Calcular scores para cada fato\n",
    "        scored_facts = []\n",
    "        choices = ast.literal_eval(row['choices'])\n",
    "        \n",
    "        # Prote√ß√£o contra √≠ndices inv√°lidos\n",
    "        try:\n",
    "            correct_txt = choices['text'][number[target]] if target in number else str(target)\n",
    "        except (IndexError, KeyError):\n",
    "            correct_txt = str(target)\n",
    "        \n",
    "        try:\n",
    "            chosen_txt = choices['text'][number[pred]] if pred in number else str(pred)\n",
    "        except (IndexError, KeyError):\n",
    "            chosen_txt = str(pred)\n",
    "        \n",
    "        status = \"SUCCESS\" if is_correct else \"FAILURE\"\n",
    "        \n",
    "        for fact in cleaned_facts:\n",
    "            try:\n",
    "                score_response = score_chain.invoke({\n",
    "                    \"question\": question,\n",
    "                    \"chosen\": chosen_txt,\n",
    "                    \"correct\": correct_txt,\n",
    "                    \"outcome_status\": status,\n",
    "                    \"fact\": fact,\n",
    "                    \"reasoning\": response_text\n",
    "                })\n",
    "                \n",
    "                reflection, score = parse_simple_score(score_response)\n",
    "                \n",
    "                scored_facts.append({\n",
    "                    'content': fact,\n",
    "                    'score': score,\n",
    "                    'reflection': reflection,\n",
    "                    'original_id': str(row['id']),\n",
    "                    'question': question,\n",
    "                    'correct_answer': target\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erro ao calcular score: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 4. Salvar fatos no vectorstore\n",
    "        if scored_facts:\n",
    "            new_memory_entries = []\n",
    "            for fact_data in scored_facts:\n",
    "                new_memory_entries.append({\n",
    "                    'content': fact_data['content'],\n",
    "                    'metadata': {\n",
    "                        'original_id': fact_data['original_id'],\n",
    "                        'question': fact_data['question'],\n",
    "                        'correct_answer': fact_data['correct_answer'],\n",
    "                        'score': fact_data['score'],\n",
    "                        'frequency': 0,\n",
    "                        'scientific_fact': fact_data['content'],\n",
    "                        'origin': 'validation_generated',\n",
    "                        'type': 'semantic_memory',\n",
    "                        'initial_score': fact_data['score']\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            manager.add_new_memories(new_memory_entries, id_prefix='generated')\n",
    "        \n",
    "        return scored_facts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro na extra√ß√£o de fatos: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a59422",
   "metadata": {},
   "source": [
    "### E) Fact Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84503fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_negative_facts(used_facts, is_correct, question, pred, target, row, manager, \n",
    "                          refine_threshold=0.3, verbose=False):\n",
    "    \"\"\"\n",
    "    ‚ú® NEW: Refina fatos recuperados que contribu√≠ram para erro.\n",
    "    Aplica apenas quando houve falha (is_correct = False).\n",
    "    \n",
    "    Args:\n",
    "        refine_threshold: Score abaixo do qual um fato ser√° refinado (default: 0.3)\n",
    "        verbose: Se True, mostra informa√ß√µes de debug\n",
    "    \n",
    "    Returns:\n",
    "        int: N√∫mero de fatos refinados\n",
    "    \"\"\"\n",
    "    if is_correct or not used_facts:\n",
    "        return 0\n",
    "    \n",
    "    refined_count = 0\n",
    "    candidates_count = 0\n",
    "    choices = ast.literal_eval(row['choices'])\n",
    "    \n",
    "    # Prote√ß√£o contra √≠ndices inv√°lidos\n",
    "    try:\n",
    "        correct_txt = choices['text'][number[target]] if target in number else str(target)\n",
    "    except (IndexError, KeyError):\n",
    "        correct_txt = str(target)\n",
    "    \n",
    "    try:\n",
    "        wrong_txt = choices['text'][number[pred]] if pred in number else str(pred)\n",
    "    except (IndexError, KeyError):\n",
    "        wrong_txt = str(pred)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüîç Analisando {len(used_facts)} fatos para refinamento (threshold: {refine_threshold})...\")\n",
    "    \n",
    "    for memory in used_facts:\n",
    "        fact_text = memory['content']\n",
    "        memory_score = memory.get('memory_strength', 0.0)\n",
    "        \n",
    "        # Refina apenas fatos com score baixo (< refine_threshold)\n",
    "        if memory_score >= refine_threshold:\n",
    "            if verbose:\n",
    "                print(f\"   ‚äó Fato ignorado (score {memory_score:.2f} >= {refine_threshold})\")\n",
    "            continue\n",
    "        \n",
    "        candidates_count += 1\n",
    "        if verbose:\n",
    "            print(f\"   ‚úì Candidato #{candidates_count} (score: {memory_score:.2f})\")\n",
    "        \n",
    "        try:\n",
    "            # Gerar reflex√£o/cr√≠tica sobre o fato\n",
    "            score_response = score_chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"chosen\": wrong_txt,\n",
    "                \"correct\": correct_txt,\n",
    "                \"outcome_status\": \"FAILURE\",\n",
    "                \"fact\": fact_text,\n",
    "                \"reasoning\": f\"Used fact led to wrong answer: {wrong_txt}\"\n",
    "            })\n",
    "            \n",
    "            reflection, _ = parse_simple_score(score_response)\n",
    "            \n",
    "            # Tentar gerar fato refinado\n",
    "            max_attempts = 3\n",
    "            for attempt in range(max_attempts):\n",
    "                new_fact_text = refine_fact_chain.invoke({\n",
    "                    \"question\": question,\n",
    "                    \"correct_answer\": correct_txt,\n",
    "                    \"old_fact\": fact_text,\n",
    "                    \"reflection\": reflection,\n",
    "                    \"wrong_answer\": wrong_txt\n",
    "                })\n",
    "                \n",
    "                clean_new_fact = call_organizer(new_fact_text, temperature=0.1)\n",
    "                \n",
    "                # Validar fato refinado\n",
    "                q_c = f\"{question} {correct_txt}\"\n",
    "                if not is_invalid(question=q_c, fact=clean_new_fact, original_fact=fact_text, \n",
    "                                check_size=True, check_content=True, check_similarity=True):\n",
    "                    # Salvar fato refinado\n",
    "                    manager.add_new_memories([{\n",
    "                        'content': clean_new_fact,\n",
    "                        'metadata': {\n",
    "                            'original_id': str(row['id']),\n",
    "                            'question': question,\n",
    "                            'correct_answer': target,\n",
    "                            'score': 0,  # Score neutro inicial\n",
    "                            'frequency': 0,\n",
    "                            'scientific_fact': clean_new_fact,\n",
    "                            'origin': 'validation_refined',\n",
    "                            'type': 'semantic_memory',\n",
    "                            'initial_score': 0,\n",
    "                            'refined_from': fact_text\n",
    "                        }\n",
    "                    }], id_prefix='refined')\n",
    "                    \n",
    "                    refined_count += 1\n",
    "                    if verbose:\n",
    "                        print(f\"   ‚úÖ Fato refinado com sucesso (tentativa {attempt+1})\")\n",
    "                    break\n",
    "                elif verbose and attempt == max_attempts - 1:\n",
    "                    print(f\"   ‚ùå Falha ap√≥s {max_attempts} tentativas\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"   ‚ö†Ô∏è Erro ao refinar fato: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìä Resultado: {candidates_count} candidatos, {refined_count} refinados\\n\")\n",
    "    \n",
    "    return refined_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c2c9e",
   "metadata": {},
   "source": [
    "### D) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1c0091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_dataset_cognitive_plus(\n",
    "    df, \n",
    "    chain=rag_chain,\n",
    "    manager_semantic=None,\n",
    "    manager_reflection=None,  \n",
    "    score_chain=score_chain,\n",
    "    k_semantic=3,\n",
    "    k_reflection=3,  #  Quantidade de mem√≥rias reflexivas\n",
    "    threshold_semantic=0.24,\n",
    "    threshold_reflection=0.24,  #  Threshold para reflex√£o\n",
    "    semantic_weight=0.7,\n",
    "    update_memory=False,\n",
    "    decay_batch_size=False,\n",
    "    decay_frequency=50,\n",
    "    extract_new_facts=False,\n",
    "    refine_facts=False,\n",
    "    refine_threshold=0.3,\n",
    "    refine_verbose=False,\n",
    "    backup_frequency=160,\n",
    "    backup_path=None,\n",
    "    desc=\"Hybrid Memory System PLUS\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Avalia dataset com sistema h√≠brido (sem√¢ntica + reflex√£o) + gera√ß√£o/refinamento.\n",
    "    \n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    acertos = 0\n",
    "    total = len(df)\n",
    "    decay_counter = 0\n",
    "    backup_counter = 0\n",
    "    \n",
    "    # Contadores de gera√ß√£o/refinamento\n",
    "    total_facts_generated = 0\n",
    "    total_facts_refined = 0\n",
    "    total_facts_errors = 0\n",
    "    \n",
    "    # Acumuladores globais\n",
    "    global_sims_sem = []\n",
    "    global_sims_ref = []\n",
    "    global_counts_sem = []\n",
    "    global_counts_ref = []\n",
    "    \n",
    "    erros = 0\n",
    "\n",
    "    # Preparar diret√≥rio de backup\n",
    "    if backup_path:\n",
    "        backup_dir = os.path.dirname(backup_path)\n",
    "        if backup_dir and not os.path.exists(backup_dir):\n",
    "            os.makedirs(backup_dir, exist_ok=True)\n",
    "    \n",
    "    loop = tqdm(df.iterrows(), total=total, desc=desc)\n",
    "\n",
    "    for idx, row in loop:\n",
    "        try:\n",
    "            full_question = make_question(row, inline=False)[0]\n",
    "            question_for_retriever = re.sub(r'\\([A-Z]\\)\\s*', '', row['question'])\n",
    "            \n",
    "            # Recuperar mem√≥rias sem√¢nticas (COM SCORE COGNITIVO)\n",
    "            items_semantic, _ = manager_semantic.retrieve_ranked_memories(\n",
    "                query=question_for_retriever, \n",
    "                k=k_semantic,\n",
    "                threshold=threshold_semantic,\n",
    "                semantic_weight=semantic_weight,\n",
    "                show_scores=False\n",
    "            )\n",
    "            \n",
    "            # Recuperar mem√≥rias reflexivas (SEM SCORE - recupera√ß√£o simples)\n",
    "            items_reflection = []\n",
    "            if manager_reflection:\n",
    "                items_reflection = manager_reflection.retrieve_memories(\n",
    "                    query=question_for_retriever, \n",
    "                    k=k_reflection,\n",
    "                    threshold=threshold_reflection\n",
    "                )\n",
    "            \n",
    "            # NEW: Combinar contextos (h√≠brido)\n",
    "            hybrid_context_formatted = format_hybrid_context(items_semantic, items_reflection)\n",
    "\n",
    "            count_sem, avg_sim_sem, raw_sims_sem = calcular_metricas_memoria(items_semantic)\n",
    "            count_ref, avg_sim_ref, raw_sims_ref = calcular_metricas_memoria(items_reflection)\n",
    "            \n",
    "            global_counts_sem.append(count_sem)\n",
    "            global_counts_ref.append(count_ref)\n",
    "            global_sims_sem.extend(raw_sims_sem)\n",
    "            global_sims_ref.extend(raw_sims_ref)\n",
    "\n",
    "            response_obj = chain.invoke({\n",
    "                \"question\": full_question,\n",
    "                \"hybrid_context\": hybrid_context_formatted,  # ‚ú® Mudan√ßa de similar_facts para hybrid_context\n",
    "            })\n",
    "\n",
    "            response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "            \n",
    "            pred = extract_answer(\n",
    "                small_llm_model=gpt5_nano, \n",
    "                model_text_output=response_text,\n",
    "                question=full_question,\n",
    "            )\n",
    "\n",
    "            if pred not in ['A', 'B', 'C', 'D']:\n",
    "                pred = 'E'\n",
    "                erros += 1\n",
    "            \n",
    "            target = row['answerKey']\n",
    "\n",
    "            is_correct = (pred == target)\n",
    "            acertos += int(is_correct)\n",
    "            \n",
    "            # === EXTRA√á√ÉO DE NOVOS FATOS ===\n",
    "            if extract_new_facts and pred != 'E':\n",
    "                try:\n",
    "                    new_facts = extract_and_score_new_facts(\n",
    "                        question=full_question,\n",
    "                        response_text=response_text,\n",
    "                        is_correct=is_correct,\n",
    "                        pred=pred,\n",
    "                        target=target,\n",
    "                        row=row,\n",
    "                        manager=manager_semantic\n",
    "                    )\n",
    "                    if new_facts:\n",
    "                        total_facts_generated += len(new_facts)\n",
    "                    else:\n",
    "                        total_facts_errors += 1\n",
    "                except Exception as e:\n",
    "                    total_facts_errors += 1\n",
    "                    tqdm.write(f\"‚ùå Q{idx+1}: Erro na extra√ß√£o de fatos: {e}\")\n",
    "            \n",
    "            # === UPDATE DE MEM√ìRIA EXISTENTE ===\n",
    "            if update_memory and items_semantic and pred != 'E':\n",
    "                used_ids = [m['doc_id'] for m in items_semantic]\n",
    "                \n",
    "                # Calcular feedback scores\n",
    "                feedback_scores = []\n",
    "                choices = ast.literal_eval(row['choices'])\n",
    "                \n",
    "                try:\n",
    "                    correct_txt = choices['text'][number[target]] if target in number else str(target)\n",
    "                except (IndexError, KeyError):\n",
    "                    correct_txt = str(target)\n",
    "                \n",
    "                try:\n",
    "                    chosen_txt = choices['text'][number[pred]] if pred in number else str(pred)\n",
    "                except (IndexError, KeyError):\n",
    "                    chosen_txt = str(pred)\n",
    "                \n",
    "                status = \"SUCCESS\" if is_correct else \"FAILURE\"\n",
    "                \n",
    "                for memory in items_semantic:\n",
    "                    fact_text = memory['content']\n",
    "                    \n",
    "                    try:\n",
    "                        score_response = score_chain.invoke({\n",
    "                            \"question\": full_question,\n",
    "                            \"chosen\": chosen_txt,\n",
    "                            \"correct\": correct_txt,\n",
    "                            \"outcome_status\": status,\n",
    "                            \"fact\": fact_text,\n",
    "                            \"reasoning\": response_text\n",
    "                        })\n",
    "                        \n",
    "                        _, score = parse_simple_score(score_response)\n",
    "                        feedback_scores.append(score)\n",
    "                    except:\n",
    "                        feedback_scores.append(0)\n",
    "                \n",
    "                # Aplicar update EMA\n",
    "                manager_semantic.update_memories_feedback(used_ids, feedback_scores)\n",
    "            \n",
    "            # === REFINAMENTO DE FATOS ===\n",
    "            if refine_facts and not is_correct:\n",
    "                refined = refine_negative_facts(\n",
    "                    used_facts=items_semantic,\n",
    "                    is_correct=is_correct,\n",
    "                    question=full_question,\n",
    "                    pred=pred,\n",
    "                    target=target,\n",
    "                    row=row,\n",
    "                    manager=manager_semantic,\n",
    "                    refine_threshold=refine_threshold,\n",
    "                    verbose=refine_verbose\n",
    "                )\n",
    "                total_facts_refined += refined\n",
    "            \n",
    "            # === DECAY ===\n",
    "            if update_memory and decay_frequency > 0 and (idx + 1) % decay_frequency == 0:\n",
    "                decay_counter += 1\n",
    "                tqdm.write(f\"üåô [Q{idx+1}/{total}] Aplicando ciclo de decay #{decay_counter}...\")\n",
    "                manager_semantic.apply_decay_cycle(decay_threshold=0.0)\n",
    "            \n",
    "            # === BACKUP  ===\n",
    "            if backup_path and backup_frequency > 0 and (idx + 1) % backup_frequency == 0:\n",
    "                backup_counter += 1\n",
    "                df_partial = pd.DataFrame(resultados)\n",
    "                timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                backup_file = backup_path.replace('.csv', f'_backup.csv')\n",
    "                df_partial.to_csv(backup_file, index=False)\n",
    "            \n",
    "            # Atualiza barra de progresso\n",
    "            acc_atual = (acertos / (loop.n + 1)) * 100\n",
    "            loop.set_postfix(\n",
    "                acc=f\"{acc_atual:.2f}%\", \n",
    "                sem=count_sem,\n",
    "                ref=count_ref,  # ‚ú® NEW\n",
    "                decay=decay_counter,\n",
    "                gen=total_facts_generated,\n",
    "                refine=total_facts_refined,\n",
    "                err=total_facts_errors,\n",
    "                bkp=backup_counter,\n",
    "                erros=erros\n",
    "            )\n",
    "\n",
    "            resultados.append({\n",
    "                'index': idx,\n",
    "                'question': full_question,\n",
    "                'retrieved_context': hybrid_context_formatted,  # ‚ú® Contexto h√≠brido\n",
    "                'retrieved_count_semantic': count_sem,\n",
    "                'retrieved_count_reflection': count_ref,  # ‚ú® NEW\n",
    "                'avg_similarity_semantic': avg_sim_sem,\n",
    "                'avg_similarity_reflection': avg_sim_ref,  # ‚ú® NEW\n",
    "                'raw_output': response_text,\n",
    "                'pred': pred,\n",
    "                'target': target,\n",
    "                'is_correct': is_correct,\n",
    "                'source': desc,\n",
    "                'erros': erros\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Erro no √≠ndice {idx}: {e}\")\n",
    "            resultados.append({\n",
    "                'index': idx, \n",
    "                'error': str(e), \n",
    "                'is_correct': False, \n",
    "                'retrieved_count_semantic': 0,\n",
    "                'retrieved_count_reflection': 0, \n",
    "                'avg_similarity_semantic': 0.0,\n",
    "                'avg_similarity_reflection': 0.0,\n",
    "            })\n",
    "    \n",
    "    # === DECAY FINAL ===\n",
    "    if update_memory and decay_batch_size:\n",
    "        decay_counter += 1\n",
    "        print(f\"\\nüåô [FINAL] Aplicando ciclo de decay final (#{decay_counter})...\")\n",
    "        manager_semantic.apply_decay_cycle(decay_threshold=0.0)\n",
    "    \n",
    "    # Sum√°rio final\n",
    "    if update_memory:\n",
    "        print(f\"\\nüìä SUM√ÅRIO DE MEM√ìRIA:\")\n",
    "        print(f\"   Ciclos de decay: {decay_counter}\")\n",
    "        print(f\"   Fatos gerados: {total_facts_generated}\")\n",
    "        print(f\"   Fatos refinados: {total_facts_refined}\")\n",
    "        print(f\"   Erros na extra√ß√£o: {total_facts_errors}\")\n",
    "        print(f\"   Mem√≥rias sem√¢nticas finais: {manager_semantic._collection.count()}\")\n",
    "        if manager_reflection:\n",
    "            print(f\"   Mem√≥rias reflexivas: {manager_reflection._collection.count()}\")\n",
    "    \n",
    "    if backup_path and backup_counter > 0:\n",
    "        print(f\"üíæ Total de backups salvos: {backup_counter}\")\n",
    "\n",
    "    \n",
    "    if backup_path and backup_counter > 0:    return pd.DataFrame(resultados)\n",
    "\n",
    "    return pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4cc20",
   "metadata": {},
   "source": [
    "# 2) Constru√ß√£o dos Bancos de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d97b02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados\n",
    "scientific_facts = pd.read_csv(\"../../scientific_facts_expanded.csv\").fillna(\"N/A\")\n",
    "\n",
    "df_semantic = scientific_facts[scientific_facts['scientific_fact'] != \"N/A\"].drop_duplicates(subset=['scientific_fact'])\n",
    "df_reflection = scientific_facts[scientific_facts['clean_reasoning'] != \"N/A\"].drop_duplicates(subset=['clean_reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b211ed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Carregando mem√≥ria existente de: vectorstores/hybrid_refinement_var/chroma_semantic\n",
      "‚úÖ Mem√≥ria carregada com 3993 itens.\n",
      "üìÇ Carregando base de reflex√£o de: vectorstores/semantic_refinement_cognitive_aaa/chroma_reflection\n",
      "‚úÖ Base carregada com 1080 itens.\n"
     ]
    }
   ],
   "source": [
    "FORCE_RESET = False\n",
    "\n",
    "manager_semantic = CognitiveMemoryManager(\n",
    "    db_path=\"vectorstores/hybrid_refinement_var/chroma_semantic\",\n",
    "    embedding_model=embedding_model, alpha=0.1, decay_lambda=0.99, forget_threshold=-0.6\n",
    ").init_from_dataframe(df=df_semantic, content_col='scientific_fact',\n",
    "                      id_prefix='semantic', metadata_func=build_metadata_semantic, reset_db=FORCE_RESET)\n",
    "\n",
    "manager_reflection = SimpleVectorMemory(\n",
    "    db_path=\"vectorstores/semantic_refinement_cognitive_aaa/chroma_reflection\",\n",
    "    embedding_model=embedding_model\n",
    ").init_from_dataframe(df=df_reflection, content_col='clean_reasoning',\n",
    "                      id_prefix='reflection', metadata_func=build_metadata_reflection, reset_db=FORCE_RESET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00a5d5",
   "metadata": {},
   "source": [
    "# 3)  Testing in one question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3044aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "k_semantic = 3\n",
    "k_reflection = 3\n",
    "row = test_df.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f80d9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?\n",
      "(A) Planetary density will decrease.\n",
      "(B) Planetary years will become longer.\n",
      "(C) Planetary days will become shorter.\n",
      "(D) Planetary gravity will become stronger.\n",
      "\n",
      "Hybrid Context: \n",
      " ### PART 1: SCIENTIFIC PRINCIPLES (THEORY)\n",
      "\n",
      "    * **Principle #1** (Sim: 0.53, Mem: 1.00, Final: 0.77)\n",
      "        Context: \"Why is it winter in North America when it is summer in South America?\"\n",
      "        Fact: \"Axial precession causes the Earth's axis to change its orientation relative to its orbit around the sun over a period of approximately 26,000 years.\"\n",
      "\n",
      "\n",
      "    * **Principle #2** (Sim: 0.50, Mem: 1.00, Final: 0.75)\n",
      "        Context: \"Michael learned that the movement of Earth in the solar system causes changes that can be seen on the planet. Which change could be seen on Earth in the time it takes Earth to rotate once on its axis?\"\n",
      "        Fact: \"This apparent movement and day-night change are due to Earth's axial tilt and its revolution around the Sun.\"\n",
      "\n",
      "\n",
      "    * **Principle #3** (Sim: 0.48, Mem: 1.00, Final: 0.74)\n",
      "        Context: \"Michael learned that the movement of Earth in the solar system causes changes that can be seen on the planet. Which change could be seen on Earth in the time it takes Earth to rotate once on its axis?\"\n",
      "        Fact: \"The rotation of the Earth causes the apparent movement of celestial bodies across the sky and the changing of day into night.\"\n",
      "\n",
      "\n",
      "### PART 2: SOLVED EXAMPLES (PRACTICE)\n",
      "\n",
      "    * **Example Case #1**\n",
      "        > Context: The Late Heavy Bombardment was a period of extensive comet impact on Earth about 3.8 billion years ago. Scientists believe this period provided much of the matter now found in which part of the Earth system?\n",
      "        > Analysis: The Late Heavy Bombardment was a period when many comets and asteroids collided with Earth, causing significant changes to the planet's surface. These impacts would have released large amounts of matter into the Earth's system.\n",
      "\n",
      "\n",
      "    * **Example Case #2**\n",
      "        > Context: If a solid object is taken from Earth far into space, which of the following measurements of the object will change most?\n",
      "        > Analysis: The property that would change the most when a solid object is taken from Earth into space is its weight, as it depends on gravity. In space, there is no significant gravitational force acting on the object, so its weight will be negligible compared to its mass and volume.\n",
      "\n",
      "\n",
      "    * **Example Case #3**\n",
      "        > Context: How long does it take for Earth to rotate on its axis seven times?\n",
      "        > Analysis: The rotation of the Earth is a fundamental concept in astronomy and physics. It takes approximately 24 hours for the Earth to complete one full rotation on its axis, which we refer to as a day. If we want to find out how long it would take for the Earth to rotate seven times, we can simply multiply this time by 7.\n",
      "\n",
      "\n",
      "M√©tricas Sem√¢nticas: Count: 3 AVG_SIM: 0.5050206383069357\n",
      "M√©tricas Reflexivas: Count: 3 AVG_SIM: 0.4185266097386678\n"
     ]
    }
   ],
   "source": [
    "q_text = make_question(row, inline=False)[0]\n",
    "question_clean = re.sub(r'\\([A-Z]\\)\\s*', '', q_text)\n",
    "\n",
    "# Recuperar mem√≥rias sem√¢nticas (com score cognitivo)\n",
    "items_semantic, _ = manager_semantic.retrieve_ranked_memories(\n",
    "    query=question_clean, \n",
    "    k=k_semantic, \n",
    "    threshold=0.33,\n",
    "    semantic_weight=0.5,\n",
    "    show_scores=True\n",
    ")\n",
    "\n",
    "# ‚ú® NEW: Recuperar mem√≥rias reflexivas (simples)\n",
    "items_reflection = manager_reflection.retrieve_memories(\n",
    "    query=question_clean, \n",
    "    k=k_reflection,\n",
    "    threshold=0.33\n",
    ")\n",
    "\n",
    "# ‚ú® NEW: Combinar contextos\n",
    "hybrid_context = format_hybrid_context(items_semantic, items_reflection, show_scores=True)\n",
    "\n",
    "print(\"Question: \\n\", q_text)\n",
    "print(\"\\nHybrid Context: \\n\", hybrid_context)\n",
    "\n",
    "count_sem, avg_sim_sem, raw_sims_sem = calcular_metricas_memoria(items_semantic)\n",
    "count_ref, avg_sim_ref, raw_sims_ref = calcular_metricas_memoria(items_reflection)\n",
    "print(\"\\nM√©tricas Sem√¢nticas: Count:\", count_sem, \"AVG_SIM:\", avg_sim_sem)\n",
    "print(\"M√©tricas Reflexivas: Count:\", count_ref, \"AVG_SIM:\", avg_sim_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69becdf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectError",
     "evalue": "[WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpcore\\_sync\\connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpcore\\_backends\\sync.py:207\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Usar o contexto h√≠brido (sem mostrar scores para o teste final)\u001b[39;00m\n\u001b[32m      5\u001b[39m hybrid_context_clean = format_hybrid_context(items_semantic, items_reflection, show_scores=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response_obj = \u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhybrid_context\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhybrid_context_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ‚ú® Mudan√ßa: similar_facts -> hybrid_context\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTime to answer: \u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(time.time()-a,\u001b[32m2\u001b[39m))\n\u001b[32m     11\u001b[39m response_text = response_obj.content \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response_obj, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(response_obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3153\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3151\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3152\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3153\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3154\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3155\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\langchain_ollama\\chat_models.py:1030\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1025\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1028\u001b[39m     **kwargs: Any,\n\u001b[32m   1029\u001b[39m ) -> ChatResult:\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1034\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1035\u001b[39m         message=AIMessage(\n\u001b[32m   1036\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1043\u001b[39m         generation_info=generation_info,\n\u001b[32m   1044\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\langchain_ollama\\chat_models.py:965\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    958\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    962\u001b[39m     **kwargs: Any,\n\u001b[32m    963\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    964\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\langchain_ollama\\chat_models.py:1054\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m   1048\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1049\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   1050\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1051\u001b[39m     **kwargs: Any,\n\u001b[32m   1052\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m   1053\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1060\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\langchain_ollama\\chat_models.py:952\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    951\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m    954\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\ollama\\_client.py:174\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m      \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpx\\_client.py:868\u001b[39m, in \u001b[36mClient.stream\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[33;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[33;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    853\u001b[39m \u001b[33;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    855\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    856\u001b[39m     method=method,\n\u001b[32m    857\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    866\u001b[39m     extensions=extensions,\n\u001b[32m    867\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    875\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpx\\_transports\\default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    156\u001b[39m     value = typ()\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\anaconda3\\envs\\agent_lab\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou ativamente"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "a = time.time()\n",
    "# Usar o contexto h√≠brido (sem mostrar scores para o teste final)\n",
    "hybrid_context_clean = format_hybrid_context(items_semantic, items_reflection, show_scores=False)\n",
    "response_obj = rag_chain.invoke({\n",
    "    \"question\": q_text,\n",
    "    \"hybrid_context\": hybrid_context_clean,  # ‚ú® Mudan√ßa: similar_facts -> hybrid_context\n",
    "})\n",
    "print(\"Time to answer: \", round(time.time()-a,2))\n",
    "response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "\n",
    "pred = extract_answer(\n",
    "    small_llm_model=gpt5_nano, \n",
    "    model_text_output=response_text,\n",
    "    question=q_text,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "target = row['answerKey']\n",
    "is_correct = (pred == target)\n",
    "\n",
    "print(\"# Raw Response: \\n\", response_text)\n",
    "print(f\"# Alternative chosen: {pred} (Correta: {target})\")\n",
    "print(f\"# Resultado: {'CORRETO' if is_correct else 'ERRO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Fatos extra√≠dos: 2\n",
      "\n",
      "Fato 1 (Score: 0): The length of a planet's day is determined by its rotation rate; faster rotation yields a shorter day.\n",
      "\n",
      "Fato 2 (Score: 0): Changes in rotation rate occur when external torques act on a body, altering the length of its day.\n"
     ]
    }
   ],
   "source": [
    "# Teste de extra√ß√£o de fatos\n",
    "new_facts = extract_and_score_new_facts(\n",
    "    question=q_text,\n",
    "    response_text=response_text,\n",
    "    is_correct=is_correct,\n",
    "    pred=pred,\n",
    "    target=target,\n",
    "    row=row,\n",
    "    manager=manager_semantic\n",
    ")\n",
    "\n",
    "print(f\"\\n‚ú® Fatos extra√≠dos: {len(new_facts)}\")\n",
    "for i, fact in enumerate(new_facts, 1):\n",
    "    print(f\"\\nFato {i} (Score: {fact['score']}): {fact['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2203aef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Resposta correta - refinamento n√£o aplicado\n"
     ]
    }
   ],
   "source": [
    "# ‚ú® Teste de refinamento de fatos (apenas quando houver erro)\n",
    "if not is_correct:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîß TESTE DE REFINAMENTO DE FATOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    refined_count = refine_negative_facts(\n",
    "        used_facts=items_semantic,\n",
    "        is_correct=is_correct,\n",
    "        question=q_text,\n",
    "        pred=pred,\n",
    "        target=target,\n",
    "        row=row,\n",
    "        manager=manager_semantic,\n",
    "        refine_threshold=0.3,  # Refina fatos com score < 0.3\n",
    "        verbose=True  # Mostra logs detalhados\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚ú® Total de fatos refinados: {refined_count}\")\n",
    "else:\n",
    "    print(\"\\n‚úì Resposta correta - refinamento n√£o aplicado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a51a5",
   "metadata": {},
   "source": [
    "# 4) Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4b2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o (Hybrid Memory + Gen/Refine):  11%|‚ñà         | 32/299 [12:03<1:39:26, 22.35s/it, acc=78.12%, bkp=0, decay=0, err=2, erros=1, gen=48, ref=3, refine=0, sem=3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåô [Q33/299] Aplicando ciclo de decay #1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o (Hybrid Memory + Gen/Refine):  11%|‚ñà         | 33/299 [12:03<1:50:13, 24.86s/it, acc=75.76%, bkp=0, decay=1, err=2, erros=1, gen=50, ref=3, refine=0, sem=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Ciclo de Limpeza: 792 mem√≥rias esquecidas (Score < -0.6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o (Hybrid Memory + Gen/Refine):  22%|‚ñà‚ñà‚ñè       | 65/299 [27:34<1:47:35, 27.59s/it, acc=83.33%, bkp=0, decay=2, err=8, erros=1, gen=94, ref=3, refine=1, sem=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåô [Q66/299] Aplicando ciclo de decay #2...\n",
      "üóëÔ∏è Ciclo de Limpeza: 12 mem√≥rias esquecidas (Score < -0.6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o (Hybrid Memory + Gen/Refine):  33%|‚ñà‚ñà‚ñà‚ñé      | 98/299 [46:39<1:30:25, 26.99s/it, acc=78.79%, bkp=0, decay=3, err=13, erros=1, gen=143, ref=3, refine=2, sem=3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåô [Q99/299] Aplicando ciclo de decay #3...\n",
      "üóëÔ∏è Ciclo de Limpeza: 14 mem√≥rias esquecidas (Score < -0.6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o (Hybrid Memory + Gen/Refine):  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 131/299 [1:02:33<1:09:46, 24.92s/it, acc=78.03%, bkp=1, decay=4, err=14, erros=1, gen=194, ref=3, refine=3, sem=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåô [Q132/299] Aplicando ciclo de decay #4...\n",
      "üóëÔ∏è Ciclo de Limpeza: 15 mem√≥rias esquecidas (Score < -0.6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o (Hybrid Memory + Gen/Refine):  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/299 [1:18:49<32:20, 14.38s/it, acc=79.39%, bkp=1, decay=5, err=18, erros=1, gen=239, ref=3, refine=3, sem=3]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåô [Q165/299] Aplicando ciclo de decay #5...\n",
      "üóëÔ∏è Ciclo de Limpeza: 10 mem√≥rias esquecidas (Score < -0.6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o (Hybrid Memory + Gen/Refine):  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/299 [1:30:44<32:32, 19.14s/it, acc=79.29%, bkp=1, decay=6, err=20, erros=1, gen=294, ref=3, refine=3, sem=3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåô [Q198/299] Aplicando ciclo de decay #6...\n",
      "üóëÔ∏è Ciclo de Limpeza: 13 mem√≥rias esquecidas (Score < -0.6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o (Hybrid Memory + Gen/Refine):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/299 [1:41:19<28:09, 24.48s/it, acc=78.35%, bkp=1, decay=7, err=27, erros=2, gen=335, ref=3, refine=4, sem=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåô [Q231/299] Aplicando ciclo de decay #7...\n",
      "üóëÔ∏è Ciclo de Limpeza: 10 mem√≥rias esquecidas (Score < -0.6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o (Hybrid Memory + Gen/Refine):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/299 [1:52:01<12:39, 21.70s/it, acc=78.79%, bkp=2, decay=8, err=28, erros=2, gen=386, ref=3, refine=4, sem=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåô [Q264/299] Aplicando ciclo de decay #8...\n",
      "üóëÔ∏è Ciclo de Limpeza: 10 mem√≥rias esquecidas (Score < -0.6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o (Hybrid Memory + Gen/Refine):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 296/299 [2:05:06<01:02, 20.69s/it, acc=78.38%, bkp=2, decay=8, err=32, erros=2, gen=431, ref=3, refine=4, sem=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåô [Q297/299] Aplicando ciclo de decay #9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o (Hybrid Memory + Gen/Refine):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/299 [2:05:06<00:41, 20.73s/it, acc=78.45%, bkp=2, decay=9, err=32, erros=2, gen=433, ref=3, refine=4, sem=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Ciclo de Limpeza: 19 mem√≥rias esquecidas (Score < -0.6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o (Hybrid Memory + Gen/Refine): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299/299 [2:05:42<00:00, 25.23s/it, acc=78.26%, bkp=2, decay=9, err=33, erros=2, gen=435, ref=3, refine=4, sem=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåô [FINAL] Aplicando ciclo de decay final (#10)...\n",
      "\n",
      "üìä SUM√ÅRIO DE MEM√ìRIA:\n",
      "   Ciclos de decay: 10\n",
      "   Fatos gerados: 435\n",
      "   Fatos refinados: 4\n",
      "   Erros na extra√ß√£o: 33\n",
      "   Mem√≥rias sem√¢nticas finais: 3559\n",
      "   Mem√≥rias reflexivas: 1080\n",
      "üíæ Total de backups salvos: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# VALIDA√á√ÉO: Mem√≥ria ATIVA + Gera√ß√£o/Refinamento + Recupera√ß√£o de Reflex√µes\n",
    "df_resultado_valid_semantic_plus = avaliar_dataset_cognitive_plus(\n",
    "    df=valid_df, \n",
    "    chain=rag_chain,\n",
    "    manager_semantic=manager_semantic,\n",
    "    manager_reflection=manager_reflection, \n",
    "    k_semantic=3,\n",
    "    k_reflection=3,  \n",
    "    threshold_reflection=0.24,  \n",
    "    semantic_weight=0.7,\n",
    "    update_memory=True,\n",
    "    decay_batch_size=True,\n",
    "    decay_frequency=100,\n",
    "    extract_new_facts=True,\n",
    "    refine_facts=True,\n",
    "    refine_threshold=0.3,\n",
    "    backup_frequency=130,\n",
    "    backup_path=\"../../results/hybrid_refinement_cognitive_valid.csv\",\n",
    "    desc=\"Valida√ß√£o (Hybrid Memory + Gen/Refine)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31b1d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_valid_semantic_plus.to_csv(\"hybrid_refinement_cognitive_valid_var.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4448076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teste (Hybrid Memory Frozen): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1172/1172 [46:43<00:00,  2.39s/it, acc=73.12%, bkp=7, decay=0, err=0, erros=0, gen=0, ref=3, refine=0, sem=3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Total de backups salvos: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TESTE: Mem√≥ria CONGELADA + Recupera√ß√£o de Reflex√µes (apenas recupera√ß√£o)\n",
    "df_resultado_test_semantic_plus07 = avaliar_dataset_cognitive_plus(\n",
    "    df=test_df, \n",
    "    chain=rag_chain,\n",
    "    manager_semantic=manager_semantic,\n",
    "    manager_reflection=manager_reflection,  # ‚ú® NEW: Adiciona mem√≥ria reflexiva\n",
    "    score_chain=score_chain,\n",
    "    k_semantic=3,\n",
    "    k_reflection=3,  # ‚ú® NEW: Quantidade de reflex√µes\n",
    "    threshold_reflection=0.24,  # ‚ú® NEW: Threshold para reflex√µes\n",
    "    semantic_weight=0.7,\n",
    "    update_memory=False,\n",
    "    decay_batch_size=False,\n",
    "    extract_new_facts=False,\n",
    "    refine_facts=False,\n",
    "    backup_frequency=150,\n",
    "    backup_path=\"../../results/hybrid_refinement_cognitive_test.csv\",\n",
    "    desc=\"Teste (Hybrid Memory Frozen)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42da75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_test_semantic_plus07.to_csv(\"hybrid_refinement_cognitive_test_var_07.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f5ac6903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7312286689419796)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultado_test_semantic_plus07['is_correct'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
