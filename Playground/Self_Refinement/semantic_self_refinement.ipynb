{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8266045a",
   "metadata": {},
   "source": [
    "# Semantic Memory System with Cognitive Scoring + Fact Generation & Refinement (PLUS VERSION)\n",
    "\n",
    "**Fases:**\n",
    "- **Valida√ß√£o**: Update de scores (EMA), decay em mem√≥rias fracas, **gera√ß√£o/refinamento de fatos**, salvamento imediato\n",
    "- **Teste**: Mem√≥ria congelada (read-only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa3d087",
   "metadata": {},
   "source": [
    "# 0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcb457",
   "metadata": {},
   "source": [
    "### A) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b10beb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "# Add vectorstore & prompts folders to path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'vectorstore')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'prompts')))\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Memory managers & metadata builders\n",
    "from cognitive_memory_manager import CognitiveMemoryManager, build_metadata_semantic\n",
    "from simple_vector_memory import SimpleVectorMemory, build_metadata_reflection\n",
    "\n",
    "# Prompt templates\n",
    "from templates import (\n",
    "    SEMANTIC_TEMPLATE,\n",
    "    SCORE_TEMPLATE,\n",
    "    EXTRACT_FACTS_TEMPLATE,\n",
    "    REFINE_FACT_TEMPLATE,\n",
    ")\n",
    "\n",
    "# # Local imports\n",
    "from utils_notebook import (\n",
    "    SemanticCleaner,\n",
    "    calcular_metricas_memoria,\n",
    "    format_choices,\n",
    "    parse_simple_score,\n",
    "    make_question\n",
    ")\n",
    "cleaner = SemanticCleaner()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e501fc",
   "metadata": {},
   "source": [
    "### B) Language Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc4432b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['A','B','C','D']\n",
    "number = {'A':0,'B':1,'C':2,'D':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dc0344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset do arc\n",
    "valid_df = pd.read_csv(\"../../dataset/arc_challenge_valid_processed.csv\")\n",
    "test_df = pd.read_csv(\"../../dataset/arc_challenge_test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f74751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos para extra√ß√£o e tratamento\n",
    "gpt5_nano = ChatOpenAI(model='gpt-5-nano-2025-08-07',temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdb7caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi2 = ChatOllama(model=\"phi\", temperature=0)\n",
    "phi2_creative = ChatOllama(model=\"phi\", temperature=0.6)  # Para gera√ß√£o de fatos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0625cd",
   "metadata": {},
   "source": [
    "### C) Prompts and Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df3a4adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_prompt = PromptTemplate.from_template(SEMANTIC_TEMPLATE)\n",
    "rag_chain = semantic_prompt | phi2\n",
    "\n",
    "score_prompt = PromptTemplate.from_template(SCORE_TEMPLATE)\n",
    "score_chain = score_prompt | phi2 | StrOutputParser()\n",
    "\n",
    "extract_facts_prompt = PromptTemplate.from_template(EXTRACT_FACTS_TEMPLATE)\n",
    "extract_facts_chain = extract_facts_prompt | gpt5_nano | StrOutputParser()\n",
    "\n",
    "refine_fact_prompt = PromptTemplate.from_template(REFINE_FACT_TEMPLATE)\n",
    "refine_fact_chain = refine_fact_prompt | phi2_creative | StrOutputParser()\n",
    "\n",
    "def call_organizer(fact, temperature=0.1):\n",
    "    \"\"\"Limpa e resume um fato cient√≠fico.\"\"\"\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Extract ONLY one objective scientific sentence from the text below. \n",
    "NO introductions, NO explanations, NO conversational filler.\n",
    "\n",
    "Text: \"{fact.strip()}\"\n",
    "Scientific Fact (Write it in only one sentence):\"\"\"\n",
    "    \n",
    "    model = ChatOllama(model=\"phi\", temperature=temperature)\n",
    "    fact_cleaned = model.invoke(prompt).content.strip().strip('\"').strip(\"'\")\n",
    "    fact_cleaned = cleaner.clean(fact_cleaned)\n",
    "    return fact_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af97ae",
   "metadata": {},
   "source": [
    "# 1) Utils Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1257d",
   "metadata": {},
   "source": [
    "### A) Context Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8c974fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_semantic_context(semantic_items, show_scores=False):\n",
    "    if not semantic_items:\n",
    "        return \"No specific reference information found.\"\n",
    "\n",
    "    formatted_parts = []\n",
    "    formatted_parts.append(\"\")\n",
    "    \n",
    "    for i, fact in enumerate(semantic_items, 1):\n",
    "        raw_q = fact['metadata'].get('question', '')\n",
    "        q_text = raw_q.split('\\n')[0].strip()\n",
    "        f_text = fact.get('content', '').strip()\n",
    "        \n",
    "        block = f\"\"\"\n",
    "    * **Principle #{i}**\n",
    "        * Context: \"{q_text}\"\n",
    "        * Fact: \"{f_text}\"\n",
    "\"\"\"\n",
    "        formatted_parts.append(block)        \n",
    "            \n",
    "    return \"\\n\".join(formatted_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae90ab",
   "metadata": {},
   "source": [
    "### B) Answer Extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d77f7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Essa fun√ß√£o deve posteriormente ser dividida em fun√ß√µes menores, cada uma respons√°vel por um aspecto espec√≠fico do processo de formata√ß√£o.\n",
    "# TODO Migrar a fun√ß√£o para um arquivo utils\n",
    "\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "def extract_answer(\n",
    "    small_llm_model, \n",
    "    model_text_output: str, \n",
    "    valid_labels: List[str] = None,\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extra√ß√£o robusta baseada em padr√µes de alta confian√ßa (Tier 1) \n",
    "    com fallback para LLM Juiz em caso de falha.\n",
    "    \"\"\"\n",
    "    if valid_labels is None:\n",
    "        valid_labels = ['A', 'B', 'C', 'D', 'E']\n",
    "    \n",
    "    if not model_text_output:\n",
    "        return \"N/A\"\n",
    "    \n",
    "    text = model_text_output.strip()\n",
    "\n",
    "    if \"```python\" in text or \"def solution\" in text:\n",
    "        if debug: print(\"‚ö† [CODE DETECTED] Enviando para LLM Judge.\")\n",
    "        return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "    strong_patterns = [\n",
    "        r\"\\\\boxed\\s*\\{\\s*([A-H])\\s*\\}\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]?\\s*(?:is)?\\s*(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]](?![a-z])\",\n",
    "        r\"The\\s+(?:correct\\s+)?(?:answer|option|choice)\\s+is\\s*(?:Option)?\\s*[:\\-]?\\s*[\\(\\[]([A-H])[\\)\\]](?![a-z])\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]\\s*(?:is\\s+)?(?:Option\\s+)?([A-H])(?=\\s|\\.|,|!|\\?|$)(?![A-Za-z])\",\n",
    "        r\"(?:Therefore|Thus|Hence|So),\\s*(?:the\\s+answer\\s+is\\s*)?(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]](?![a-z])\",\n",
    "        r\"(?:^|\\n)\\s*Answer\\s*:\\s*([A-H])(?=\\s|$|\\.|\\,)(?![A-Za-z])\",\n",
    "        r\"\\*\\*([A-H])\\*\\*(?![a-z])\",\n",
    "        r\"^([A-H])$\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in strong_patterns:\n",
    "        matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "        if matches:\n",
    "            # Pega a √∫ltima ocorr√™ncia desse padr√£o espec√≠fico\n",
    "            last_match = matches[-1]\n",
    "            candidate = last_match.group(1).upper()\n",
    "            \n",
    "            if candidate in valid_labels:\n",
    "                if debug: \n",
    "                    print(f\"‚úì [REGEX] Padr√£o encontrado: '{pattern}' -> {candidate}\")\n",
    "                return candidate\n",
    "\n",
    "    #  FALLBACK: LLM \n",
    "    # Se nenhum padr√£o forte foi encontrado\n",
    "    # Deixe o LLM ler e decidir.\n",
    "    \n",
    "    if debug: print(\"‚úó [REGEX] Falha nos padr√µes fortes. Chamando LLM Judge.\")\n",
    "    return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "\n",
    "def _llm_judge(\n",
    "    small_llm_model, \n",
    "    text: str, \n",
    "    valid_labels: List[str],\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Juiz focado em INFER√äNCIA. Ele deve ler o racioc√≠nio e mapear para a letra correta,\n",
    "    mesmo que o modelo n√£o tenha dito a letra explicitamente.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Monta contexto apenas se a pergunta existir\n",
    "    context_block = \"\"\n",
    "    if question:\n",
    "        context_block = f\"\"\"\n",
    "### CONTEXT (Use this to infer the answer letter):\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are an Answer Extraction Bot. \n",
    "Your ONLY job is to identify which option the following \"Model Output\" concluded is correct.\n",
    "\n",
    "{context_block}\n",
    "\n",
    "### Model Output to Analyze:\n",
    "{text}\n",
    "\n",
    "### Instructions:\n",
    "1. Look for an explicit answer (e.g., \"Answer: A\").\n",
    "2. If NO explicit letter is found, read the conclusion of the \"Model Output\" and match it against the Options in the Context. **Infer the letter.**\n",
    "   - Example: If Context has \"(A) 5 (B) 10\" and Model Output says \"The result is 10\", you must output B.\n",
    "3. Do NOT calculate or solve the problem yourself. Trust the \"Model Output\".\n",
    "4. If the model refuses to answer or is unclear, return \"E\".\n",
    "\n",
    "Output format: Just the single letter (A, B, C, D, or E). No other text.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = small_llm_model.invoke(prompt) \n",
    "        output = response.content if hasattr(response, 'content') else str(response)\n",
    "        clean_cand = re.sub(r\"[^A-E]\", \"\", output.strip().upper())\n",
    "        \n",
    "        if len(clean_cand) > 1:\n",
    "            clean_cand = clean_cand[-1]\n",
    "            \n",
    "        if clean_cand in valid_labels:\n",
    "            if debug: print(f\"‚úì [LLM JUIZ] Inferido: {clean_cand}\")\n",
    "            return clean_cand\n",
    "            \n",
    "        return \"N/A\"\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug: print(f\"‚úó [ERRO JUIZ] {e}\")\n",
    "        return \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3d557",
   "metadata": {},
   "source": [
    "### C) Check Fact Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c15bf5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_invalid(question, fact, original_fact, check_size=True, check_content=True, \n",
    "               check_similarity=True, min_similarity=0.2, return_reason=False):\n",
    "    \"\"\"\n",
    "    Valida se um fato cient√≠fico gerado √© inv√°lido.\n",
    "    Inclui verifica√ß√£o de drift sem√¢ntico via Similaridade de Cosseno.\n",
    "    \"\"\"\n",
    "    if not fact or not isinstance(fact, str):\n",
    "        return (True, \"Fato vazio ou inv√°lido\") if return_reason else True\n",
    "\n",
    "    fact_lower = fact.lower()\n",
    "    word_count = len(fact.split())\n",
    "    \n",
    "    # 1. Valida√ß√µes de Estrutura\n",
    "    if check_size:\n",
    "        size_checks = [\n",
    "            (word_count > 100, f\"Muito longo ({word_count} tokens)\"),\n",
    "            (word_count < 10, f\"Muito curto ({word_count} tokens)\"),\n",
    "            ('\\n' in fact, \"Cont√©m quebras de linha\"),\n",
    "            ('#' in fact or '_' in fact, \"Cont√©m caracteres especiais (#/_)\"),\n",
    "            (fact_lower == original_fact.lower(), \"Id√™ntico ao original\")\n",
    "        ]\n",
    "        for condition, reason in size_checks:\n",
    "            if condition: return (True, reason) if return_reason else True\n",
    "\n",
    "    # 2. Valida√ß√µes de Conte√∫do\n",
    "    if check_content:\n",
    "        forbidden_terms = ['score', 'correct', 'answer', 'instruction:', 'critique:']\n",
    "        for term in forbidden_terms:\n",
    "            if term in fact_lower:\n",
    "                return (True, f\"Cont√©m termo proibido: {term}\") if return_reason else True\n",
    "\n",
    "    # 3. Valida√ß√£o de Similaridade\n",
    "    if check_similarity:\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(stop_words=None)\n",
    "            tfidf = vectorizer.fit_transform([fact_lower, question.lower()])\n",
    "            sim = cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]\n",
    "            \n",
    "            if sim < min_similarity:\n",
    "                reason = f\"Baixa similaridade ({sim:.2f}). Poss√≠vel alucina√ß√£o.\"\n",
    "                return (True, reason) if return_reason else True\n",
    "        except Exception as e:\n",
    "            return (True, \"Erro no c√°lculo de similaridade\") if return_reason else True\n",
    "\n",
    "    return (False, None) if return_reason else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8851c9f7",
   "metadata": {},
   "source": [
    "### D) Score Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86824b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_score_new_facts(question, response_text, is_correct, pred, target, row, manager):\n",
    "    \"\"\"\n",
    "    Extrai fatos cient√≠ficos da resposta do modelo, atribui scores e salva imediatamente.\n",
    "    \n",
    "    Returns:\n",
    "        List[dict]: Lista de fatos com scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Extra√ß√£o de fatos com GPT-5 nano\n",
    "        facts_text = extract_facts_chain.invoke({\n",
    "            \"model_response\": response_text,\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        # Parse fatos (separados por linha)\n",
    "        raw_facts = [f.strip() for f in facts_text.split('\\n') if f.strip()]\n",
    "        if not raw_facts:\n",
    "            return []\n",
    "        \n",
    "        # Limitar a 2 fatos\n",
    "        raw_facts = raw_facts[:2]\n",
    "        \n",
    "        # 2. Limpar fatos\n",
    "        cleaned_facts = []\n",
    "        for fact in raw_facts:\n",
    "            cleaned = cleaner.clean(fact)\n",
    "            if cleaned and len(cleaned.split()) >= 10:  # M√≠nimo 10 palavras\n",
    "                cleaned_facts.append(cleaned)\n",
    "        \n",
    "        if not cleaned_facts:\n",
    "            return []\n",
    "        \n",
    "        # 3. Calcular scores para cada fato\n",
    "        scored_facts = []\n",
    "        choices = ast.literal_eval(row['choices'])\n",
    "        \n",
    "        # Prote√ß√£o contra √≠ndices inv√°lidos\n",
    "        try:\n",
    "            correct_txt = choices['text'][number[target]] if target in number else str(target)\n",
    "        except (IndexError, KeyError):\n",
    "            correct_txt = str(target)\n",
    "        \n",
    "        try:\n",
    "            chosen_txt = choices['text'][number[pred]] if pred in number else str(pred)\n",
    "        except (IndexError, KeyError):\n",
    "            chosen_txt = str(pred)\n",
    "        \n",
    "        status = \"SUCCESS\" if is_correct else \"FAILURE\"\n",
    "        \n",
    "        for fact in cleaned_facts:\n",
    "            try:\n",
    "                score_response = score_chain.invoke({\n",
    "                    \"question\": question,\n",
    "                    \"chosen\": chosen_txt,\n",
    "                    \"correct\": correct_txt,\n",
    "                    \"outcome_status\": status,\n",
    "                    \"fact\": fact,\n",
    "                    \"reasoning\": response_text\n",
    "                })\n",
    "                \n",
    "                reflection, score = parse_simple_score(score_response)\n",
    "                \n",
    "                scored_facts.append({\n",
    "                    'content': fact,\n",
    "                    'score': score,\n",
    "                    'reflection': reflection,\n",
    "                    'original_id': str(row['id']),\n",
    "                    'question': question,\n",
    "                    'correct_answer': target\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erro ao calcular score: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 4. Salvar fatos no vectorstore\n",
    "        if scored_facts:\n",
    "            new_memory_entries = []\n",
    "            for fact_data in scored_facts:\n",
    "                new_memory_entries.append({\n",
    "                    'content': fact_data['content'],\n",
    "                    'metadata': {\n",
    "                        'original_id': fact_data['original_id'],\n",
    "                        'question': fact_data['question'],\n",
    "                        'correct_answer': fact_data['correct_answer'],\n",
    "                        'score': fact_data['score'],\n",
    "                        'frequency': 0,\n",
    "                        'scientific_fact': fact_data['content'],\n",
    "                        'origin': 'validation_generated',\n",
    "                        'type': 'semantic_memory',\n",
    "                        'initial_score': fact_data['score']\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            manager.add_new_memories(new_memory_entries, id_prefix='generated')\n",
    "        \n",
    "        return scored_facts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro na extra√ß√£o de fatos: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca55990",
   "metadata": {},
   "source": [
    "### E) Fact Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b9d0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_negative_facts(used_facts, is_correct, question, pred, target, row, manager, \n",
    "                          refine_threshold=0.3, verbose=False):\n",
    "    \"\"\"\n",
    "    ‚ú® NEW: Refina fatos recuperados que contribu√≠ram para erro.\n",
    "    Aplica apenas quando houve falha (is_correct = False).\n",
    "    \n",
    "    Args:\n",
    "        refine_threshold: Score abaixo do qual um fato ser√° refinado (default: 0.3)\n",
    "        verbose: Se True, mostra informa√ß√µes de debug\n",
    "    \n",
    "    Returns:\n",
    "        int: N√∫mero de fatos refinados\n",
    "    \"\"\"\n",
    "    if is_correct or not used_facts:\n",
    "        return 0\n",
    "    \n",
    "    refined_count = 0\n",
    "    candidates_count = 0\n",
    "    choices = ast.literal_eval(row['choices'])\n",
    "    \n",
    "    # Prote√ß√£o contra √≠ndices inv√°lidos\n",
    "    try:\n",
    "        correct_txt = choices['text'][number[target]] if target in number else str(target)\n",
    "    except (IndexError, KeyError):\n",
    "        correct_txt = str(target)\n",
    "    \n",
    "    try:\n",
    "        wrong_txt = choices['text'][number[pred]] if pred in number else str(pred)\n",
    "    except (IndexError, KeyError):\n",
    "        wrong_txt = str(pred)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüîç Analisando {len(used_facts)} fatos para refinamento (threshold: {refine_threshold})...\")\n",
    "    \n",
    "    for memory in used_facts:\n",
    "        fact_text = memory['content']\n",
    "        memory_score = memory.get('memory_strength', 0.0)\n",
    "        \n",
    "        # Refina apenas fatos com score baixo (< refine_threshold)\n",
    "        if memory_score >= refine_threshold:\n",
    "            if verbose:\n",
    "                print(f\"   ‚äó Fato ignorado (score {memory_score:.2f} >= {refine_threshold})\")\n",
    "            continue\n",
    "        \n",
    "        candidates_count += 1\n",
    "        if verbose:\n",
    "            print(f\"   ‚úì Candidato #{candidates_count} (score: {memory_score:.2f})\")\n",
    "        \n",
    "        try:\n",
    "            # Gerar reflex√£o/cr√≠tica sobre o fato\n",
    "            score_response = score_chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"chosen\": wrong_txt,\n",
    "                \"correct\": correct_txt,\n",
    "                \"outcome_status\": \"FAILURE\",\n",
    "                \"fact\": fact_text,\n",
    "                \"reasoning\": f\"Used fact led to wrong answer: {wrong_txt}\"\n",
    "            })\n",
    "            \n",
    "            reflection, _ = parse_simple_score(score_response)\n",
    "            \n",
    "            # Tentar gerar fato refinado\n",
    "            max_attempts = 3\n",
    "            for attempt in range(max_attempts):\n",
    "                new_fact_text = refine_fact_chain.invoke({\n",
    "                    \"question\": question,\n",
    "                    \"correct_answer\": correct_txt,\n",
    "                    \"old_fact\": fact_text,\n",
    "                    \"reflection\": reflection,\n",
    "                    \"wrong_answer\": wrong_txt\n",
    "                })\n",
    "                \n",
    "                clean_new_fact = call_organizer(new_fact_text, temperature=0.1)\n",
    "                \n",
    "                # Validar fato refinado\n",
    "                q_c = f\"{question} {correct_txt}\"\n",
    "                if not is_invalid(question=q_c, fact=clean_new_fact, original_fact=fact_text, \n",
    "                                check_size=True, check_content=True, check_similarity=True):\n",
    "                    # Salvar fato refinado\n",
    "                    manager.add_new_memories([{\n",
    "                        'content': clean_new_fact,\n",
    "                        'metadata': {\n",
    "                            'original_id': str(row['id']),\n",
    "                            'question': question,\n",
    "                            'correct_answer': target,\n",
    "                            'score': 0,  # Score neutro inicial\n",
    "                            'frequency': 0,\n",
    "                            'scientific_fact': clean_new_fact,\n",
    "                            'origin': 'validation_refined',\n",
    "                            'type': 'semantic_memory',\n",
    "                            'initial_score': 0,\n",
    "                            'refined_from': fact_text\n",
    "                        }\n",
    "                    }], id_prefix='refined')\n",
    "                    \n",
    "                    refined_count += 1\n",
    "                    if verbose:\n",
    "                        print(f\"   ‚úÖ Fato refinado com sucesso (tentativa {attempt+1})\")\n",
    "                    break\n",
    "                elif verbose and attempt == max_attempts - 1:\n",
    "                    print(f\"   ‚ùå Falha ap√≥s {max_attempts} tentativas\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"   ‚ö†Ô∏è Erro ao refinar fato: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìä Resultado: {candidates_count} candidatos, {refined_count} refinados\\n\")\n",
    "    \n",
    "    return refined_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84713619",
   "metadata": {},
   "source": [
    "### D) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eaf2b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_dataset_cognitive_plus(\n",
    "    df, \n",
    "    chain=rag_chain,\n",
    "    manager_semantic=None,\n",
    "    score_chain=score_chain,\n",
    "    k_semantic=3,\n",
    "    threshold_semantic=0.24,\n",
    "    semantic_weight=0.7,\n",
    "    update_memory=False,\n",
    "    decay_batch_size=False,\n",
    "    decay_frequency=50,\n",
    "    extract_new_facts=False,  # ‚ú® NEW\n",
    "    refine_facts=False,       # ‚ú® NEW\n",
    "    backup_frequency=160,      # üíæ NEW\n",
    "    backup_path=None,         # üíæ NEW\n",
    "    desc=\"Cognitive Memory System PLUS\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Avalia dataset com sistema h√≠brido (sem√¢ntica + reflex√£o) + gera√ß√£o/refinamento.\n",
    "    \n",
    "    Args:\n",
    "        extract_new_facts: Se True, extrai fatos das respostas do modelo\n",
    "        refine_facts: Se True, refina fatos com baixo score ap√≥s erro\n",
    "        backup_frequency: Salva backup a cada N quest√µes\n",
    "        backup_path: Caminho base para salvar backups\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    acertos = 0\n",
    "    total = len(df)\n",
    "    decay_counter = 0\n",
    "    backup_counter = 0\n",
    "    \n",
    "    # Contadores de gera√ß√£o/refinamento\n",
    "    total_facts_generated = 0\n",
    "    total_facts_refined = 0\n",
    "    total_facts_errors = 0\n",
    "    \n",
    "    # Acumuladores globais\n",
    "    global_sims_sem = []\n",
    "    global_counts_sem = []\n",
    "    \n",
    "    erros = 0\n",
    "\n",
    "    # üíæ Preparar diret√≥rio de backup\n",
    "    if backup_path:\n",
    "        backup_dir = os.path.dirname(backup_path)\n",
    "        if backup_dir and not os.path.exists(backup_dir):\n",
    "            os.makedirs(backup_dir, exist_ok=True)\n",
    "    \n",
    "    loop = tqdm(df.iterrows(), total=total, desc=desc)\n",
    "\n",
    "    for idx, row in loop:\n",
    "        try:\n",
    "            full_question = make_question(row, inline=False)[0]\n",
    "            question_for_retriever = re.sub(r'\\([A-Z]\\)\\s*', '', row['question'])\n",
    "            \n",
    "            # Recuperar mem√≥rias sem√¢nticas (COM SCORE COGNITIVO)\n",
    "            items_semantic, _ = manager_semantic.retrieve_ranked_memories(\n",
    "                query=question_for_retriever, \n",
    "                k=k_semantic,\n",
    "                threshold=threshold_semantic,\n",
    "                semantic_weight=semantic_weight,\n",
    "                show_scores=False\n",
    "            )\n",
    "            \n",
    "\n",
    "            \n",
    "            # Combinar contextos\n",
    "            cognitive_context_formatted = format_semantic_context(items_semantic)\n",
    "\n",
    "            count_sem, avg_sim_sem, raw_sims_sem = calcular_metricas_memoria(items_semantic)\n",
    "            \n",
    "            global_counts_sem.append(count_sem)\n",
    "            global_sims_sem.extend(raw_sims_sem)\n",
    "\n",
    "            # Invocar o modelo com retry mechanism\n",
    "            #temperatures = [0]  # Temperaturas para tentar\n",
    "            #pred = 'E'\n",
    "            #response_text = \"\"\n",
    "            \n",
    "            #for attempt, temp in enumerate(temperatures):\n",
    "            #    try:\n",
    "                    # # Criar chain com temperatura espec√≠fica\n",
    "                    # current_model = ChatOllama(model=\"phi\", temperature=temp)\n",
    "                    # current_chain = cognitive_prompt | current_model\n",
    "                    \n",
    "            response_obj = chain.invoke({\n",
    "                \"question\": full_question,\n",
    "                \"similar_facts\": cognitive_context_formatted, \n",
    "            })\n",
    "\n",
    "            response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "            \n",
    "            pred = extract_answer(\n",
    "                small_llm_model=gpt5_nano, \n",
    "                model_text_output=response_text,\n",
    "                question=full_question,\n",
    "            )\n",
    "\n",
    "            if pred not in ['A', 'B', 'C', 'D']:\n",
    "                pred = 'E'  # E de Error/Exception\n",
    "                erros += 1\n",
    "                    \n",
    "                    # Se a extra√ß√£o foi bem-sucedida (pred != 'E'), sair do loop\n",
    "                    # if pred != 'E':\n",
    "                    #     if attempt > 0:\n",
    "                    #         tqdm.write(f\"‚úì Q{idx+1}: Sucesso na tentativa {attempt+1} (temp={temp})\")\n",
    "                    #     break\n",
    "                    # else:\n",
    "                    #     if attempt < len(temperatures) - 1:\n",
    "                    #         tqdm.write(f\"‚ö† Q{idx+1}: Extra√ß√£o falhou (temp={temp}), tentando novamente...\")\n",
    "                \n",
    "             #   except Exception as e:\n",
    "              #      tqdm.write(f\"‚ö† Q{idx+1}: Erro na tentativa {attempt+1}: {e}\")\n",
    "               #     if attempt == len(temperatures) - 1:\n",
    "                #        raise\n",
    "            \n",
    "            target = row['answerKey']\n",
    "\n",
    "            is_correct = (pred == target)\n",
    "            acertos += int(is_correct)\n",
    "            \n",
    "            # === ‚ú® NEW: EXTRA√á√ÉO DE NOVOS FATOS ===\n",
    "            if extract_new_facts and pred != 'E':\n",
    "                try:\n",
    "                    new_facts = extract_and_score_new_facts(\n",
    "                        question=full_question,\n",
    "                        response_text=response_text,\n",
    "                        is_correct=is_correct,\n",
    "                        pred=pred,\n",
    "                        target=target,\n",
    "                        row=row,\n",
    "                        manager=manager_semantic\n",
    "                    )\n",
    "                    if new_facts:\n",
    "                        total_facts_generated += len(new_facts)\n",
    "                    else:\n",
    "                        total_facts_errors += 1\n",
    "                except Exception as e:\n",
    "                    total_facts_errors += 1\n",
    "                    tqdm.write(f\"‚ùå Q{idx+1}: Erro na extra√ß√£o de fatos: {e}\")\n",
    "            \n",
    "            # === UPDATE DE MEM√ìRIA EXISTENTE ===\n",
    "            if update_memory and items_semantic and pred != 'E':\n",
    "                used_ids = [m['doc_id'] for m in items_semantic]\n",
    "                \n",
    "                # Calcular feedback scores\n",
    "                feedback_scores = []\n",
    "                choices = ast.literal_eval(row['choices'])\n",
    "                \n",
    "                try:\n",
    "                    correct_txt = choices['text'][number[target]] if target in number else str(target)\n",
    "                except (IndexError, KeyError):\n",
    "                    correct_txt = str(target)\n",
    "                \n",
    "                try:\n",
    "                    chosen_txt = choices['text'][number[pred]] if pred in number else str(pred)\n",
    "                except (IndexError, KeyError):\n",
    "                    chosen_txt = str(pred)\n",
    "                \n",
    "                status = \"SUCCESS\" if is_correct else \"FAILURE\"\n",
    "                \n",
    "                for memory in items_semantic:\n",
    "                    fact_text = memory['content']\n",
    "                    \n",
    "                    try:\n",
    "                        score_response = score_chain.invoke({\n",
    "                            \"question\": full_question,\n",
    "                            \"chosen\": chosen_txt,\n",
    "                            \"correct\": correct_txt,\n",
    "                            \"outcome_status\": status,\n",
    "                            \"fact\": fact_text,\n",
    "                            \"reasoning\": response_text\n",
    "                        })\n",
    "                        \n",
    "                        _, score = parse_simple_score(score_response)\n",
    "                        feedback_scores.append(score)\n",
    "                    except:\n",
    "                        feedback_scores.append(0)\n",
    "                \n",
    "                # Aplicar update EMA\n",
    "                manager_semantic.update_memories_feedback(used_ids, feedback_scores)\n",
    "            \n",
    "            # === ‚ú® NEW: REFINAMENTO DE FATOS ===\n",
    "            if refine_facts and not is_correct:\n",
    "                refined = refine_negative_facts(\n",
    "                    used_facts=items_semantic,\n",
    "                    is_correct=is_correct,\n",
    "                    question=full_question,\n",
    "                    pred=pred,\n",
    "                    target=target,\n",
    "                    row=row,\n",
    "                    manager=manager_semantic\n",
    "                )\n",
    "                total_facts_refined += refined\n",
    "            \n",
    "            # === DECAY PERI√ìDICO ===\n",
    "            if update_memory and decay_frequency > 0 and (idx + 1) % decay_frequency == 0:\n",
    "                decay_counter += 1\n",
    "                tqdm.write(f\"üåô [Q{idx+1}/{total}] Aplicando ciclo de decay #{decay_counter}...\")\n",
    "                manager_semantic.apply_decay_cycle(decay_threshold=0.0)\n",
    "            \n",
    "            # === üíæ BACKUP PERI√ìDICO ===\n",
    "            if backup_path and backup_frequency > 0 and (idx + 1) % backup_frequency == 0:\n",
    "                backup_counter += 1\n",
    "                df_partial = pd.DataFrame(resultados)\n",
    "                timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                backup_file = backup_path.replace('.csv', f'_backup.csv')\n",
    "                df_partial.to_csv(backup_file, index=False)\n",
    "                #tqdm.write(f\"üíæ [Q{idx+1}/{total}] Backup #{backup_counter} salvo: {backup_file}\")\n",
    "            \n",
    "            # Atualiza barra de progresso\n",
    "            acc_atual = (acertos / (loop.n + 1)) * 100\n",
    "            loop.set_postfix(\n",
    "                acc=f\"{acc_atual:.2f}%\", \n",
    "                sem=count_sem, \n",
    "                decay=decay_counter,\n",
    "                gen=total_facts_generated,\n",
    "                refine=total_facts_refined,\n",
    "                err=total_facts_errors,\n",
    "                bkp=backup_counter,\n",
    "                erros=erros\n",
    "            )\n",
    "\n",
    "            resultados.append({\n",
    "                'index': idx,\n",
    "                'question': full_question,\n",
    "                'retrieved_context': cognitive_context_formatted,\n",
    "                'retrieved_count_semantic': count_sem,\n",
    "                'avg_similarity_semantic': avg_sim_sem,\n",
    "                'raw_output': response_text,\n",
    "                'pred': pred,\n",
    "                'target': target,\n",
    "                'is_correct': is_correct,\n",
    "                'source': desc,\n",
    "                'erros': erros\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Erro no √≠ndice {idx}: {e}\")\n",
    "            resultados.append({\n",
    "                'index': idx, \n",
    "                'error': str(e), \n",
    "                'is_correct': False, \n",
    "                'retrieved_count_semantic': 0,\n",
    "                'retrieved_count_reflection': 0, \n",
    "                'avg_similarity_semantic': 0.0,\n",
    "                'avg_similarity_reflection': 0.0,\n",
    "            })\n",
    "    \n",
    "    # === DECAY FINAL ===\n",
    "    if update_memory and decay_batch_size:\n",
    "        decay_counter += 1\n",
    "        print(f\"\\nüåô [FINAL] Aplicando ciclo de decay final (#{decay_counter})...\")\n",
    "        manager_semantic.apply_decay_cycle(decay_threshold=0.0)\n",
    "    \n",
    "    # Sum√°rio final\n",
    "    if update_memory:\n",
    "        print(f\"\\nüìä SUM√ÅRIO DE MEM√ìRIA:\")\n",
    "        print(f\"   Ciclos de decay: {decay_counter}\")\n",
    "        print(f\"   Fatos gerados: {total_facts_generated}\")\n",
    "        print(f\"   Fatos refinados: {total_facts_refined}\")\n",
    "        print(f\"   Erros na extra√ß√£o: {total_facts_errors}\")\n",
    "        print(f\"   Mem√≥rias sem√¢nticas finais: {manager_semantic._collection.count()}\")\n",
    "    \n",
    "    if backup_path and backup_counter > 0:\n",
    "        print(f\"üíæ Total de backups salvos: {backup_counter}\")\n",
    "    \n",
    "    return pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d68fa",
   "metadata": {},
   "source": [
    "# 2) Constru√ß√£o do Bancos de Dados Sem√¢ntico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d629dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific_facts = pd.read_csv(\"../../scientific_facts_expanded.csv\")\n",
    "scientific_facts = scientific_facts.fillna(\"N/A\")\n",
    "\n",
    "# Preparar dados sem√¢nticos\n",
    "df_semantic = scientific_facts[scientific_facts['scientific_fact'] != \"N/A\"].copy()\n",
    "df_semantic.drop_duplicates(subset=['scientific_fact'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b211ed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Carregando mem√≥ria existente de: vectorstores/semantic_refinement/chroma_semantic\n",
      "‚úÖ Mem√≥ria carregada com 3993 itens.\n"
     ]
    }
   ],
   "source": [
    "FORCE_RESET = False  # True quando quisermos recriar os bancos do zero\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") \n",
    "\n",
    "manager_semantic = CognitiveMemoryManager(\n",
    "    db_path=\"vectorstores/semantic_refinement/chroma_semantic\",\n",
    "    embedding_model=embedding_model, alpha=0.1, decay_lambda=0.99, forget_threshold=-0.6\n",
    ").init_from_dataframe(df=df_semantic, content_col='scientific_fact',\n",
    "                      id_prefix='semantic', metadata_func=build_metadata_semantic, reset_db=FORCE_RESET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18567d03",
   "metadata": {},
   "source": [
    "# 3) Testing in one question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3044aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "k_semantic = 3\n",
    "row = test_df.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f80d9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?\n",
      "(A) Planetary density will decrease.\n",
      "(B) Planetary years will become longer.\n",
      "(C) Planetary days will become shorter.\n",
      "(D) Planetary gravity will become stronger.\n",
      "\n",
      "Semantic Context: \n",
      " \n",
      "\n",
      "    * **Principle #1**\n",
      "        * Context: \"Why is it winter in North America when it is summer in South America?\"\n",
      "        * Fact: \"Axial precession causes the Earth's axis to change its orientation relative to its orbit around the sun over a period of approximately 26,000 years.\"\n",
      "\n",
      "\n",
      "    * **Principle #2**\n",
      "        * Context: \"Michael learned that the movement of Earth in the solar system causes changes that can be seen on the planet. Which change could be seen on Earth in the time it takes Earth to rotate once on its axis?\"\n",
      "        * Fact: \"This apparent movement and day-night change are due to Earth's axial tilt and its revolution around the Sun.\"\n",
      "\n",
      "\n",
      "    * **Principle #3**\n",
      "        * Context: \"Which would a scientist use in trying to model the cause of planetary years?\"\n",
      "        * Fact: \"A greater planetary mass can increase the gravitational interaction with the Sun and can affect the planet's orbital period.\"\n",
      "\n",
      "\n",
      "M√©tricas Sem√¢nticas: Count: 3 AVG_SIM: 0.5421754916508993\n"
     ]
    }
   ],
   "source": [
    "q_text = make_question(row, inline=False)[0]\n",
    "question_clean = re.sub(r'\\([A-Z]\\)\\s*', '', q_text)\n",
    "\n",
    "# Recuperar mem√≥rias sem√¢nticas\n",
    "items_semantic, _ = manager_semantic.retrieve_ranked_memories(\n",
    "    query=question_clean, \n",
    "    k=k_semantic, \n",
    "    threshold=0.24,\n",
    "    semantic_weight=0.9,\n",
    "    show_scores=True\n",
    ")\n",
    "\n",
    "semantic_context = format_semantic_context(items_semantic, show_scores=True)\n",
    "\n",
    "print(\"Question: \\n\", q_text)\n",
    "print(\"\\nSemantic Context: \\n\", semantic_context)\n",
    "\n",
    "count_sem, avg_sim_sem, raw_sims_sem = calcular_metricas_memoria(items_semantic)\n",
    "print(\"\\nM√©tricas Sem√¢nticas: Count:\", count_sem, \"AVG_SIM:\", avg_sim_sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69becdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to answer:  3.63\n",
      "‚úì [REGEX] Padr√£o encontrado: '(?:Final|Correct)\\s+Answer\\s*[:\\-]?\\s*(?:is)?\\s*(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]](?![a-z])' -> C\n",
      "# Raw Response: \n",
      "  \n",
      "The correct answer is (C) Planetary days will become shorter.\n",
      "\n",
      "Reasoning:\n",
      "1. The rotation of a planet determines the length of its day, so an increase in rotation would result in shorter planetary days.\n",
      "2. This change could be due to the impact causing a shift in the planet's mass distribution, which affects its rotational speed.\n",
      "3. The other options are not supported by the given information.\n",
      "\n",
      "# Alternative chosen: C (Correta: C)\n",
      "# Resultado: CORRETO\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "a = time.time()\n",
    "semantic_context = format_semantic_context(items_semantic, show_scores=False)\n",
    "response_obj = rag_chain.invoke({\n",
    "    \"question\": q_text,\n",
    "    \"similar_facts\": semantic_context, \n",
    "})\n",
    "print(\"Time to answer: \", round(time.time()-a,2))\n",
    "response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "\n",
    "pred = extract_answer(\n",
    "    small_llm_model=gpt5_nano, \n",
    "    model_text_output=response_text,\n",
    "    question=q_text,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "target = row['answerKey']\n",
    "is_correct = (pred == target)\n",
    "\n",
    "print(\"# Raw Response: \\n\", response_text)\n",
    "print(f\"# Alternative chosen: {pred} (Correta: {target})\")\n",
    "print(f\"# Resultado: {'CORRETO' if is_correct else 'ERRO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b814c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Fatos extra√≠dos: 2\n",
      "\n",
      "Fato 1 (Score: 1): The length of a planet's day is determined by its rotation rate.\n",
      "\n",
      "Fato 2 (Score: 1): Day length is inversely related to rotation rate; faster rotation yields shorter days.\n"
     ]
    }
   ],
   "source": [
    "# Teste de extra√ß√£o de fatos\n",
    "new_facts = extract_and_score_new_facts(\n",
    "    question=q_text,\n",
    "    response_text=response_text,\n",
    "    is_correct=is_correct,\n",
    "    pred=pred,\n",
    "    target=target,\n",
    "    row=row,\n",
    "    manager=manager_semantic\n",
    ")\n",
    "\n",
    "print(f\"\\n‚ú® Fatos extra√≠dos: {len(new_facts)}\")\n",
    "for i, fact in enumerate(new_facts, 1):\n",
    "    print(f\"\\nFato {i} (Score: {fact['score']}): {fact['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a51a5",
   "metadata": {},
   "source": [
    "# 4) Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDA√á√ÉO: Mem√≥ria ATIVA + Gera√ß√£o/Refinamento\n",
    "df_resultado_valid_semantic_plus = avaliar_dataset_cognitive_plus(\n",
    "    df=valid_df, \n",
    "    chain=rag_chain,\n",
    "    manager_semantic=manager_semantic,\n",
    "    score_chain=score_chain,\n",
    "    k_semantic=3,\n",
    "    semantic_weight=0.7,\n",
    "    update_memory=True,\n",
    "    decay_batch_size=True,\n",
    "    decay_frequency=100,\n",
    "    extract_new_facts=True,   # ‚ú® ATIVA extra√ß√£o\n",
    "    refine_facts=True,        # ‚ú® ATIVA refinamento\n",
    "    backup_frequency=130,      # üíæ Backup a cada 25 quest√µes\n",
    "    backup_path=\"../../results/semantic_refinement_cognitive_valid.csv\",  # üíæ Caminho do backup\n",
    "    desc=\"Valida√ß√£o (Memory Active + Gen/Refine)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcab3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teste (Memory Frozen): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1172/1172 [40:41<00:00,  2.08s/it, acc=70.65%, bkp=7, decay=0, err=0, erros=5, gen=0, refine=0, sem=3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Total de backups salvos: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TESTE: Mem√≥ria CONGELADA (apenas recupera√ß√£o)\n",
    "df_resultado_test_semantic_plus = avaliar_dataset_cognitive_plus(\n",
    "    df=test_df, \n",
    "    chain=rag_chain,\n",
    "    manager_semantic=manager_semantic,\n",
    "    score_chain=score_chain,\n",
    "    k_semantic=3,\n",
    "    semantic_weight=0.7,\n",
    "    update_memory=False,\n",
    "    decay_batch_size=False,\n",
    "    extract_new_facts=False,  # ‚ú® DESATIVA extra√ß√£o\n",
    "    refine_facts=False,       # ‚ú® DESATIVA refinamento\n",
    "    backup_frequency=150,      # üíæ Backup a cada 150 quest√µes\n",
    "    backup_path=\"../../results/semantic_refinement_cognitive_test07.csv\",  # üíæ Caminho do backup\n",
    "    desc=\"Teste (Memory Frozen)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "400c6f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_test_semantic_plus.to_csv(\"../../results/semantic_refinement_cognitive_test.csv\", index=False)\n",
    "df_resultado_valid_semantic_plus.to_csv(\"../../results/semantic_refinement_cognitive_valid.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
