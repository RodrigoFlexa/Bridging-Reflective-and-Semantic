{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1928fce6",
   "metadata": {},
   "source": [
    "# Hybrid Memory System (Semantic + Reflection) with Cognitive Scoring ‚Äî Dynamic Pruning Only\n",
    "\n",
    "**Fases:**\n",
    "- **Validation**: Update de scores (EMA) + decay\n",
    "- **Teste**: Freeze Memory\n",
    "\n",
    "> ‚ö†Ô∏è Nesta vers√£o **n√£o h√° gera√ß√£o de novos fatos nem refinamento**. O √∫nico mecanismo ativo na valida√ß√£o √© o update de score (EMA) e o decaimento (decay)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f16436",
   "metadata": {},
   "source": [
    "# 0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ee6e04",
   "metadata": {},
   "source": [
    "### A) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e8959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "# Add vectorstore & prompts folders to path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'vectorstore')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'prompts')))\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Memory managers & metadata builders\n",
    "from cognitive_memory_manager import CognitiveMemoryManager, build_metadata_semantic\n",
    "from simple_vector_memory import SimpleVectorMemory, build_metadata_reflection\n",
    "\n",
    "# Prompt templates\n",
    "from templates import (\n",
    "    HYBRID_TEMPLATE,\n",
    "    SCORE_TEMPLATE,\n",
    ")\n",
    "\n",
    "# Local imports\n",
    "from utils_notebook import (\n",
    "    SemanticCleaner,\n",
    "    calcular_metricas_memoria,\n",
    "    format_choices,\n",
    "    parse_simple_score,\n",
    "    make_question\n",
    ")\n",
    "cleaner = SemanticCleaner()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad63e73a",
   "metadata": {},
   "source": [
    "### B) Language Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78be919",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['A','B','C','D']\n",
    "number = {'A':0,'B':1,'C':2,'D':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043db6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset do arc\n",
    "valid_df = pd.read_csv(\"../../dataset/arc_challenge_valid_processed.csv\")\n",
    "test_df = pd.read_csv(\"../../dataset/arc_challenge_test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM judge\n",
    "gpt5_nano = ChatOpenAI(model='gpt-5-nano-2025-08-07',temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b977fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi2 = ChatOllama(model=\"phi\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229fc613",
   "metadata": {},
   "source": [
    "### C) Prompts and Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_prompt = PromptTemplate.from_template(HYBRID_TEMPLATE)\n",
    "rag_chain = hybrid_prompt | phi2\n",
    "\n",
    "score_prompt = PromptTemplate.from_template(SCORE_TEMPLATE)\n",
    "score_chain = score_prompt | phi2 | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20dce6a",
   "metadata": {},
   "source": [
    "# 1) Utils Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27f857",
   "metadata": {},
   "source": [
    "### A) Context Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa118b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_hybrid_context(semantic_items, reflection_items, show_scores=False):\n",
    "    formatted_parts = []\n",
    "    \n",
    "    if semantic_items:\n",
    "        formatted_parts.append(\"### PART 1: SCIENTIFIC PRINCIPLES (THEORY)\")\n",
    "        for i, fact in enumerate(semantic_items, 1):\n",
    "            raw_q = fact['metadata'].get('question', '')\n",
    "            q_text = raw_q.split('\\n')[0].strip()\n",
    "            f_text = fact.get('content', '').strip()\n",
    "            \n",
    "            if show_scores:\n",
    "                sim = fact.get('similarity', 0)\n",
    "                mem = fact.get('memory_strength', 0)\n",
    "                final = fact.get('final_score', 0)\n",
    "                block = f\"\"\"\n",
    "    * **Principle #{i}** (Sim: {sim:.2f}, Mem: {mem:.2f}, Final: {final:.2f})\n",
    "        Context: \"{q_text}\"\n",
    "        Fact: \"{f_text}\"\n",
    "\"\"\"\n",
    "            else:\n",
    "                block = f\"\"\"\n",
    "    * **Principle #{i}**\n",
    "        * Context: \"{q_text}\"\n",
    "        * Fact: \"{f_text}\"\n",
    "\"\"\"\n",
    "            formatted_parts.append(block)\n",
    "\n",
    "    if reflection_items:\n",
    "        formatted_parts.append(\"\\n### PART 2: SOLVED EXAMPLES (PRACTICE)\")\n",
    "        for i, reflection in enumerate(reflection_items, 1):\n",
    "            raw_q = reflection['metadata'].get('question', '')\n",
    "            q_text = raw_q.split('\\n')[0].strip()\n",
    "            r_text = reflection.get('content', '').strip()\n",
    "            \n",
    "            block = f\"\"\"\n",
    "    * **Example Case #{i}**\n",
    "        > Context: {q_text}\n",
    "        > Analysis: {r_text}\n",
    "\"\"\"\n",
    "            formatted_parts.append(block)\n",
    "    \n",
    "    if not formatted_parts:\n",
    "        return \"No specific reference information found.\"\n",
    "    \n",
    "    return \"\\n\".join(formatted_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b46a9e",
   "metadata": {},
   "source": [
    "### B) Answer Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863355b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "def extract_answer(\n",
    "    small_llm_model, \n",
    "    model_text_output: str, \n",
    "    valid_labels: List[str] = None,\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extra√ß√£o robusta baseada em padr√µes de alta confian√ßa (Tier 1) \n",
    "    com fallback para LLM Juiz em caso de falha.\n",
    "    \"\"\"\n",
    "    if valid_labels is None:\n",
    "        valid_labels = ['A', 'B', 'C', 'D', 'E']\n",
    "    \n",
    "    if not model_text_output:\n",
    "        return \"N/A\"\n",
    "    \n",
    "    text = model_text_output.strip()\n",
    "\n",
    "    if \"```python\" in text or \"def solution\" in text:\n",
    "        if debug: print(\"‚ö† [CODE DETECTED] Enviando para LLM Judge.\")\n",
    "        return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "    strong_patterns = [\n",
    "        r\"\\\\boxed\\s*\\{\\s*([A-H])\\s*\\}\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]?\\s*(?:is)?\\s*(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]](?![a-z])\",\n",
    "        r\"The\\s+(?:correct\\s+)?(?:answer|option|choice)\\s+is\\s*(?:Option)?\\s*[:\\-]?\\s*[\\(\\[]([A-H])[\\)\\]](?![a-z])\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]\\s*(?:is\\s+)?(?:Option\\s+)?([A-H])(?=\\s|\\.|,|!|\\?|$)(?![A-Za-z])\",\n",
    "        r\"(?:Therefore|Thus|Hence|So),\\s*(?:the\\s+answer\\s+is\\s*)?(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]](?![a-z])\",\n",
    "        r\"(?:^|\\n)\\s*Answer\\s*:\\s*([A-H])(?=\\s|$|\\.|\\,)(?![A-Za-z])\",\n",
    "        r\"\\*\\*([A-H])\\*\\*(?![a-z])\",\n",
    "        r\"^([A-H])$\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in strong_patterns:\n",
    "        matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "        if matches:\n",
    "            last_match = matches[-1]\n",
    "            candidate = last_match.group(1).upper()\n",
    "            \n",
    "            if candidate in valid_labels:\n",
    "                if debug: \n",
    "                    print(f\"‚úì [REGEX] Padr√£o encontrado: '{pattern}' -> {candidate}\")\n",
    "                return candidate\n",
    "\n",
    "    if debug: print(\"‚úó [REGEX] Falha nos padr√µes fortes. Chamando LLM Judge.\")\n",
    "    return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "\n",
    "def _llm_judge(\n",
    "    small_llm_model, \n",
    "    text: str, \n",
    "    valid_labels: List[str],\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    context_block = \"\"\n",
    "    if question:\n",
    "        context_block = f\"\"\"\n",
    "### CONTEXT (Use this to infer the answer letter):\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are an Answer Extraction Bot. \n",
    "Your ONLY job is to identify which option the following \"Model Output\" concluded is correct.\n",
    "\n",
    "{context_block}\n",
    "\n",
    "### Model Output to Analyze:\n",
    "{text}\n",
    "\n",
    "### Instructions:\n",
    "1. Look for an explicit answer (e.g., \"Answer: A\").\n",
    "2. If NO explicit letter is found, read the conclusion of the \"Model Output\" and match it against the Options in the Context. **Infer the letter.**\n",
    "   - Example: If Context has \"(A) 5 (B) 10\" and Model Output says \"The result is 10\", you must output B.\n",
    "3. Do NOT calculate or solve the problem yourself. Trust the \"Model Output\".\n",
    "4. If the model refuses to answer or is unclear, return \"E\".\n",
    "\n",
    "Output format: Just the single letter (A, B, C, D, or E). No other text.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = small_llm_model.invoke(prompt) \n",
    "        output = response.content if hasattr(response, 'content') else str(response)\n",
    "        clean_cand = re.sub(r\"[^A-E]\", \"\", output.strip().upper())\n",
    "        \n",
    "        if len(clean_cand) > 1:\n",
    "            clean_cand = clean_cand[-1]\n",
    "            \n",
    "        if clean_cand in valid_labels:\n",
    "            if debug: print(f\"‚úì [LLM JUIZ] Inferido: {clean_cand}\")\n",
    "            return clean_cand\n",
    "            \n",
    "        return \"N/A\"\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug: print(f\"‚úó [ERRO JUIZ] {e}\")\n",
    "        return \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca803ca",
   "metadata": {},
   "source": [
    "# 2) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d87230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_dataset_dynamic_pruning(\n",
    "    df, \n",
    "    chain=rag_chain,\n",
    "    manager_semantic=None,\n",
    "    manager_reflection=None,  \n",
    "    score_chain=score_chain,\n",
    "    k_semantic=3,\n",
    "    k_reflection=3,\n",
    "    threshold_semantic=0.24,\n",
    "    threshold_reflection=0.24,\n",
    "    semantic_weight=0.7,\n",
    "    update_memory=False,\n",
    "    decay_batch_size=False,\n",
    "    decay_frequency=50,\n",
    "    backup_frequency=160,\n",
    "    backup_path=None,\n",
    "    desc=\"Dynamic Pruning (Score Update + Decay)\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Avalia dataset com sistema h√≠brido (sem√¢ntica + reflex√£o).\n",
    "    Na valida√ß√£o: apenas update de score (EMA) e decaimento.\n",
    "    N√£o h√° gera√ß√£o de novos fatos nem refinamento.\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    acertos = 0\n",
    "    total = len(df)\n",
    "    decay_counter = 0\n",
    "    backup_counter = 0\n",
    "\n",
    "    # Acumuladores globais\n",
    "    global_sims_sem = []\n",
    "    global_sims_ref = []\n",
    "    global_counts_sem = []\n",
    "    global_counts_ref = []\n",
    "\n",
    "    erros = 0\n",
    "\n",
    "    # Preparar diret√≥rio de backup\n",
    "    if backup_path:\n",
    "        backup_dir = os.path.dirname(backup_path)\n",
    "        if backup_dir and not os.path.exists(backup_dir):\n",
    "            os.makedirs(backup_dir, exist_ok=True)\n",
    "\n",
    "    loop = tqdm(df.iterrows(), total=total, desc=desc)\n",
    "\n",
    "    for idx, row in loop:\n",
    "        try:\n",
    "            full_question = make_question(row, inline=False)[0]\n",
    "            question_for_retriever = re.sub(r'\\([A-Z]\\)\\s*', '', row['question'])\n",
    "\n",
    "            # Recuperar mem√≥rias sem√¢nticas (COM SCORE COGNITIVO)\n",
    "            items_semantic, _ = manager_semantic.retrieve_ranked_memories(\n",
    "                query=question_for_retriever, \n",
    "                k=k_semantic,\n",
    "                threshold=threshold_semantic,\n",
    "                semantic_weight=semantic_weight,\n",
    "                show_scores=False\n",
    "            )\n",
    "\n",
    "            # Recuperar mem√≥rias reflexivas\n",
    "            items_reflection = []\n",
    "            if manager_reflection:\n",
    "                items_reflection = manager_reflection.retrieve_memories(\n",
    "                    query=question_for_retriever, \n",
    "                    k=k_reflection,\n",
    "                    threshold=threshold_reflection\n",
    "                )\n",
    "\n",
    "            # Combinar contextos\n",
    "            hybrid_context_formatted = format_hybrid_context(items_semantic, items_reflection)\n",
    "\n",
    "            count_sem, avg_sim_sem, raw_sims_sem = calcular_metricas_memoria(items_semantic)\n",
    "            count_ref, avg_sim_ref, raw_sims_ref = calcular_metricas_memoria(items_reflection)\n",
    "\n",
    "            global_counts_sem.append(count_sem)\n",
    "            global_counts_ref.append(count_ref)\n",
    "            global_sims_sem.extend(raw_sims_sem)\n",
    "            global_sims_ref.extend(raw_sims_ref)\n",
    "\n",
    "            response_obj = chain.invoke({\n",
    "                \"question\": full_question,\n",
    "                \"hybrid_context\": hybrid_context_formatted,\n",
    "            })\n",
    "\n",
    "            response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "\n",
    "            pred = extract_answer(\n",
    "                small_llm_model=gpt5_nano, \n",
    "                model_text_output=response_text,\n",
    "                question=full_question,\n",
    "            )\n",
    "\n",
    "            if pred not in ['A', 'B', 'C', 'D']:\n",
    "                pred = 'E'\n",
    "                erros += 1\n",
    "\n",
    "            target = row['answerKey']\n",
    "            is_correct = (pred == target)\n",
    "            acertos += int(is_correct)\n",
    "\n",
    "            # === UPDATE DE MEM√ìRIA (EMA) ===\n",
    "            if update_memory and items_semantic and pred != 'E':\n",
    "                used_ids = [m['doc_id'] for m in items_semantic]\n",
    "\n",
    "                feedback_scores = []\n",
    "                choices = ast.literal_eval(row['choices'])\n",
    "\n",
    "                try:\n",
    "                    correct_txt = choices['text'][number[target]] if target in number else str(target)\n",
    "                except (IndexError, KeyError):\n",
    "                    correct_txt = str(target)\n",
    "\n",
    "                try:\n",
    "                    chosen_txt = choices['text'][number[pred]] if pred in number else str(pred)\n",
    "                except (IndexError, KeyError):\n",
    "                    chosen_txt = str(pred)\n",
    "\n",
    "                status = \"SUCCESS\" if is_correct else \"FAILURE\"\n",
    "\n",
    "                for memory in items_semantic:\n",
    "                    fact_text = memory['content']\n",
    "                    try:\n",
    "                        score_response = score_chain.invoke({\n",
    "                            \"question\": full_question,\n",
    "                            \"chosen\": chosen_txt,\n",
    "                            \"correct\": correct_txt,\n",
    "                            \"outcome_status\": status,\n",
    "                            \"fact\": fact_text,\n",
    "                            \"reasoning\": response_text\n",
    "                        })\n",
    "                        _, score = parse_simple_score(score_response)\n",
    "                        feedback_scores.append(score)\n",
    "                    except:\n",
    "                        feedback_scores.append(0)\n",
    "\n",
    "                # Aplicar update EMA\n",
    "                manager_semantic.update_memories_feedback(used_ids, feedback_scores)\n",
    "\n",
    "            # === DECAY ===\n",
    "            if update_memory and decay_frequency > 0 and (idx + 1) % decay_frequency == 0:\n",
    "                decay_counter += 1\n",
    "                tqdm.write(f\"üåô [Q{idx+1}/{total}] Aplicando ciclo de decay #{decay_counter}...\")\n",
    "                manager_semantic.apply_decay_cycle(decay_threshold=0.0)\n",
    "\n",
    "            # === BACKUP ===\n",
    "            if backup_path and backup_frequency > 0 and (idx + 1) % backup_frequency == 0:\n",
    "                backup_counter += 1\n",
    "                df_partial = pd.DataFrame(resultados)\n",
    "                backup_file = backup_path.replace('.csv', '_backup.csv')\n",
    "                df_partial.to_csv(backup_file, index=False)\n",
    "\n",
    "            # Atualiza barra de progresso\n",
    "            acc_atual = (acertos / (loop.n + 1)) * 100\n",
    "            loop.set_postfix(\n",
    "                acc=f\"{acc_atual:.2f}%\",\n",
    "                sem=count_sem,\n",
    "                ref=count_ref,\n",
    "                decay=decay_counter,\n",
    "                bkp=backup_counter,\n",
    "                erros=erros\n",
    "            )\n",
    "\n",
    "            resultados.append({\n",
    "                'index': idx,\n",
    "                'question': full_question,\n",
    "                'retrieved_context': hybrid_context_formatted,\n",
    "                'retrieved_count_semantic': count_sem,\n",
    "                'retrieved_count_reflection': count_ref,\n",
    "                'avg_similarity_semantic': avg_sim_sem,\n",
    "                'avg_similarity_reflection': avg_sim_ref,\n",
    "                'raw_output': response_text,\n",
    "                'pred': pred,\n",
    "                'target': target,\n",
    "                'is_correct': is_correct,\n",
    "                'source': desc,\n",
    "                'erros': erros\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Erro no √≠ndice {idx}: {e}\")\n",
    "            resultados.append({\n",
    "                'index': idx,\n",
    "                'error': str(e),\n",
    "                'is_correct': False,\n",
    "                'retrieved_count_semantic': 0,\n",
    "                'retrieved_count_reflection': 0,\n",
    "                'avg_similarity_semantic': 0.0,\n",
    "                'avg_similarity_reflection': 0.0,\n",
    "            })\n",
    "\n",
    "    # === DECAY FINAL ===\n",
    "    if update_memory and decay_batch_size:\n",
    "        decay_counter += 1\n",
    "        print(f\"\\nüåô [FINAL] Aplicando ciclo de decay final (#{decay_counter})...\")\n",
    "        manager_semantic.apply_decay_cycle(decay_threshold=0.0)\n",
    "\n",
    "    # Sum√°rio final\n",
    "    if update_memory:\n",
    "        print(f\"\\nüìä SUM√ÅRIO DE MEM√ìRIA:\")\n",
    "        print(f\"   Ciclos de decay: {decay_counter}\")\n",
    "        print(f\"   Mem√≥rias sem√¢nticas finais: {manager_semantic._collection.count()}\")\n",
    "        if manager_reflection:\n",
    "            print(f\"   Mem√≥rias reflexivas: {manager_reflection._collection.count()}\")\n",
    "\n",
    "    if backup_path and backup_counter > 0:\n",
    "        print(f\"üíæ Total de backups salvos: {backup_counter}\")\n",
    "\n",
    "    return pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fd9f81",
   "metadata": {},
   "source": [
    "# 3) Constru√ß√£o dos Bancos de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771396fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados\n",
    "scientific_facts = pd.read_csv(\"../../scientific_facts_expanded.csv\").fillna(\"N/A\")\n",
    "\n",
    "df_semantic = scientific_facts[scientific_facts['scientific_fact'] != \"N/A\"].drop_duplicates(subset=['scientific_fact'])\n",
    "df_reflection = scientific_facts[scientific_facts['clean_reasoning'] != \"N/A\"].drop_duplicates(subset=['clean_reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RESET = False\n",
    "\n",
    "manager_semantic = CognitiveMemoryManager(\n",
    "    db_path=\"vectorstores/dynamic_pruning/chroma_semantic\",\n",
    "    embedding_model=embedding_model, alpha=0.1, decay_lambda=0.99, forget_threshold=-0.6\n",
    ").init_from_dataframe(df=df_semantic, content_col='scientific_fact',\n",
    "                      id_prefix='semantic', metadata_func=build_metadata_semantic, reset_db=FORCE_RESET)\n",
    "\n",
    "manager_reflection = SimpleVectorMemory(\n",
    "    db_path=\"vectorstores/dynamic_pruning/chroma_reflection\",\n",
    "    embedding_model=embedding_model\n",
    ").init_from_dataframe(df=df_reflection, content_col='clean_reasoning',\n",
    "                      id_prefix='reflection', metadata_func=build_metadata_reflection, reset_db=FORCE_RESET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54011726",
   "metadata": {},
   "source": [
    "# 4) Testing in one question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf434dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "k_semantic = 3\n",
    "k_reflection = 3\n",
    "row = test_df.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3963dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_text = make_question(row, inline=False)[0]\n",
    "question_clean = re.sub(r'\\([A-Z]\\)\\s*', '', q_text)\n",
    "\n",
    "# Recuperar mem√≥rias sem√¢nticas (com score cognitivo)\n",
    "items_semantic, _ = manager_semantic.retrieve_ranked_memories(\n",
    "    query=question_clean, \n",
    "    k=k_semantic, \n",
    "    threshold=0.33,\n",
    "    semantic_weight=0.5,\n",
    "    show_scores=True\n",
    ")\n",
    "\n",
    "# Recuperar mem√≥rias reflexivas\n",
    "items_reflection = manager_reflection.retrieve_memories(\n",
    "    query=question_clean, \n",
    "    k=k_reflection,\n",
    "    threshold=0.33\n",
    ")\n",
    "\n",
    "# Combinar contextos\n",
    "hybrid_context = format_hybrid_context(items_semantic, items_reflection, show_scores=True)\n",
    "\n",
    "print(\"Question: \\n\", q_text)\n",
    "print(\"\\nHybrid Context: \\n\", hybrid_context)\n",
    "\n",
    "count_sem, avg_sim_sem, raw_sims_sem = calcular_metricas_memoria(items_semantic)\n",
    "count_ref, avg_sim_ref, raw_sims_ref = calcular_metricas_memoria(items_reflection)\n",
    "print(\"\\nM√©tricas Sem√¢nticas: Count:\", count_sem, \"AVG_SIM:\", avg_sim_sem)\n",
    "print(\"M√©tricas Reflexivas: Count:\", count_ref, \"AVG_SIM:\", avg_sim_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "a = time.time()\n",
    "hybrid_context_clean = format_hybrid_context(items_semantic, items_reflection, show_scores=False)\n",
    "response_obj = rag_chain.invoke({\n",
    "    \"question\": q_text,\n",
    "    \"hybrid_context\": hybrid_context_clean,\n",
    "})\n",
    "print(\"Time to answer: \", round(time.time()-a,2))\n",
    "response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "\n",
    "pred = extract_answer(\n",
    "    small_llm_model=gpt5_nano, \n",
    "    model_text_output=response_text,\n",
    "    question=q_text,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "target = row['answerKey']\n",
    "is_correct = (pred == target)\n",
    "\n",
    "print(\"# Raw Response: \\n\", response_text)\n",
    "print(f\"# Alternative chosen: {pred} (Correta: {target})\")\n",
    "print(f\"# Resultado: {'CORRETO' if is_correct else 'ERRO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d70bb",
   "metadata": {},
   "source": [
    "# 5) Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDA√á√ÉO: Mem√≥ria ATIVA ‚Äî apenas update de score (EMA) + decay\n",
    "df_resultado_valid_dp = avaliar_dataset_dynamic_pruning(\n",
    "    df=valid_df, \n",
    "    chain=rag_chain,\n",
    "    manager_semantic=manager_semantic,\n",
    "    manager_reflection=manager_reflection, \n",
    "    k_semantic=3,\n",
    "    k_reflection=3,\n",
    "    threshold_reflection=0.24,\n",
    "    semantic_weight=0.7,\n",
    "    update_memory=True,\n",
    "    decay_batch_size=True,\n",
    "    decay_frequency=100,\n",
    "    backup_frequency=130,\n",
    "    backup_path=\"../../results/dynamic_pruning_valid.csv\",\n",
    "    desc=\"Valida√ß√£o (Dynamic Pruning ‚Äî EMA + Decay)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11e9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_valid_dp.to_csv(\"dynamic_pruning_valid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTE: Mem√≥ria CONGELADA\n",
    "df_resultado_test_dp = avaliar_dataset_dynamic_pruning(\n",
    "    df=test_df, \n",
    "    chain=rag_chain,\n",
    "    manager_semantic=manager_semantic,\n",
    "    manager_reflection=manager_reflection,\n",
    "    score_chain=score_chain,\n",
    "    k_semantic=3,\n",
    "    k_reflection=3,\n",
    "    threshold_reflection=0.24,\n",
    "    semantic_weight=0.7,\n",
    "    update_memory=False,\n",
    "    decay_batch_size=False,\n",
    "    backup_frequency=150,\n",
    "    backup_path=\"../../results/dynamic_pruning_test.csv\",\n",
    "    desc=\"Teste (Dynamic Pruning Frozen)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_test_dp.to_csv(\"dynamic_pruning_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_test_dp['is_correct'].mean()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
