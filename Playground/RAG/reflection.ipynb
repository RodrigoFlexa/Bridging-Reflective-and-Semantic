{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb147e6",
   "metadata": {},
   "source": [
    "# Reflection RAG System â€” Simple Retrieval\n",
    "\n",
    "Sistema de memÃ³ria reflexiva para comparaÃ§Ã£o com sistemas cognitivos:\n",
    "- **MemÃ³rias Reflexivas**: RaciocÃ­nios (reasoning traces) de questÃµes resolvidas\n",
    "- **Retrieval ClÃ¡ssico**: Apenas similaridade semÃ¢ntica (sem score cognitivo, sem decay, sem geraÃ§Ã£o/refinamento)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f287465",
   "metadata": {},
   "source": [
    "# 0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a758d415",
   "metadata": {},
   "source": [
    "### A) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07afc1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "# Add vectorstore & prompts folders to path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'vectorstore')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'prompts')))\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Memory managers & metadata builders\n",
    "from simple_vector_memory import SimpleVectorMemory, build_metadata_reflection\n",
    "\n",
    "# Prompt templates\n",
    "from templates import REFLECTION_TEMPLATE\n",
    "\n",
    "# Local imports\n",
    "from utils_notebook import (\n",
    "    SemanticCleaner,\n",
    "    calcular_metricas_memoria,\n",
    "    format_choices,\n",
    "    make_question\n",
    ")\n",
    "cleaner = SemanticCleaner()\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d575edd5",
   "metadata": {},
   "source": [
    "### B) Language Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['A','B','C','D']\n",
    "number = {'A':0,'B':1,'C':2,'D':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f292f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset do arc. (Caso nÃ£o tenha Ã© necessÃ¡rio instalar e guardar na pasta datasets como csv)\n",
    "# train_df = pd.read_csv(\"dataset/arc_challenge_train_processed.csv\")\n",
    "valid_df = pd.read_csv(\"../../dataset/arc_challenge_valid_processed.csv\")\n",
    "test_df = pd.read_csv(\"../../dataset/arc_challenge_test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c0f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM judge\n",
    "gpt5_nano = ChatOpenAI(model='gpt-5-nano-2025-08-07', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d1d9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi2 = ChatOllama(model=\"phi\", temperature=0) # phi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc44f4d",
   "metadata": {},
   "source": [
    "### C) Prompts and Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = PromptTemplate.from_template(REFLECTION_TEMPLATE)\n",
    "rag_chain = reflection_prompt | phi2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2c793",
   "metadata": {},
   "source": [
    "# 1) Utils Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06204ad8",
   "metadata": {},
   "source": [
    "### A) Context Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7097003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reflection_context(reflection_items, show_scores=False):\n",
    "    if not reflection_items:\n",
    "        return \"No specific reference information found.\"\n",
    "\n",
    "    formatted_parts = [\"### SOLVED EXAMPLES (PRACTICE)\"]\n",
    "    for i, reflection in enumerate(reflection_items, 1):\n",
    "        raw_q = reflection['metadata'].get('question', '')\n",
    "        q_text = raw_q.split('\\n')[0].strip()\n",
    "        r_text = reflection.get('content', '').strip()\n",
    "\n",
    "        if show_scores:\n",
    "            sim = reflection.get('similarity', 0)\n",
    "            block = f\"\"\"\n",
    "    * **Example Case #{i}** (Sim: {sim:.2f})\n",
    "        > Context: {q_text}\n",
    "        > Analysis: {r_text}\n",
    "\"\"\"\n",
    "        else:\n",
    "            block = f\"\"\"\n",
    "    * **Example Case #{i}**\n",
    "        > Context: {q_text}\n",
    "        > Analysis: {r_text}\n",
    "\"\"\"\n",
    "        formatted_parts.append(block)\n",
    "\n",
    "    return \"\\n\".join(formatted_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460a9f64",
   "metadata": {},
   "source": [
    "### B) Answer Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e70fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "def extract_answer(\n",
    "    small_llm_model,\n",
    "    model_text_output: str,\n",
    "    valid_labels: List[str] = None,\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    ExtraÃ§Ã£o robusta baseada em padrÃµes de alta confianÃ§a (Tier 1)\n",
    "    com fallback para LLM Juiz em caso de falha.\n",
    "    \"\"\"\n",
    "    if valid_labels is None:\n",
    "        valid_labels = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "    if not model_text_output:\n",
    "        return \"N/A\"\n",
    "\n",
    "    text = model_text_output.strip()\n",
    "\n",
    "    if \"```python\" in text or \"def solution\" in text:\n",
    "        if debug: print(\"âš  [CODE DETECTED] Enviando para LLM Judge.\")\n",
    "        return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "    strong_patterns = [\n",
    "        r\"\\\\boxed\\s*\\{\\s*([A-H])\\s*\\}\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]?\\s*(?:is)?\\s*(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]]\",\n",
    "        r\"The\\s+(?:correct\\s+)?(?:answer|option|choice)\\s+is\\s*(?:Option)?\\s*[:\\-]?\\s*[\\(\\[]([A-H])[\\)\\]]\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]\\s*(?:is\\s+)?(?:Option\\s+)?([A-H])(?=\\s|\\.|,|!|\\?|$)\",\n",
    "        r\"(?:Therefore|Thus|Hence|So),\\s*(?:the\\s+answer\\s+is\\s*)?(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]]\",\n",
    "        r\"(?:^|\\n)\\s*Answer\\s*:\\s*([A-H])(?=\\s|$|\\.|\\,)\",\n",
    "        r\"\\*\\*([A-H])\\*\\*\",\n",
    "        r\"^([A-H])$\"\n",
    "    ]\n",
    "\n",
    "    for pattern in strong_patterns:\n",
    "        matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "        if matches:\n",
    "            last_match = matches[-1]\n",
    "            candidate = last_match.group(1).upper()\n",
    "            if candidate in valid_labels:\n",
    "                if debug:\n",
    "                    print(f\"âœ“ [REGEX] PadrÃ£o encontrado: '{pattern}' -> {candidate}\")\n",
    "                return candidate\n",
    "\n",
    "    if debug: print(\"âœ— [REGEX] Falha nos padrÃµes fortes. Chamando LLM Judge.\")\n",
    "    return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "\n",
    "def _llm_judge(\n",
    "    small_llm_model,\n",
    "    text: str,\n",
    "    valid_labels: List[str],\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    context_block = \"\"\n",
    "    if question:\n",
    "        context_block = f\"\"\"\n",
    "### CONTEXT (Use this to infer the answer letter):\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are an Answer Extraction Bot.\n",
    "Your ONLY job is to identify which option the following \"Model Output\" concluded is correct.\n",
    "\n",
    "{context_block}\n",
    "\n",
    "### Model Output to Analyze:\n",
    "{text}\n",
    "\n",
    "### Instructions:\n",
    "1. Look for an explicit answer (e.g., \"Answer: A\").\n",
    "2. If NO explicit letter is found, read the conclusion of the \"Model Output\" and match it against the Options in the Context. **Infer the letter.**\n",
    "   - Example: If Context has \"(A) 5 (B) 10\" and Model Output says \"The result is 10\", you must output B.\n",
    "3. Do NOT calculate or solve the problem yourself. Trust the \"Model Output\".\n",
    "4. If the model refuses to answer or is unclear, return \"E\".\n",
    "\n",
    "Output format: Just the single letter (A, B, C, D, or E). No other text.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = small_llm_model.invoke(prompt)\n",
    "        output = response.content if hasattr(response, 'content') else str(response)\n",
    "        clean_cand = re.sub(r\"[^A-E]\", \"\", output.strip().upper())\n",
    "\n",
    "        if len(clean_cand) > 1:\n",
    "            clean_cand = clean_cand[-1]\n",
    "\n",
    "        if clean_cand in valid_labels:\n",
    "            if debug: print(f\"âœ“ [LLM JUIZ] Inferido: {clean_cand}\")\n",
    "            return clean_cand\n",
    "\n",
    "        return \"N/A\"\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug: print(f\"âœ— [ERRO JUIZ] {e}\")\n",
    "        return \"N/A\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2aecd",
   "metadata": {},
   "source": [
    "# 2) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe436286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_dataset_rag_reflection(\n",
    "    df,\n",
    "    chain=rag_chain,\n",
    "    manager_reflection=None,\n",
    "    k_reflection=3,\n",
    "    threshold_reflection=0.24,\n",
    "    backup_frequency=160,\n",
    "    backup_path=None,\n",
    "    desc=\"RAG Reflection\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Avalia dataset com sistema de memÃ³ria reflexiva simples.\n",
    "    RecuperaÃ§Ã£o por similaridade pura â€” sem score cognitivo, sem EMA, sem decay.\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    acertos = 0\n",
    "    total = len(df)\n",
    "    backup_counter = 0\n",
    "\n",
    "    global_sims = []\n",
    "    global_counts = []\n",
    "    erros = 0\n",
    "\n",
    "    # Preparar diretÃ³rio de backup\n",
    "    if backup_path:\n",
    "        backup_dir = os.path.dirname(backup_path)\n",
    "        if backup_dir and not os.path.exists(backup_dir):\n",
    "            os.makedirs(backup_dir, exist_ok=True)\n",
    "\n",
    "    loop = tqdm(df.iterrows(), total=total, desc=desc)\n",
    "\n",
    "    for idx, row in loop:\n",
    "        try:\n",
    "            full_question = make_question(row, inline=False)[0]\n",
    "            question_for_retriever = re.sub(r'\\([A-Z]\\)\\s*', '', row['question'])\n",
    "\n",
    "            # Recuperar memÃ³rias reflexivas\n",
    "            items_reflection = manager_reflection.retrieve_memories(\n",
    "                query=question_for_retriever,\n",
    "                k=k_reflection,\n",
    "                threshold=threshold_reflection\n",
    "            )\n",
    "\n",
    "            # Formatar contexto de reflexÃ£o\n",
    "            reflection_context_formatted = format_reflection_context(items_reflection)\n",
    "\n",
    "            count, avg_sim, raw_sims = calcular_metricas_memoria(items_reflection)\n",
    "            global_counts.append(count)\n",
    "            global_sims.extend(raw_sims)\n",
    "\n",
    "            response_obj = chain.invoke({\n",
    "                \"question\": full_question,\n",
    "                \"reflection_context\": reflection_context_formatted,\n",
    "            })\n",
    "\n",
    "            response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "\n",
    "            pred = extract_answer(\n",
    "                small_llm_model=gpt5_nano,\n",
    "                model_text_output=response_text,\n",
    "                question=full_question,\n",
    "            )\n",
    "\n",
    "            if pred not in ['A', 'B', 'C', 'D']:\n",
    "                pred = 'E'\n",
    "                erros += 1\n",
    "\n",
    "            target = row['answerKey']\n",
    "            is_correct = (pred == target)\n",
    "            acertos += int(is_correct)\n",
    "\n",
    "            # === BACKUP ===\n",
    "            if backup_path and backup_frequency > 0 and (idx + 1) % backup_frequency == 0:\n",
    "                backup_counter += 1\n",
    "                df_partial = pd.DataFrame(resultados)\n",
    "                backup_file = backup_path.replace('.csv', '_backup.csv')\n",
    "                df_partial.to_csv(backup_file, index=False)\n",
    "\n",
    "            # Atualiza barra de progresso\n",
    "            acc_atual = (acertos / (loop.n + 1)) * 100\n",
    "            loop.set_postfix(\n",
    "                acc=f\"{acc_atual:.2f}%\",\n",
    "                ref=count,\n",
    "                bkp=backup_counter,\n",
    "                erros=erros\n",
    "            )\n",
    "\n",
    "            resultados.append({\n",
    "                'index': idx,\n",
    "                'question': full_question,\n",
    "                'retrieved_context': reflection_context_formatted,\n",
    "                'retrieved_count': count,\n",
    "                'avg_similarity': avg_sim,\n",
    "                'raw_output': response_text,\n",
    "                'pred': pred,\n",
    "                'target': target,\n",
    "                'is_correct': is_correct,\n",
    "                'source': desc,\n",
    "                'erros': erros\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Erro no Ã­ndice {idx}: {e}\")\n",
    "            resultados.append({\n",
    "                'index': idx,\n",
    "                'error': str(e),\n",
    "                'is_correct': False,\n",
    "                'retrieved_count': 0,\n",
    "                'avg_similarity': 0.0,\n",
    "            })\n",
    "\n",
    "    if backup_path and backup_counter > 0:\n",
    "        print(f\"ðŸ’¾ Total de backups salvos: {backup_counter}\")\n",
    "\n",
    "    return pd.DataFrame(resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f534ed43",
   "metadata": {},
   "source": [
    "# 3) ConstruÃ§Ã£o dos Bancos de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados\n",
    "scientific_facts = pd.read_csv(\"../../scientific_facts_expanded.csv\").fillna(\"N/A\")\n",
    "\n",
    "df_reflection = scientific_facts[scientific_facts['clean_reasoning'] != \"N/A\"].drop_duplicates(subset=['clean_reasoning'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6f443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ [Sistema] Reset total solicitado. Apagando vectorstores/reflection/chroma_reflection...\n",
      "ðŸ“‚ Carregando base de vetores de: vectorstores/reflection/chroma_reflection\n",
      "âœ… Base carregada com 1084 itens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SimpleVectorMemory at 0x23816049d50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FORCE_RESET = False\n",
    "\n",
    "manager_reflection = SimpleVectorMemory(\n",
    "    db_path=\"vectorstores/reflection_rag/chroma_reflection\",\n",
    "    embedding_model=embedding_model\n",
    ").init_from_dataframe(df=df_reflection, content_col='clean_reasoning',\n",
    "                      id_prefix='reflection', metadata_func=build_metadata_reflection, reset_db=FORCE_RESET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693be348",
   "metadata": {},
   "source": [
    "# 4) Testing in one question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081bf191",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "k_reflection = 3\n",
    "row = test_df.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f51f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " Students visited the Morris W. Offit telescope located at the Maryland Space Grant Observatory in Baltimore. They learned about the stars, planets, and moon. The students recorded the information below. â€¢ Star patterns stay the same, but their locations in the sky seem to change. â€¢ The sun, planets, and moon appear to move in the sky. â€¢ Proxima Centauri is the nearest star to our solar system. â€¢ Polaris is a star that is part of a pattern of stars called the Little Dipper. Which statement best explains why the sun appears to move across the sky each day?\n",
      "(A) The sun revolves around Earth.\n",
      "(B) Earth rotates around the sun.\n",
      "(C) The sun revolves on its axis.\n",
      "(D) Earth rotates on its axis.\n",
      "\n",
      "Contexto:  \n",
      "        ### Reference Example 1\n",
      "        **Question:** Stars are organized into patterns called constellations. One constellation is named Leo. Which statement best explains why Leo appears in different areas of the sky throughout the year?\n",
      "        **Reference Logic:** The subject appears in different areas of the sky throughout the year because of Earth's revolution around the Sun. As Earth orbits the Sun, it moves from one position to another relative to the stars. This movement causes the appearance of constellations like Leo to change over time.\n",
      "        ---\n",
      "        \n",
      "\n",
      "        ### Reference Example 2\n",
      "        **Question:** Approximately 59% of the Moon is visible from Earth because the Moon rotates and revolves in the same period. Which most likely causes this phenomenon?\n",
      "        **Reference Logic:** The phenomenon can be explained by considering the relative positions and movements of the Earth, the subject, and Sun. As the the subject rotates on its axis and revolves around the Earth, it is also moving in an orbit around the Sun. At any given time, different parts of the the subject are illuminated by sunlight as seen from Earth.\n",
      "        ---\n",
      "        \n",
      "\n",
      "MÃ©tricas das memÃ³rias recuperadas: \n",
      "Count:  2  AVG_SIM:  0.560680478811264  Raw_SIM:  [0.5758861899375916, 0.5454747676849365]\n"
     ]
    }
   ],
   "source": [
    "q_text = make_question(row, inline=False)[0]\n",
    "question_clean = re.sub(r'\\([A-Z]\\)\\s*', '', q_text)\n",
    "\n",
    "# Recuperar memÃ³rias reflexivas\n",
    "items_reflection = manager_reflection.retrieve_memories(\n",
    "    query=question_clean,\n",
    "    k=k_reflection,\n",
    "    threshold=0.33\n",
    ")\n",
    "\n",
    "# Formatar contexto de reflexÃ£o\n",
    "reflection_context = format_reflection_context(items_reflection, show_scores=True)\n",
    "\n",
    "print(\"Question: \\n\", q_text)\n",
    "print(\"\\nReflection Context: \\n\", reflection_context)\n",
    "\n",
    "count, avg_sim, raw_sims = calcular_metricas_memoria(items_reflection)\n",
    "print(\"\\nMÃ©tricas Reflexivas: Count:\", count, \"AVG_SIM:\", avg_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c7db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ— [REGEX] Falha nos padrÃµes fortes. Chamando LLM Judge.\n",
      "âœ“ [LLM JUIZ] Inferido: B\n",
      "# Raw Response: \n",
      "  (B) Earth rotates around the sun.\n",
      "\n",
      "# Alternative chosen:  B (Correta: D)\n",
      "----------------------------------------------\n",
      "Tempo para rodar extraction:  1.55\n",
      "Tempo para rodar invoke:  0.53\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "a = time.time()\n",
    "reflection_context_clean = format_reflection_context(items_reflection, show_scores=False)\n",
    "response_obj = rag_chain.invoke({\n",
    "    \"question\": q_text,\n",
    "    \"reflection_context\": reflection_context_clean,\n",
    "})\n",
    "print(\"Time to answer: \", round(time.time()-a, 2))\n",
    "response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "\n",
    "pred = extract_answer(\n",
    "    small_llm_model=gpt5_nano,\n",
    "    model_text_output=response_text,\n",
    "    question=q_text,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "target = row['answerKey']\n",
    "is_correct = (pred == target)\n",
    "\n",
    "print(\"# Raw Response: \\n\", response_text)\n",
    "print(f\"# Alternative chosen: {pred} (Correta: {target})\")\n",
    "print(f\"# Resultado: {'CORRETO' if is_correct else 'ERRO'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e5743",
   "metadata": {},
   "source": [
    "# 5) Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba26ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset ValidaÃ§Ã£o: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [10:58<00:00,  2.20s/it, acc=70.23%, avg_sim=0.6164, erros=0, n_mem=3]\n"
     ]
    }
   ],
   "source": [
    "# VALIDAÃ‡ÃƒO\n",
    "df_resultado_valid = avaliar_dataset_rag_reflection(\n",
    "    df=valid_df,\n",
    "    chain=rag_chain,\n",
    "    manager_reflection=manager_reflection,\n",
    "    k_reflection=3,\n",
    "    threshold_reflection=0.24,\n",
    "    backup_frequency=130,\n",
    "    backup_path=\"../../results/rag_reflection_valid.csv\",\n",
    "    desc=\"ValidaÃ§Ã£o (RAG Reflection)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b293910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_valid.to_csv(\"rag_reflection_valid.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e947c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset Teste: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1172/1172 [33:58<00:00,  1.74s/it, acc=68.86%, avg_sim=0.5434, erros=0, n_mem=3] \n"
     ]
    }
   ],
   "source": [
    "# TESTE\n",
    "df_resultado_test = avaliar_dataset_rag_reflection(\n",
    "    df=test_df,\n",
    "    chain=rag_chain,\n",
    "    manager_reflection=manager_reflection,\n",
    "    k_reflection=3,\n",
    "    threshold_reflection=0.24,\n",
    "    backup_frequency=150,\n",
    "    backup_path=\"../../results/rag_reflection_test.csv\",\n",
    "    desc=\"Teste (RAG Reflection)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19766dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Salvando resultados finais...\n",
      "   âœ“ reflection_test.csv salvo\n",
      "   âœ“ reflection_valid.csv salvo\n",
      "\n",
      "âœ… Todos os arquivos salvos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "df_resultado_test.to_csv(\"rag_reflection_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd677b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_test['is_correct'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
