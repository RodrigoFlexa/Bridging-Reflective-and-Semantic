{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b925512b",
   "metadata": {},
   "source": [
    "# Hybrid RAG System (Semantic + Reflection) ‚Äî Simple Retrieval\n",
    "\n",
    "Sistema h√≠brido de recupera√ß√£o simples sem mecanismos cognitivos:\n",
    "- **Mem√≥rias Sem√¢nticas**: Fatos cient√≠ficos extra√≠dos\n",
    "- **Mem√≥rias Reflexivas**: Racioc√≠nios de problemas resolvidos\n",
    "- **Sem update de score, sem decay, sem gera√ß√£o/refinamento de fatos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238e862",
   "metadata": {},
   "source": [
    "# 0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad8e35",
   "metadata": {},
   "source": [
    "### A) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e16564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "# Add vectorstore & prompts folders to path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'vectorstore')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'prompts')))\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Memory managers & metadata builders\n",
    "from simple_vector_memory import SimpleVectorMemory, build_metadata_reflection\n",
    "\n",
    "# Prompt templates\n",
    "from templates import HYBRID_TEMPLATE\n",
    "\n",
    "# Local imports\n",
    "from utils_notebook import (\n",
    "    SemanticCleaner,\n",
    "    calcular_metricas_memoria,\n",
    "    format_choices,\n",
    "    make_question\n",
    ")\n",
    "cleaner = SemanticCleaner()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d7403",
   "metadata": {},
   "source": [
    "### B) Language Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['A','B','C','D']\n",
    "number = {'A':0,'B':1,'C':2,'D':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset do arc\n",
    "valid_df = pd.read_csv(\"../../dataset/arc_challenge_valid_processed.csv\")\n",
    "test_df = pd.read_csv(\"../../dataset/arc_challenge_test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171bb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM judge\n",
    "gpt5_nano = ChatOpenAI(model='gpt-5-nano-2025-08-07', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi2 = ChatOllama(model=\"phi\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9fb04",
   "metadata": {},
   "source": [
    "### C) Prompts and Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea85fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_prompt = PromptTemplate.from_template(HYBRID_TEMPLATE)\n",
    "rag_chain = hybrid_prompt | phi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72dd92d",
   "metadata": {},
   "source": [
    "# 1) Utils Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b905e",
   "metadata": {},
   "source": [
    "### A) Context Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_hybrid_context(semantic_items, reflection_items, show_scores=False):\n",
    "    formatted_parts = []\n",
    "\n",
    "    if semantic_items:\n",
    "        formatted_parts.append(\"### PART 1: SCIENTIFIC PRINCIPLES (THEORY)\")\n",
    "        for i, fact in enumerate(semantic_items, 1):\n",
    "            raw_q = fact['metadata'].get('question', '')\n",
    "            q_text = raw_q.split('\\n')[0].strip()\n",
    "            f_text = fact.get('content', '').strip()\n",
    "\n",
    "            if show_scores:\n",
    "                sim = fact.get('similarity', 0)\n",
    "                block = f\"\"\"\n",
    "    * **Principle #{i}** (Sim: {sim:.2f})\n",
    "        Context: \"{q_text}\"\n",
    "        Fact: \"{f_text}\"\n",
    "\"\"\"\n",
    "            else:\n",
    "                block = f\"\"\"\n",
    "    * **Principle #{i}**\n",
    "        * Context: \"{q_text}\"\n",
    "        * Fact: \"{f_text}\"\n",
    "\"\"\"\n",
    "            formatted_parts.append(block)\n",
    "\n",
    "    if reflection_items:\n",
    "        formatted_parts.append(\"\\n### PART 2: SOLVED EXAMPLES (PRACTICE)\")\n",
    "        for i, reflection in enumerate(reflection_items, 1):\n",
    "            raw_q = reflection['metadata'].get('question', '')\n",
    "            q_text = raw_q.split('\\n')[0].strip()\n",
    "            r_text = reflection.get('content', '').strip()\n",
    "\n",
    "            block = f\"\"\"\n",
    "    * **Example Case #{i}**\n",
    "        > Context: {q_text}\n",
    "        > Analysis: {r_text}\n",
    "\"\"\"\n",
    "            formatted_parts.append(block)\n",
    "\n",
    "    if not formatted_parts:\n",
    "        return \"No specific reference information found.\"\n",
    "\n",
    "    return \"\\n\".join(formatted_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f47c1",
   "metadata": {},
   "source": [
    "### B) Answer Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "def extract_answer(\n",
    "    small_llm_model,\n",
    "    model_text_output: str,\n",
    "    valid_labels: List[str] = None,\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extra√ß√£o robusta baseada em padr√µes de alta confian√ßa (Tier 1)\n",
    "    com fallback para LLM Juiz em caso de falha.\n",
    "    \"\"\"\n",
    "    if valid_labels is None:\n",
    "        valid_labels = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "    if not model_text_output:\n",
    "        return \"N/A\"\n",
    "\n",
    "    text = model_text_output.strip()\n",
    "\n",
    "    if \"```python\" in text or \"def solution\" in text:\n",
    "        if debug: print(\"‚ö† [CODE DETECTED] Enviando para LLM Judge.\")\n",
    "        return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "    strong_patterns = [\n",
    "        r\"\\\\boxed\\s*\\{\\s*([A-H])\\s*\\}\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]?\\s*(?:is)?\\s*(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]]\",\n",
    "        r\"The\\s+(?:correct\\s+)?(?:answer|option|choice)\\s+is\\s*(?:Option)?\\s*[:\\-]?\\s*[\\(\\[]([A-H])[\\)\\]]\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]\\s*(?:is\\s+)?(?:Option\\s+)?([A-H])(?=\\s|\\.|,|!|\\?|$)\",\n",
    "        r\"(?:Therefore|Thus|Hence|So),\\s*(?:the\\s+answer\\s+is\\s*)?(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]]\",\n",
    "        r\"(?:^|\\n)\\s*Answer\\s*:\\s*([A-H])(?=\\s|$|\\.|\\,)\",\n",
    "        r\"\\*\\*([A-H])\\*\\*\",\n",
    "        r\"^([A-H])$\"\n",
    "    ]\n",
    "\n",
    "    for pattern in strong_patterns:\n",
    "        matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "        if matches:\n",
    "            last_match = matches[-1]\n",
    "            candidate = last_match.group(1).upper()\n",
    "            if candidate in valid_labels:\n",
    "                if debug:\n",
    "                    print(f\"‚úì [REGEX] Padr√£o encontrado: '{pattern}' -> {candidate}\")\n",
    "                return candidate\n",
    "\n",
    "    if debug: print(\"‚úó [REGEX] Falha nos padr√µes fortes. Chamando LLM Judge.\")\n",
    "    return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "\n",
    "def _llm_judge(\n",
    "    small_llm_model,\n",
    "    text: str,\n",
    "    valid_labels: List[str],\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    context_block = \"\"\n",
    "    if question:\n",
    "        context_block = f\"\"\"\n",
    "### CONTEXT (Use this to infer the answer letter):\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are an Answer Extraction Bot.\n",
    "Your ONLY job is to identify which option the following \"Model Output\" concluded is correct.\n",
    "\n",
    "{context_block}\n",
    "\n",
    "### Model Output to Analyze:\n",
    "{text}\n",
    "\n",
    "### Instructions:\n",
    "1. Look for an explicit answer (e.g., \"Answer: A\").\n",
    "2. If NO explicit letter is found, read the conclusion of the \"Model Output\" and match it against the Options in the Context. **Infer the letter.**\n",
    "   - Example: If Context has \"(A) 5 (B) 10\" and Model Output says \"The result is 10\", you must output B.\n",
    "3. Do NOT calculate or solve the problem yourself. Trust the \"Model Output\".\n",
    "4. If the model refuses to answer or is unclear, return \"E\".\n",
    "\n",
    "Output format: Just the single letter (A, B, C, D, or E). No other text.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = small_llm_model.invoke(prompt)\n",
    "        output = response.content if hasattr(response, 'content') else str(response)\n",
    "        clean_cand = re.sub(r\"[^A-E]\", \"\", output.strip().upper())\n",
    "\n",
    "        if len(clean_cand) > 1:\n",
    "            clean_cand = clean_cand[-1]\n",
    "\n",
    "        if clean_cand in valid_labels:\n",
    "            if debug: print(f\"‚úì [LLM JUIZ] Inferido: {clean_cand}\")\n",
    "            return clean_cand\n",
    "\n",
    "        return \"N/A\"\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug: print(f\"‚úó [ERRO JUIZ] {e}\")\n",
    "        return \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60465b6b",
   "metadata": {},
   "source": [
    "# 2) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_dataset_rag_hybrid(\n",
    "    df,\n",
    "    chain=rag_chain,\n",
    "    manager_semantic=None,\n",
    "    manager_reflection=None,\n",
    "    k_semantic=3,\n",
    "    k_reflection=3,\n",
    "    threshold_semantic=0.24,\n",
    "    threshold_reflection=0.24,\n",
    "    backup_frequency=160,\n",
    "    backup_path=None,\n",
    "    desc=\"RAG Hybrid (Semantic + Reflection)\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Avalia dataset com sistema h√≠brido simples (sem√¢ntica + reflex√£o).\n",
    "    Recupera√ß√£o por similaridade pura ‚Äî sem score cognitivo, sem EMA, sem decay.\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    acertos = 0\n",
    "    total = len(df)\n",
    "    backup_counter = 0\n",
    "\n",
    "    # Acumuladores globais\n",
    "    global_sims_sem = []\n",
    "    global_sims_ref = []\n",
    "    global_counts_sem = []\n",
    "    global_counts_ref = []\n",
    "\n",
    "    erros = 0\n",
    "\n",
    "    # Preparar diret√≥rio de backup\n",
    "    if backup_path:\n",
    "        backup_dir = os.path.dirname(backup_path)\n",
    "        if backup_dir and not os.path.exists(backup_dir):\n",
    "            os.makedirs(backup_dir, exist_ok=True)\n",
    "\n",
    "    loop = tqdm(df.iterrows(), total=total, desc=desc)\n",
    "\n",
    "    for idx, row in loop:\n",
    "        try:\n",
    "            full_question = make_question(row, inline=False)[0]\n",
    "            question_for_retriever = re.sub(r'\\([A-Z]\\)\\s*', '', row['question'])\n",
    "\n",
    "            # Recuperar mem√≥rias sem√¢nticas (similaridade simples)\n",
    "            items_semantic = manager_semantic.retrieve_memories(\n",
    "                query=question_for_retriever,\n",
    "                k=k_semantic,\n",
    "                threshold=threshold_semantic\n",
    "            )\n",
    "\n",
    "            # Recuperar mem√≥rias reflexivas\n",
    "            items_reflection = []\n",
    "            if manager_reflection:\n",
    "                items_reflection = manager_reflection.retrieve_memories(\n",
    "                    query=question_for_retriever,\n",
    "                    k=k_reflection,\n",
    "                    threshold=threshold_reflection\n",
    "                )\n",
    "\n",
    "            # Combinar contextos\n",
    "            hybrid_context_formatted = format_hybrid_context(items_semantic, items_reflection)\n",
    "\n",
    "            count_sem, avg_sim_sem, raw_sims_sem = calcular_metricas_memoria(items_semantic)\n",
    "            count_ref, avg_sim_ref, raw_sims_ref = calcular_metricas_memoria(items_reflection)\n",
    "\n",
    "            global_counts_sem.append(count_sem)\n",
    "            global_counts_ref.append(count_ref)\n",
    "            global_sims_sem.extend(raw_sims_sem)\n",
    "            global_sims_ref.extend(raw_sims_ref)\n",
    "\n",
    "            response_obj = chain.invoke({\n",
    "                \"question\": full_question,\n",
    "                \"hybrid_context\": hybrid_context_formatted,\n",
    "            })\n",
    "\n",
    "            response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "\n",
    "            pred = extract_answer(\n",
    "                small_llm_model=gpt5_nano,\n",
    "                model_text_output=response_text,\n",
    "                question=full_question,\n",
    "            )\n",
    "\n",
    "            if pred not in ['A', 'B', 'C', 'D']:\n",
    "                pred = 'E'\n",
    "                erros += 1\n",
    "\n",
    "            target = row['answerKey']\n",
    "            is_correct = (pred == target)\n",
    "            acertos += int(is_correct)\n",
    "\n",
    "            # === BACKUP ===\n",
    "            if backup_path and backup_frequency > 0 and (idx + 1) % backup_frequency == 0:\n",
    "                backup_counter += 1\n",
    "                df_partial = pd.DataFrame(resultados)\n",
    "                backup_file = backup_path.replace('.csv', '_backup.csv')\n",
    "                df_partial.to_csv(backup_file, index=False)\n",
    "\n",
    "            # Atualiza barra de progresso\n",
    "            acc_atual = (acertos / (loop.n + 1)) * 100\n",
    "            loop.set_postfix(\n",
    "                acc=f\"{acc_atual:.2f}%\",\n",
    "                sem=count_sem,\n",
    "                ref=count_ref,\n",
    "                bkp=backup_counter,\n",
    "                erros=erros\n",
    "            )\n",
    "\n",
    "            resultados.append({\n",
    "                'index': idx,\n",
    "                'question': full_question,\n",
    "                'retrieved_context': hybrid_context_formatted,\n",
    "                'retrieved_count_semantic': count_sem,\n",
    "                'retrieved_count_reflection': count_ref,\n",
    "                'avg_similarity_semantic': avg_sim_sem,\n",
    "                'avg_similarity_reflection': avg_sim_ref,\n",
    "                'raw_output': response_text,\n",
    "                'pred': pred,\n",
    "                'target': target,\n",
    "                'is_correct': is_correct,\n",
    "                'source': desc,\n",
    "                'erros': erros\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Erro no √≠ndice {idx}: {e}\")\n",
    "            resultados.append({\n",
    "                'index': idx,\n",
    "                'error': str(e),\n",
    "                'is_correct': False,\n",
    "                'retrieved_count_semantic': 0,\n",
    "                'retrieved_count_reflection': 0,\n",
    "                'avg_similarity_semantic': 0.0,\n",
    "                'avg_similarity_reflection': 0.0,\n",
    "            })\n",
    "\n",
    "    if backup_path and backup_counter > 0:\n",
    "        print(f\"üíæ Total de backups salvos: {backup_counter}\")\n",
    "\n",
    "    return pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177f58c",
   "metadata": {},
   "source": [
    "# 3) Constru√ß√£o dos Bancos de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados\n",
    "scientific_facts = pd.read_csv(\"../../scientific_facts_expanded.csv\").fillna(\"N/A\")\n",
    "\n",
    "df_semantic = scientific_facts[scientific_facts['scientific_fact'] != \"N/A\"].drop_duplicates(subset=['scientific_fact'])\n",
    "df_reflection = scientific_facts[scientific_facts['clean_reasoning'] != \"N/A\"].drop_duplicates(subset=['clean_reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fe70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RESET = False\n",
    "\n",
    "def build_metadata_semantic(index, row, doc_id):\n",
    "    return {\n",
    "        \"chroma_id\": doc_id,\n",
    "        \"original_id\": str(row['id']),\n",
    "        \"question\": str(row['question']),\n",
    "        \"correct_answer\": str(row['correct_answer']) if pd.notna(row.get('correct_answer')) else \"None\",\n",
    "        \"scientific_fact\": row['scientific_fact'],\n",
    "        \"origin\": \"original_training\",\n",
    "        \"type\": \"semantic_memory\",\n",
    "    }\n",
    "\n",
    "manager_semantic = SimpleVectorMemory(\n",
    "    db_path=\"vectorstores/hybrid_rag/chroma_semantic\",\n",
    "    embedding_model=embedding_model\n",
    ").init_from_dataframe(df=df_semantic, content_col='scientific_fact',\n",
    "                      id_prefix='semantic', metadata_func=build_metadata_semantic, reset_db=FORCE_RESET)\n",
    "\n",
    "manager_reflection = SimpleVectorMemory(\n",
    "    db_path=\"vectorstores/hybrid_rag/chroma_reflection\",\n",
    "    embedding_model=embedding_model\n",
    ").init_from_dataframe(df=df_reflection, content_col='clean_reasoning',\n",
    "                      id_prefix='reflection', metadata_func=build_metadata_reflection, reset_db=FORCE_RESET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe72e3",
   "metadata": {},
   "source": [
    "# 4) Testing in one question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f2c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "k_semantic = 3\n",
    "k_reflection = 3\n",
    "row = test_df.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb93085",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_text = make_question(row, inline=False)[0]\n",
    "question_clean = re.sub(r'\\([A-Z]\\)\\s*', '', q_text)\n",
    "\n",
    "# Recuperar mem√≥rias sem√¢nticas\n",
    "items_semantic = manager_semantic.retrieve_memories(\n",
    "    query=question_clean,\n",
    "    k=k_semantic,\n",
    "    threshold=0.33\n",
    ")\n",
    "\n",
    "# Recuperar mem√≥rias reflexivas\n",
    "items_reflection = manager_reflection.retrieve_memories(\n",
    "    query=question_clean,\n",
    "    k=k_reflection,\n",
    "    threshold=0.33\n",
    ")\n",
    "\n",
    "# Combinar contextos\n",
    "hybrid_context = format_hybrid_context(items_semantic, items_reflection, show_scores=True)\n",
    "\n",
    "print(\"Question: \\n\", q_text)\n",
    "print(\"\\nHybrid Context: \\n\", hybrid_context)\n",
    "\n",
    "count_sem, avg_sim_sem, raw_sims_sem = calcular_metricas_memoria(items_semantic)\n",
    "count_ref, avg_sim_ref, raw_sims_ref = calcular_metricas_memoria(items_reflection)\n",
    "print(\"\\nM√©tricas Sem√¢nticas: Count:\", count_sem, \"AVG_SIM:\", avg_sim_sem)\n",
    "print(\"M√©tricas Reflexivas: Count:\", count_ref, \"AVG_SIM:\", avg_sim_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "a = time.time()\n",
    "hybrid_context_clean = format_hybrid_context(items_semantic, items_reflection, show_scores=False)\n",
    "response_obj = rag_chain.invoke({\n",
    "    \"question\": q_text,\n",
    "    \"hybrid_context\": hybrid_context_clean,\n",
    "})\n",
    "print(\"Time to answer: \", round(time.time()-a, 2))\n",
    "response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "\n",
    "pred = extract_answer(\n",
    "    small_llm_model=gpt5_nano,\n",
    "    model_text_output=response_text,\n",
    "    question=q_text,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "target = row['answerKey']\n",
    "is_correct = (pred == target)\n",
    "\n",
    "print(\"# Raw Response: \\n\", response_text)\n",
    "print(f\"# Alternative chosen: {pred} (Correta: {target})\")\n",
    "print(f\"# Resultado: {'CORRETO' if is_correct else 'ERRO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9521b320",
   "metadata": {},
   "source": [
    "# 5) Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3fa659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDA√á√ÉO\n",
    "df_resultado_valid = avaliar_dataset_rag_hybrid(\n",
    "    df=valid_df,\n",
    "    chain=rag_chain,\n",
    "    manager_semantic=manager_semantic,\n",
    "    manager_reflection=manager_reflection,\n",
    "    k_semantic=3,\n",
    "    k_reflection=3,\n",
    "    threshold_semantic=0.24,\n",
    "    threshold_reflection=0.24,\n",
    "    backup_frequency=130,\n",
    "    backup_path=\"../../results/rag_hybrid_valid.csv\",\n",
    "    desc=\"Valida√ß√£o (RAG Hybrid)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef5271",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_valid.to_csv(\"rag_hybrid_valid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTE\n",
    "df_resultado_test = avaliar_dataset_rag_hybrid(\n",
    "    df=test_df,\n",
    "    chain=rag_chain,\n",
    "    manager_semantic=manager_semantic,\n",
    "    manager_reflection=manager_reflection,\n",
    "    k_semantic=3,\n",
    "    k_reflection=3,\n",
    "    threshold_semantic=0.24,\n",
    "    threshold_reflection=0.24,\n",
    "    backup_frequency=150,\n",
    "    backup_path=\"../../results/rag_hybrid_test.csv\",\n",
    "    desc=\"Teste (RAG Hybrid)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb51bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_test.to_csv(\"rag_hybrid_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_test['is_correct'].mean()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
