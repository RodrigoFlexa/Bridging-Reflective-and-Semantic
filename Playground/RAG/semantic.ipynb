{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1fed17",
   "metadata": {},
   "source": [
    "# Semantic RAG System â€” Simple Retrieval\n",
    "\n",
    "Sistema de memÃ³ria semÃ¢ntica para comparaÃ§Ã£o com sistemas cognitivos:\n",
    "- **MemÃ³rias SemÃ¢nticas**: Fatos cientÃ­ficos extraÃ­dos de questÃµes resolvidas\n",
    "- **Retrieval ClÃ¡ssico**: Apenas similaridade semÃ¢ntica (sem score cognitivo, sem decay, sem geraÃ§Ã£o/refinamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb59bb2",
   "metadata": {},
   "source": [
    "# 0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb50f0",
   "metadata": {},
   "source": [
    "### A) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84394769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "# Add vectorstore & prompts folders to path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'vectorstore')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'prompts')))\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Memory managers & metadata builders\n",
    "from simple_vector_memory import SimpleVectorMemory, build_metadata_semantic\n",
    "\n",
    "# Prompt templates\n",
    "from templates import SEMANTIC_TEMPLATE\n",
    "\n",
    "# Local imports\n",
    "from utils_notebook import (\n",
    "    SemanticCleaner,\n",
    "    calcular_metricas_memoria,\n",
    "    format_choices,\n",
    "    make_question\n",
    ")\n",
    "cleaner = SemanticCleaner()\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe18282",
   "metadata": {},
   "source": [
    "### B) Language Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fedb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['A','B','C','D']\n",
    "number = {'A':0,'B':1,'C':2,'D':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86501bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset do arc. (Caso nÃ£o tenha Ã© necessÃ¡rio instalar e guardar na pasta datasets como csv)\n",
    "# train_df = pd.read_csv(\"dataset/arc_challenge_train_processed.csv\")\n",
    "valid_df = pd.read_csv(\"../../dataset/arc_challenge_valid_processed.csv\")\n",
    "test_df = pd.read_csv(\"../../dataset/arc_challenge_test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM judge\n",
    "gpt5_nano = ChatOpenAI(model='gpt-5-nano-2025-08-07', temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbadb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi2 = ChatOllama(model=\"phi\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c9831a",
   "metadata": {},
   "source": [
    "### C) Prompts and Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ded9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_prompt = PromptTemplate.from_template(SEMANTIC_TEMPLATE)\n",
    "rag_chain = semantic_prompt | phi2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6435fa6",
   "metadata": {},
   "source": [
    "# 1) Utils Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3cc2bb",
   "metadata": {},
   "source": [
    "### A) Context Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_facts_clean(semantic_items, show_scores=False):\n",
    "    if not semantic_items:\n",
    "        return \"No relevant scientific facts found.\"\n",
    "\n",
    "    formatted_parts = [\"### SCIENTIFIC FACTS (THEORY)\"]\n",
    "    for i, fact in enumerate(semantic_items, 1):\n",
    "        f_text = fact.get('content', '').strip()\n",
    "\n",
    "        if show_scores:\n",
    "            sim = fact.get('similarity', 0)\n",
    "            block = f\"\"\"\n",
    "    * **Principle #{i}** (Sim: {sim:.2f})\n",
    "        Context: \"{sim}\"\n",
    "        Fact: \"{f_text}\"\n",
    "\"\"\"\n",
    "        else:\n",
    "            block = f\"\"\"\n",
    "    * **Principle #{i}**\n",
    "        * Context: \"{sim}\"\n",
    "        * Fact: \"{f_text}\"\n",
    "\"\"\"\n",
    "        formatted_parts.append(block)\n",
    "\n",
    "    return \"\\n\".join(formatted_parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf355c9",
   "metadata": {},
   "source": [
    "### B) Answer Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "def extract_answer(\n",
    "    small_llm_model,\n",
    "    model_text_output: str,\n",
    "    valid_labels: List[str] = None,\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    ExtraÃ§Ã£o robusta baseada em padrÃµes de alta confianÃ§a (Tier 1)\n",
    "    com fallback para LLM Juiz em caso de falha.\n",
    "    \"\"\"\n",
    "    if valid_labels is None:\n",
    "        valid_labels = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "    if not model_text_output:\n",
    "        return \"N/A\"\n",
    "\n",
    "    text = model_text_output.strip()\n",
    "\n",
    "    if \"```python\" in text or \"def solution\" in text:\n",
    "        if debug: print(\"âš  [CODE DETECTED] Enviando para LLM Judge.\")\n",
    "        return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "    strong_patterns = [\n",
    "        r\"\\\\boxed\\s*\\{\\s*([A-H])\\s*\\}\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]?\\s*(?:is)?\\s*(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]]\",\n",
    "        r\"The\\s+(?:correct\\s+)?(?:answer|option|choice)\\s+is\\s*(?:Option)?\\s*[:\\-]?\\s*[\\(\\[]([A-H])[\\)\\]]\",\n",
    "        r\"(?:Final|Correct)\\s+Answer\\s*[:\\-]\\s*(?:is\\s+)?(?:Option\\s+)?([A-H])(?=\\s|\\.|,|!|\\?|$)\",\n",
    "        r\"(?:Therefore|Thus|Hence|So),\\s*(?:the\\s+answer\\s+is\\s*)?(?:Option)?\\s*[\\(\\[]([A-H])[\\)\\]]\",\n",
    "        r\"(?:^|\\n)\\s*Answer\\s*:\\s*([A-H])(?=\\s|$|\\.|\\,)\",\n",
    "        r\"\\*\\*([A-H])\\*\\*\",\n",
    "        r\"^([A-H])$\"\n",
    "    ]\n",
    "\n",
    "    for pattern in strong_patterns:\n",
    "        matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "        if matches:\n",
    "            last_match = matches[-1]\n",
    "            candidate = last_match.group(1).upper()\n",
    "            if candidate in valid_labels:\n",
    "                if debug:\n",
    "                    print(f\"âœ“ [REGEX] PadrÃ£o encontrado: '{pattern}' -> {candidate}\")\n",
    "                return candidate\n",
    "\n",
    "    if debug: print(\"âœ— [REGEX] Falha nos padrÃµes fortes. Chamando LLM Judge.\")\n",
    "    return _llm_judge(small_llm_model, text, valid_labels, debug, question)\n",
    "\n",
    "\n",
    "def _llm_judge(\n",
    "    small_llm_model,\n",
    "    text: str,\n",
    "    valid_labels: List[str],\n",
    "    debug: bool = False,\n",
    "    question: Optional[str] = None,\n",
    ") -> str:\n",
    "    context_block = \"\"\n",
    "    if question:\n",
    "        context_block = f\"\"\"\n",
    "### CONTEXT (Use this to infer the answer letter):\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are an Answer Extraction Bot.\n",
    "Your ONLY job is to identify which option the following \"Model Output\" concluded is correct.\n",
    "\n",
    "{context_block}\n",
    "\n",
    "### Model Output to Analyze:\n",
    "{text}\n",
    "\n",
    "### Instructions:\n",
    "1. Look for an explicit answer (e.g., \"Answer: A\").\n",
    "2. If NO explicit letter is found, read the conclusion of the \"Model Output\" and match it against the Options in the Context. **Infer the letter.**\n",
    "   - Example: If Context has \"(A) 5 (B) 10\" and Model Output says \"The result is 10\", you must output B.\n",
    "3. Do NOT calculate or solve the problem yourself. Trust the \"Model Output\".\n",
    "4. If the model refuses to answer or is unclear, return \"E\".\n",
    "\n",
    "Output format: Just the single letter (A, B, C, D, or E). No other text.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = small_llm_model.invoke(prompt)\n",
    "        output = response.content if hasattr(response, 'content') else str(response)\n",
    "        clean_cand = re.sub(r\"[^A-E]\", \"\", output.strip().upper())\n",
    "\n",
    "        if len(clean_cand) > 1:\n",
    "            clean_cand = clean_cand[-1]\n",
    "\n",
    "        if clean_cand in valid_labels:\n",
    "            if debug: print(f\"âœ“ [LLM JUIZ] Inferido: {clean_cand}\")\n",
    "            return clean_cand\n",
    "\n",
    "        return \"N/A\"\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug: print(f\"âœ— [ERRO JUIZ] {e}\")\n",
    "        return \"N/A\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc9ce6",
   "metadata": {},
   "source": [
    "# 2) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b14c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_dataset_semantic(\n",
    "    df,\n",
    "    chain=rag_chain,\n",
    "    manager_semantic=manager_semantic,\n",
    "    k_retrieval=3,\n",
    "    threshold=0.33,\n",
    "    backup_frequency=160,\n",
    "    backup_path=None,\n",
    "    desc=\"Avaliando com Semantic Memory\"\n",
    "):\n",
    "    resultados = []\n",
    "    acertos = 0\n",
    "    total = len(df)\n",
    "    backup_counter = 0\n",
    "\n",
    "    global_sims = []\n",
    "    global_counts = []\n",
    "    erros = 0\n",
    "\n",
    "    # Preparar diretÃ³rio de backup\n",
    "    if backup_path:\n",
    "        backup_dir = os.path.dirname(backup_path)\n",
    "        if backup_dir and not os.path.exists(backup_dir):\n",
    "            os.makedirs(backup_dir, exist_ok=True)\n",
    "\n",
    "    loop = tqdm(df.iterrows(), total=total, desc=desc)\n",
    "\n",
    "    for idx, row in loop:\n",
    "        try:\n",
    "            full_question = make_question(row, inline=False)[0]\n",
    "            question_for_retriever = re.sub(r'\\([A-Z]\\)\\s*', '', row['question'])\n",
    "\n",
    "            items = manager_semantic.retrieve_memories(\n",
    "                query=question_for_retriever,\n",
    "                k=k_retrieval,\n",
    "                threshold=threshold\n",
    "            )\n",
    "            facts_formatted = format_facts_clean(items)\n",
    "\n",
    "            count, avg_sim, raw_sims = calcular_metricas_memoria(items)\n",
    "            global_counts.append(count)\n",
    "            global_sims.extend(raw_sims)\n",
    "\n",
    "            response_obj = chain.invoke({\n",
    "                \"question\": full_question,\n",
    "                \"similar_facts\": facts_formatted,\n",
    "            })\n",
    "\n",
    "            response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "\n",
    "            pred = extract_answer(\n",
    "                small_llm_model=gpt5_nano,\n",
    "                model_text_output=response_text,\n",
    "                question=full_question,\n",
    "            )\n",
    "\n",
    "            if pred not in LABELS:\n",
    "                pred = \"Invalid\"\n",
    "                erros += 1\n",
    "\n",
    "            target = row['answerKey']\n",
    "            is_correct = (pred == target)\n",
    "            acertos += int(is_correct)\n",
    "\n",
    "            # === BACKUP ===\n",
    "            if backup_path and backup_frequency > 0 and (idx + 1) % backup_frequency == 0:\n",
    "                backup_counter += 1\n",
    "                df_partial = pd.DataFrame(resultados)\n",
    "                backup_file = backup_path.replace('.csv', '_backup.csv')\n",
    "                df_partial.to_csv(backup_file, index=False)\n",
    "\n",
    "            # Atualiza barra de progresso\n",
    "            acc_atual = (acertos / (loop.n + 1)) * 100\n",
    "            loop.set_postfix(acc=f\"{acc_atual:.2f}%\", n_mem=count, avg_sim=f\"{avg_sim:.4f}\", erros=erros, bkp=backup_counter)\n",
    "\n",
    "            resultados.append({\n",
    "                'index': idx,\n",
    "                'question': full_question,\n",
    "                'retrieved_context': facts_formatted,\n",
    "                'retrieved_count': count,\n",
    "                'avg_similarity': avg_sim,\n",
    "                'raw_output': response_text,\n",
    "                'pred': pred,\n",
    "                'target': target,\n",
    "                'is_correct': is_correct,\n",
    "                'source': desc,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Erro no Ã­ndice {idx}: {e}\")\n",
    "            resultados.append({\n",
    "                'index': idx,\n",
    "                'error': str(e),\n",
    "                'is_correct': False,\n",
    "                'retrieved_count': 0,\n",
    "                'avg_similarity': 0.0,\n",
    "            })\n",
    "\n",
    "    if backup_path and backup_counter > 0:\n",
    "        print(f\"ðŸ’¾ Total de backups salvos: {backup_counter}\")\n",
    "\n",
    "    return pd.DataFrame(resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ca7ab",
   "metadata": {},
   "source": [
    "# 3) ConstruÃ§Ã£o dos Bancos de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00765ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific_facts = pd.read_csv(\"../../scientific_facts_expanded.csv\")\n",
    "scientific_facts = scientific_facts.fillna(\"N/A\")\n",
    "\n",
    "df_train_subset = scientific_facts[scientific_facts['origin'] == 'train'].copy()\n",
    "df_train_subset = df_train_subset[df_train_subset['scientific_fact'] != \"N/A\"]\n",
    "df_train_subset.drop_duplicates(subset=['scientific_fact'], keep='first', inplace=True)\n",
    "df_train_subset['scientific_fact'] = df_train_subset['scientific_fact'].apply(lambda x: cleaner.clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ced2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ [Sistema] Reset total solicitado. Apagando vectorstores/semantic/chroma_semantic...\n",
      "ðŸ“‚ Carregando base de vetores de: vectorstores/semantic/chroma_semantic\n",
      "âœ… Base carregada com 3286 itens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SimpleVectorMemory at 0x18197685850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FORCE_RESET = False\n",
    "\n",
    "manager_semantic = SimpleVectorMemory(\n",
    "    db_path=\"vectorstores/semantic/chroma_semantic\",\n",
    "    embedding_model=embedding_model\n",
    ")\n",
    "\n",
    "manager_semantic.init_from_dataframe(\n",
    "    df=df_train_subset,\n",
    "    content_col='scientific_fact',\n",
    "    id_prefix='semantic',\n",
    "    metadata_func=build_metadata_semantic,\n",
    "    reset_db=FORCE_RESET\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb24e9e",
   "metadata": {},
   "source": [
    "# 4) Testing in one question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13c8beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "k = 3\n",
    "row = valid_df.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6a312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " Juan and LaKeisha roll a few objects down a ramp. They want to see which object rolls the farthest. What should they do so they can repeat their investigation?\n",
      "(A) Put the objects in groups.\n",
      "(B) Change the height of the ramp.\n",
      "(C) Choose different objects to roll.\n",
      "(D) Record the details of the investigation.\n",
      "\n",
      "Contexto:  \n",
      "    Question: Which situation would be considered observation and measurement?\n",
      "    Knowledge: Observation involves using senses or tools to gather information.\n",
      "\n",
      "\n",
      "    Question: What should be done when the results of an experiment do not support the hypothesis?\n",
      "    Knowledge: Repeating the experiment and checking for errors can gather more data to refine understanding of the phenomenon under investigation.\n",
      "\n",
      "\n",
      "    Question: Which type of force requires contact between two objects for one to push or pull the other?\n",
      "    Knowledge: When an object rolls on a surface, microscopic irregularities interact and cause friction.\n",
      "\n",
      "\n",
      "MÃ©tricas das memÃ³rias recuperadas: \n",
      "Count:  3  AVG_SIM:  0.34378522634506226  Raw_SIM:  [0.35016751289367676, 0.34237122535705566, 0.33881694078445435]\n"
     ]
    }
   ],
   "source": [
    "q_text = make_question(row, inline=False)[0]\n",
    "question_clean = re.sub(r'\\([A-Z]\\)\\s*', '', row['question'])\n",
    "\n",
    "items = manager_semantic.retrieve_memories(\n",
    "    query=question_clean,\n",
    "    k=k,\n",
    "    threshold=0.33\n",
    ")\n",
    "facts_semantic = format_facts_clean(items, show_scores=True)\n",
    "\n",
    "print(\"Question: \\n\", q_text)\n",
    "print(\"\\nContexto: \\n\", facts_semantic)\n",
    "\n",
    "count, avg_sim, raw_sims = calcular_metricas_memoria(items)\n",
    "print(\"\\nMÃ©tricas das memÃ³rias recuperadas: \")\n",
    "print(\"Count: \", count, \" AVG_SIM: \", avg_sim, \" Raw_SIM: \", raw_sims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76353d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ [REGEX] PadrÃ£o encontrado: '(?:Final|Correct)\\s+Answer\\s*[:\\-]?\\s*(?:is)?\\s*(?:Option)?\\s*[\\(\\[]?([A-H])[\\)\\]]?' -> D\n",
      "# Raw Response: \n",
      "  \n",
      "1. Reasoning: The correct answer is (D). To repeat an investigation, it's important to record all relevant details such as the materials used, the setup, and any changes made during the experiment. This will help ensure that the same conditions are replicated in future experiments for accurate comparison of results.\n",
      "2. Principles: Recording experimental details helps maintain consistency and allows for replication by other scientists or researchers. It also aids in identifying potential sources of error and refining the experimental design.\n",
      "3. Answer: (D) Record the details of the investigation.\n",
      "\n",
      "# Alternative chosen:  D (Correta: D)\n",
      "----------------------------------------------\n",
      "Tempo para rodar extraction:  0.0\n",
      "Tempo para rodar invoke:  1.31\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "a = time.time()\n",
    "facts_clean = format_facts_clean(items, show_scores=False)\n",
    "response_obj = rag_chain.invoke({\n",
    "    \"question\": q_text,\n",
    "    \"similar_facts\": facts_clean,\n",
    "})\n",
    "b = time.time()\n",
    "response_text = response_obj.content if hasattr(response_obj, \"content\") else str(response_obj)\n",
    "\n",
    "c = time.time()\n",
    "pred = extract_answer(\n",
    "    small_llm_model=gpt5_nano,\n",
    "    model_text_output=response_text,\n",
    "    question=q_text,\n",
    "    debug=True\n",
    ")\n",
    "d = time.time()\n",
    "\n",
    "target = row['answerKey']\n",
    "is_correct = (pred == target)\n",
    "\n",
    "print(\"# Raw Response: \\n\", response_text)\n",
    "print(\"# Alternative chosen: \", pred, f\"(Correta: {target})\")\n",
    "print(f\"# Resultado: {'CORRETO' if is_correct else 'ERRO'}\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Tempo para rodar extraction: \", round(d - c, 2))\n",
    "print(\"Tempo para rodar invoke: \", round(b - a, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5af3e8",
   "metadata": {},
   "source": [
    "# 5) Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19433a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset ValidaÃ§Ã£o: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [09:10<00:00,  1.84s/it, acc=70.90%, avg_sim=0.6393, erros=6, n_mem=3]\n"
     ]
    }
   ],
   "source": [
    "# VALIDAÃ‡ÃƒO\n",
    "df_resultado_valid_semantic = avaliar_dataset_semantic(\n",
    "    df=valid_df,\n",
    "    chain=rag_chain,\n",
    "    manager_semantic=manager_semantic,\n",
    "    k_retrieval=3,\n",
    "    threshold=0.33,\n",
    "    backup_frequency=130,\n",
    "    backup_path=\"../../results/rag_semantic_valid.csv\",\n",
    "    desc=\"ValidaÃ§Ã£o (Semantic RAG)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset Teste: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1172/1172 [36:50<00:00,  1.89s/it, acc=69.28%, avg_sim=0.6920, erros=27, n_mem=3]\n"
     ]
    }
   ],
   "source": [
    "# TESTE\n",
    "df_resultado_test_semantic = avaliar_dataset_semantic(\n",
    "    df=test_df,\n",
    "    chain=rag_chain,\n",
    "    manager_semantic=manager_semantic,\n",
    "    k_retrieval=3,\n",
    "    threshold=0.33,\n",
    "    backup_frequency=150,\n",
    "    backup_path=\"../../results/rag_semantic_test.csv\",\n",
    "    desc=\"Teste (Semantic RAG)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d535d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultado_test_semantic.to_csv(\"rag_semantic_test.csv\", index=False)\n",
    "df_resultado_valid_semantic.to_csv(\"rag_semantic_valid.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
